---
layout: post
title:  "Exploring phase space with Nested Sampling"
date:   2022-05-04
categories: papers
---
![AI generated image](/assets/images/posts/2022-05-04-2205.02030.png)

<!-- BEGINNING OF GENERATED POST -->
In a significant contribution to computational physics, our paper [[2205.02030](https://arxiv.org/abs/2205.02030)], led by [David Yallup](https://www.linkedin.com/in/dyallup/) along with Timo Jan√üen, Steffen Schumann, and [Will Handley](https://willhandley.co.uk), presents the first application of Nested Sampling to the complex problem of exploring particle collision phase space. This work bridges the gap between advanced Bayesian inference methods and the practical demands of high-energy physics simulations.

### The Challenge of Simulating Particle Collisions

Simulating scattering events at particle colliders like the Large Hadron Collider (LHC) is crucial for analyzing and interpreting experimental data. A central task is the numerical integration of partonic scattering cross-sections over high-dimensional phase spaces. These integrals are notoriously difficult, often featuring highly non-trivial, multimodal target densities that arise from intermediate resonances, quantum interference, and kinematic cuts.

Traditional methods rely heavily on adaptive multi-channel importance sampling, with algorithms like `Vegas` [[10.1016/0021-9991(78)90004-9](https://doi.org/10.1016/0021-9991(78)90004-9)] being a common tool. However, the performance of these techniques can degrade significantly as the dimensionality and complexity of the problem increase. They often require detailed, a priori knowledge of the target distribution to construct efficient sampling channels, a task that becomes increasingly difficult for novel or high-multiplicity processes.

### A New Paradigm: Nested Sampling for Phase Space Exploration

Our work introduces a novel approach by adapting Nested Sampling (NS), an algorithm originally conceived for Bayesian evidence calculation [[10.1214/06-BA127](https://doi.org/10.1214/06-BA127)], to this domain. Using the `PolyChord` implementation [[1506.00171](https://arxiv.org/abs/1506.00171)], we reframe the cross-section integral as a Bayesian evidence computation. The core idea of NS involves:
*   **Live Points:** Maintaining an ensemble of sample points, called 'live points', within the phase space.
*   **Iterative Contraction:** Systematically replacing the live point with the lowest likelihood (squared matrix element) with a new point sampled from the prior, but constrained to have a higher likelihood.
*   **Volume Estimation:** This iterative process naturally compresses the volume occupied by the live points towards the regions of highest probability, allowing for a robust estimate of the total integral (the cross-section). As a byproduct, the discarded 'dead points' form a representative sample of the underlying distribution, which can be used for event generation.

### Benchmarking Performance on Gluon Scattering

To validate our method, we applied it to the challenging processes of gluon scattering into 3-, 4-, and 5-gluon final states. These serve as excellent benchmarks for complex jet production at hadron colliders. The key findings are:
*   **Superior Performance:** Starting from a completely uninformed, flat prior, Nested Sampling outperforms the `Vegas` algorithm.
*   **Robust Scaling:** Most impressively, the efficiency of NS remains consistently high as the phase space dimensionality increases from the 3-gluon (5D) to the 5-gluon (11D) case. In contrast, the efficiency of `Vegas` dropped by an order of magnitude over the same range.
*   **Competitive Results:** The performance of NS is comparable to that of a dedicated, physics-informed multi-channel sampler (`HAAG`), demonstrating its ability to learn the complex structure of the phase space without any initial guidance.

### Implications and Future Directions

This study establishes Nested Sampling as a powerful, largely self-tuning algorithm for phase space integration and event generation. Its ability to perform efficiently without prior knowledge makes it a highly promising tool for tackling the next generation of computational challenges in particle physics, particularly with the advent of the High-Luminosity LHC [[2203.11110](https://arxiv.org/abs/2203.11110)]. Our work paves the way for several exciting future developments, including the use of informed priors to further boost efficiency, the application of dynamic nested sampling, and synergistic combinations with modern machine learning techniques like Normalizing Flows. By adapting a state-of-the-art technique from the Bayesian world, we have unlocked a potent new capability for the high-energy physics community.
<!-- END OF GENERATED POST -->

<img src="/assets/group/images/david_yallup.jpg" alt="David Yallup" style="width: auto; height: 25vw;"><img src="/assets/group/images/will_handley.jpg" alt="Will Handley" style="width: auto; height: 25vw;">

Content generated by [gemini-2.5-pro](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/2022-05-04-2205.02030.txt).

Image generated by [imagen-3.0-generate-002](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/2022-05-04-2205.02030.txt).