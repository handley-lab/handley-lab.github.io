---
layout: post
title:  "Nested sampling with any prior you like"
date:   2021-02-24
categories: papers
---
![AI generated image](/assets/images/posts/2021-02-24-2102.12478.png)

<!-- BEGINNING OF GENERATED POST -->
In a significant advance for Bayesian computation, our paper [2102.12478](https://arxiv.org/abs/2102.12478), led by Justin Alsing and co-authored by [Will Handley](https://willhandley.co.uk), introduces a powerful method to make nested sampling more flexible and broadly applicable. Nested sampling, first introduced by John Skilling ([10.1214/06-BA127](https://doi.org/10.1214/06-BA127)), is a cornerstone of modern Bayesian analysis in astrophysics and beyond, enabling both parameter inference for complex models and the calculation of the Bayesian evidence for model comparison. However, a persistent technical hurdle has limited its full potential.

### The Challenge: The Unit Hyper-cube Constraint

Many widely-used nested sampling algorithms, including popular codes like `MultiNest` ([10.1111/j.1365-2966.2009.14548.x](https://doi.org/10.1111/j.1365-2966.2009.14548.x)) and our group's `PolyChord` ([10.1093/mnrasl/slv047](https://doi.org/10.1093/mnrasl/slv047)), require that prior probability distributions be specified as a transformation from a simple, uniform distribution on a unit hyper-cube. While this is straightforward for many standard, analytically-defined priors, it poses a major challenge in more complex scenarios.

A particularly common and important case is sequential Bayesian analysis, where the posterior distribution from one experiment is used as the prior for a subsequent analysis. In this situation, we typically only have samples from the posterior, not an analytic transformation from the unit cube. This has made chaining analyses with nested sampling cumbersome, often requiring extra, less efficient steps.

### A Solution with Parametric Bijectors

This work, spearheaded by Justin Alsing, presents an elegant and general-purpose solution: using parametric bijectors. A bijector is an invertible function that can map one probability distribution to another. The key idea is to fit a flexible, expressive bijector model—such as a normalizing flow parameterized by a neural network—to a set of samples drawn from the desired target prior.

The process involves:
1.  **Sampling**: Obtaining samples from the target prior distribution (e.g., the posterior samples from a previous experiment).
2.  **Training**: Optimizing the parameters of the bijector model to accurately represent the transformation from a simple base density (like a uniform or Gaussian distribution) to the target prior. This is typically done by minimizing the KL-divergence between the model and the target.
3.  **Application**: Using the trained bijector function within the nested sampling algorithm to draw new live points, effectively allowing the use of any prior from which one can sample.

This approach leverages recent advances in probabilistic machine learning, including powerful models like Inverse Autoregressive Flows and Continuous Normalizing Flows, to learn arbitrarily complex prior shapes.

### Demonstration in Cosmology

To prove the concept, the paper provides a compelling, non-trivial example from cosmology. The authors consider a $\Lambda$CDM model with spatial curvature ($\Omega_K$), a scenario known for its challenging, banana-shaped posterior distributions. They compare two routes to obtaining a final, combined constraint from Cosmic Microwave Background (CMB) and Baryon Acoustic Oscillation (BAO) data:

*   **Route 1 (Traditional):** A single nested sampling run using a simple initial prior and the combined CMB+BAO likelihood.
*   **Route 2 (Bijector Method):** A two-step process. First, an analysis is run with the CMB data alone. Then, a bijector is trained on the resulting CMB posterior samples. This bijector is then used as the prior for a second nested sampling run with the BAO data.

The results, shown in the paper's figures, demonstrate that both routes lead to statistically consistent posterior distributions and Bayesian evidence values. This confirms that the bijector method correctly captures the information from the interim posterior. Moreover, this sequential approach can be significantly more computationally efficient, as the second analysis (updating from a well-constrained prior) requires far fewer likelihood evaluations. This work effectively removes a major barrier to the practical application of nested sampling, enabling more modular, efficient, and sophisticated Bayesian workflows in cosmology and other data-driven sciences.
<!-- END OF GENERATED POST -->

<img src="/assets/group/images/will_handley.jpg" alt="Will Handley" style="width: auto; height: 25vw;">

Content generated by [gemini-2.5-pro](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/2021-02-24-2102.12478.txt).

Image generated by [imagen-3.0-generate-002](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/2021-02-24-2102.12478.txt).