---
layout: post
title:  "Nested sampling with plateaus"
date:   2020-10-26
categories: papers
---
![AI generated image](/assets/images/posts/2020-10-26-2010.13884.png)

<!-- BEGINNING OF GENERATED POST -->
In our paper, "[Nested sampling with plateaus](https://arxiv.org/abs/2010.13884)," lead author Andrew Fowlie, along with [Will Handley](https://willhandley.co.uk) and Liangliang Su, tackle a subtle but critical issue within the nested sampling (NS) algorithm, a cornerstone of Bayesian inference in astrophysics and particle physics. The original NS algorithm, pioneered by John Skilling ([10.1063/1.1835238](https://doi.org/10.1063/1.1835238)), is a powerful tool for model comparison and parameter estimation. However, as recently emphasized by other researchers ([2020arXiv200508602S](https://arxiv.org/abs/2020arXiv200508602S)), its core assumptions can be violated in the presence of likelihood "plateaus"â€”regions where the likelihood value is constant. This can lead to faulty estimates of the Bayesian evidence and posterior distributions, undermining the reliability of scientific conclusions.

### The Challenge with Likelihood Plateaus
Nested sampling operates by iteratively replacing the point with the lowest likelihood (`L*`) from a set of 'live points' with a new point sampled from the prior with a likelihood greater than `L*`. This process allows the algorithm to map out the relationship between likelihood and enclosed prior volume. The critical assumption is that the live points are uniformly distributed within the prior volume being explored.

A plateau breaks this assumption. When multiple live points share the same minimum likelihood, the algorithm's method for contracting the prior volume becomes invalid. Standard NS would replace one point and compress the volume as if a new, higher likelihood contour has been reached. In reality, the contour has not changed, as other points still exist at that same likelihood value. This leads to an overestimation of the prior volume associated with that likelihood, which in turn systematically biases the evidence calculation.

### A Robust and Simple Modification
Our paper introduces a modified NS algorithm that elegantly resolves this issue. The core change is simple yet profound: instead of replacing just one point at the likelihood threshold, our algorithm identifies *all* live points that fall on the plateau. It then proceeds to evict these points one by one, without replacement. Crucially, after each individual eviction, the algorithm performs the standard NS volume contraction, but it accounts for the dynamically shrinking number of live points. Only after all points from the plateau have been removed does the algorithm replenish the live point set with new samples drawn from a strictly higher likelihood region. This one-by-one eviction and dynamic compression ensure the volume contraction is estimated correctly. This approach is not only more accurate but also connects to the principles of dynamic nested sampling ([10.1007/s11222-018-9844-0](https://doi.org/10.1007/s11222-018-9844-0)), where the number of live points can change during a run.

### Practical Implementation and Validation
A key advantage of our proposed modification is its ease of implementation. It does not require a fundamental rewrite of existing samplers. In fact, we have implemented this correction within our `anesthetic` Python package ([10.21105/joss.01414](https://doi.org/10.21105/joss.01414)), allowing researchers to retrospectively apply the fix to existing runs from popular codes like `PolyChord` and `MultiNest`. This makes our solution immediately accessible and useful to the community.

To rigorously test the algorithm, we designed a "wedding cake" likelihood function. This function is constructed from a series of concentric, hypercubic plateaus with decreasing volume and a Gaussian height profile. It's a challenging test case that standard NS fails on, as it features multiple, distinct plateaus. Our modified algorithm, however, correctly computes the evidence for this complex likelihood, demonstrating its robustness.

In conclusion, our work identifies and provides a robust, easy-to-implement solution to a critical flaw in nested sampling. By modifying the algorithm to correctly handle likelihood plateaus, we enhance the reliability of Bayesian evidence and posterior estimation across many fields of physics. Given its simplicity and effectiveness, we propose that this modified procedure should become the canonical version of the nested sampling algorithm.
<!-- END OF GENERATED POST -->

<img src="/assets/group/images/will_handley.jpg" alt="Will Handley" style="width: auto; height: 25vw;">

Content generated by [gemini-2.5-pro](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/2020-10-26-2010.13884.txt).

Image generated by [imagen-3.0-generate-002](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/2020-10-26-2010.13884.txt).