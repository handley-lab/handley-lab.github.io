{% raw %}

Title: Create a Markdown Blog Post Integrating Research Details and a Featured Paper
====================================================================================

This task involves generating a Markdown file (ready for a GitHub-served Jekyll site) that integrates our research details with a featured research paper. The output must follow the exact format and conventions described below.

====================================================================================
Output Format (Markdown):
------------------------------------------------------------------------------------
---
layout: post
title:  "Dynamic or Systematic? Bayesian model selection between dark energy and supernova biases"
date:   2025-09-16
categories: papers
---
![AI generated image](/assets/images/posts/2025-09-16-2509.13220.png)

<!-- BEGINNING OF GENERATED POST -->
<!-- END OF GENERATED POST -->

<img src="/assets/group/images/adam_ormondroyd.jpg" alt="A. N. Ormondroyd" style="width: auto; height: 14vw;"><img src="/assets/group/images/will_handley.jpg" alt="W. J. Handley" style="width: auto; height: 14vw;"><img src="https://www.phy.cam.ac.uk/wp-content/uploads/2025/04/hobson-150x150.jpg" alt="M. P. Hobson" style="width: auto; height: 14vw;"><img src="https://www.phy.cam.ac.uk/wp-content/uploads/2025/04/lasenby-150x150.jpg" alt="A. N. Lasenby" style="width: auto; height: 14vw;"><img src="/assets/group/images/david_yallup.jpg" alt="D. Yallup" style="width: auto; height: 14vw;">

Content generated by [gemini-2.5-pro](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/2025-09-16-2509.13220.txt).

Image generated by [imagen-4.0-generate-001](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/2025-09-16-2509.13220.txt).

------------------------------------------------------------------------------------
====================================================================================

Please adhere strictly to the following instructions:

====================================================================================
Section 1: Content Creation Instructions
====================================================================================

1. **Generate the Page Body:**
   - Write a well-composed, engaging narrative that is suitable for a scholarly audience interested in advanced AI and astrophysics.
   - Ensure the narrative is original and reflective of the tone and style and content in the "Homepage Content" block (provided below), but do not reuse its content.
   - Use bullet points, subheadings, or other formatting to enhance readability.

2. **Highlight Key Research Details:**
   - Emphasize the contributions and impact of the paper, focusing on its methodology, significance, and context within current research.
   - Specifically highlight the lead author ({'name': 'A. N. Ormondroyd'}). When referencing any author, use Markdown links from the Author Information block (choose academic or GitHub links over social media).

3. **Integrate Data from Multiple Sources:**
   - Seamlessly weave information from the following:
     - **Paper Metadata (YAML):** Essential details including the title and authors.
     - **Paper Source (TeX):** Technical content from the paper.
     - **Bibliographic Information (bbl):** Extract bibliographic references.
     - **Author Information (YAML):** Profile details for constructing Markdown links.
   - Merge insights from the Paper Metadata, TeX source, Bibliographic Information, and Author Information blocks into a coherent narrativeâ€”do not treat these as separate or isolated pieces.
   - Insert the generated narrative between the HTML comments:
     <!-- BEGINNING OF GENERATED POST --> and <!-- END OF GENERATED POST -->

4. **Generate Bibliographic References:**
   - Review the Bibliographic Information block carefully.
   - For each reference that includes a DOI or arXiv identifier:
     - For DOIs, generate a link formatted as:
       [10.1234/xyz](https://doi.org/10.1234/xyz)
     - For arXiv entries, generate a link formatted as:
       [2103.12345](https://arxiv.org/abs/2103.12345)
    - **Important:** Do not use any LaTeX citation commands (e.g., `\cite{...}`). Every reference must be rendered directly as a Markdown link. For example, instead of `\cite{mycitation}`, output `[mycitation](https://doi.org/mycitation)`
        - **Incorrect:** `\cite{10.1234/xyz}`  
        - **Correct:** `[10.1234/xyz](https://doi.org/10.1234/xyz)`
   - Ensure that at least three (3) of the most relevant references are naturally integrated into the narrative.
   - Ensure that the link to the Featured paper [2509.13220](https://arxiv.org/abs/2509.13220) is included in the first sentence.

5. **Final Formatting Requirements:**
   - The output must be plain Markdown; do not wrap it in Markdown code fences.
   - Preserve the YAML front matter exactly as provided.

====================================================================================
Section 2: Provided Data for Integration
====================================================================================

1. **Homepage Content (Tone and Style Reference):**
```markdown
---
layout: home
---

![AI generated image](/assets/images/index.png)

<!-- START OF WEBSITE SUMMARY -->
The Handley Research Group stands at the forefront of cosmological exploration, pioneering novel approaches that fuse fundamental physics with the transformative power of artificial intelligence. We are a dynamic team of researchers, including PhD students, postdoctoral fellows, and project students, based at the University of Cambridge. Our mission is to unravel the mysteries of the Universe, from its earliest moments to its present-day structure and ultimate fate. We tackle fundamental questions in cosmology and astrophysics, with a particular focus on leveraging advanced Bayesian statistical methods and AI to push the frontiers of scientific discovery. Our research spans a wide array of topics, including the [primordial Universe](https://arxiv.org/abs/1907.08524), [inflation](https://arxiv.org/abs/1807.06211), the nature of [dark energy](https://arxiv.org/abs/2503.08658) and [dark matter](https://arxiv.org/abs/2405.17548), [21-cm cosmology](https://arxiv.org/abs/2210.07409), the [Cosmic Microwave Background (CMB)](https://arxiv.org/abs/1807.06209), and [gravitational wave astrophysics](https://arxiv.org/abs/2411.17663).

### Our Research Approach: Innovation at the Intersection of Physics and AI

At The Handley Research Group, we develop and apply cutting-edge computational techniques to analyze complex astronomical datasets. Our work is characterized by a deep commitment to principled [Bayesian inference](https://arxiv.org/abs/2205.15570) and the innovative application of [artificial intelligence (AI) and machine learning (ML)](https://arxiv.org/abs/2504.10230).

**Key Research Themes:**
*   **Cosmology:** We investigate the early Universe, including [quantum initial conditions for inflation](https://arxiv.org/abs/2002.07042) and the generation of [primordial power spectra](https://arxiv.org/abs/2112.07547). We explore the enigmatic nature of [dark energy, using methods like non-parametric reconstructions](https://arxiv.org/abs/2503.08658), and search for new insights into [dark matter](https://arxiv.org/abs/2405.17548). A significant portion of our efforts is dedicated to [21-cm cosmology](https://arxiv.org/abs/2104.04336), aiming to detect faint signals from the Cosmic Dawn and the Epoch of Reionization.
*   **Gravitational Wave Astrophysics:** We develop methods for [analyzing gravitational wave signals](https://arxiv.org/abs/2411.17663), extracting information about extreme astrophysical events and fundamental physics.
*   **Bayesian Methods & AI for Physical Sciences:** A core component of our research is the development of novel statistical and AI-driven methodologies. This includes advancing [nested sampling techniques](https://arxiv.org/abs/1506.00171) (e.g., [PolyChord](https://arxiv.org/abs/1506.00171), [dynamic nested sampling](https://arxiv.org/abs/1704.03459), and [accelerated nested sampling with $\beta$-flows](https://arxiv.org/abs/2411.17663)), creating powerful [simulation-based inference (SBI) frameworks](https://arxiv.org/abs/2504.10230), and employing [machine learning for tasks such as radiometer calibration](https://arxiv.org/abs/2504.16791), [cosmological emulation](https://arxiv.org/abs/2503.13263), and [mitigating radio frequency interference](https://arxiv.org/abs/2211.15448). We also explore the potential of [foundation models for scientific discovery](https://arxiv.org/abs/2401.00096).

**Technical Contributions:**
Our group has a strong track record of developing widely-used scientific software. Notable examples include:
*   [**PolyChord**](https://arxiv.org/abs/1506.00171): A next-generation nested sampling algorithm for Bayesian computation.
*   [**anesthetic**](https://arxiv.org/abs/1905.04768): A Python package for processing and visualizing nested sampling runs.
*   [**GLOBALEMU**](https://arxiv.org/abs/2104.04336): An emulator for the sky-averaged 21-cm signal.
*   [**maxsmooth**](https://arxiv.org/abs/2007.14970): A tool for rapid maximally smooth function fitting.
*   [**margarine**](https://arxiv.org/abs/2205.12841): For marginal Bayesian statistics using normalizing flows and KDEs.
*   [**fgivenx**](https://arxiv.org/abs/1908.01711): A package for functional posterior plotting.
*   [**nestcheck**](https://arxiv.org/abs/1804.06406): Diagnostic tests for nested sampling calculations.

### Impact and Discoveries
Our research has led to significant advancements in cosmological data analysis and yielded new insights into the Universe. Key achievements include:
*   Pioneering the development and application of advanced Bayesian inference tools, such as [PolyChord](https://arxiv.org/abs/1506.00171), which has become a cornerstone for cosmological parameter estimation and model comparison globally.
*   Making significant contributions to the analysis of major cosmological datasets, including the [Planck mission](https://arxiv.org/abs/1807.06209), providing some of the tightest constraints on cosmological parameters and models of [inflation](https://arxiv.org/abs/1807.06211).
*   Developing novel AI-driven approaches for astrophysical challenges, such as using [machine learning for radiometer calibration in 21-cm experiments](https://arxiv.org/abs/2504.16791) and [simulation-based inference for extracting cosmological information from galaxy clusters](https://arxiv.org/abs/2504.10230).
*   Probing the nature of dark energy through innovative [non-parametric reconstructions of its equation of state](https://arxiv.org/abs/2503.08658) from combined datasets.
*   Advancing our understanding of the early Universe through detailed studies of [21-cm signals from the Cosmic Dawn and Epoch of Reionization](https://arxiv.org/abs/2301.03298), including the development of sophisticated foreground modelling techniques and emulators like [GLOBALEMU](https://arxiv.org/abs/2104.04336).
*   Developing new statistical methods for quantifying tensions between cosmological datasets ([Quantifying tensions in cosmological parameters: Interpreting the DES evidence ratio](https://arxiv.org/abs/1902.04029)) and for robust Bayesian model selection ([Bayesian model selection without evidences: application to the dark energy equation-of-state](https://arxiv.org/abs/1506.09024)).
*   Exploring fundamental physics questions such as potential [parity violation in the Large-Scale Structure using machine learning](https://arxiv.org/abs/2410.16030).

### Charting the Future: AI-Powered Cosmological Discovery
The Handley Research Group is poised to lead a new era of cosmological analysis, driven by the explosive growth in data from next-generation observatories and transformative advances in artificial intelligence. Our future ambitions are centred on harnessing these capabilities to address the most pressing questions in fundamental physics.

**Strategic Research Pillars:**
*   **Next-Generation Simulation-Based Inference (SBI):** We are developing advanced SBI frameworks to move beyond traditional likelihood-based analyses. This involves creating sophisticated codes for simulating [Cosmic Microwave Background (CMB)](https://arxiv.org/abs/1908.00906) and [Baryon Acoustic Oscillation (BAO)](https://arxiv.org/abs/1607.00270) datasets from surveys like DESI and 4MOST, incorporating realistic astrophysical effects and systematic uncertainties. Our AI initiatives in this area focus on developing and implementing cutting-edge SBI algorithms, particularly [neural ratio estimation (NRE) methods](https://arxiv.org/abs/2407.15478), to enable robust and scalable inference from these complex simulations.
*   **Probing Fundamental Physics:** Our enhanced analytical toolkit will be deployed to test the standard cosmological model ($\Lambda$CDM) with unprecedented precision and to explore [extensions to Einstein's General Relativity](https://arxiv.org/abs/2006.03581). We aim to constrain a wide range of theoretical models, from modified gravity to the nature of [dark matter](https://arxiv.org/abs/2106.02056) and [dark energy](https://arxiv.org/abs/1701.08165). This includes leveraging data from upcoming [gravitational wave observatories](https://arxiv.org/abs/1803.10210) like LISA, alongside CMB and large-scale structure surveys from facilities such as Euclid and JWST.
*   **Synergies with Particle Physics:** We will continue to strengthen the connection between cosmology and particle physics by expanding the [GAMBIT framework](https://arxiv.org/abs/2009.03286) to interface with our new SBI tools. This will facilitate joint analyses of cosmological and particle physics data, providing a holistic approach to understanding the Universe's fundamental constituents.
*   **AI-Driven Theoretical Exploration:** We are pioneering the use of AI, including [large language models and symbolic computation](https://arxiv.org/abs/2401.00096), to automate and accelerate the process of theoretical model building and testing. This innovative approach will allow us to explore a broader landscape of physical theories and derive new constraints from diverse astrophysical datasets, such as those from GAIA.

Our overarching goal is to remain at the forefront of scientific discovery by integrating the latest AI advancements into every stage of our research, from theoretical modeling to data analysis and interpretation. We are excited by the prospect of using these powerful new tools to unlock the secrets of the cosmos.
<!-- END OF WEBSITE SUMMARY -->

Content generated by [gemini-2.5-pro-preview-05-06](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/index.txt).

Image generated by [imagen-3.0-generate-002](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/index.txt).
```

2. **Paper Metadata:**
```yaml
!!python/object/new:feedparser.util.FeedParserDict
dictitems:
  id: http://arxiv.org/abs/2509.13220v2
  guidislink: true
  link: https://arxiv.org/abs/2509.13220v2
  title: Dynamic or Systematic? Bayesian model selection between dark energy and supernova
    biases
  title_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      type: text/plain
      language: null
      base: ''
      value: Dynamic or Systematic? Bayesian model selection between dark energy and
        supernova biases
  updated: '2025-10-27T09:23:47Z'
  updated_parsed: !!python/object/apply:time.struct_time
  - !!python/tuple
    - 2025
    - 10
    - 27
    - 9
    - 23
    - 47
    - 0
    - 300
    - 0
  - tm_zone: null
    tm_gmtoff: null
  links:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      href: https://arxiv.org/abs/2509.13220v2
      rel: alternate
      type: text/html
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      href: https://arxiv.org/pdf/2509.13220v2
      rel: related
      type: application/pdf
      title: pdf
  summary: "DES-5Y supernovae, combined with DESI BAO, appear to favour Chevallier-Polarski-Linder\
    \ $(w_0, w_a)$ dynamical dark energy over $\u039B$CDM. arXiv:2408.07175 suggested\
    \ that this is driven by a systematic in the DES pipeline, which particularly\
    \ affects the low-redshift supernovae brought in from legacy surveys. It is difficult\
    \ to investigate these data in isolation, however, as the complicated supernovae\
    \ pipelines must properly account for selection effects. In this work, we discover\
    \ that the Bayesian evidence previously found for flexknot dark energy (arXiv:2503.17342)\
    \ is beaten by a magnitude offset between the low- and high-redshift supernovae.\
    \ In addition, we find that the possible tension between DES-5Y and DESI is significantly\
    \ reduced by such an offset. We also take the opportunity to trial Nested Bridge\
    \ Sampling with Sequential Monte Carlo as an alternative method for calculating\
    \ Bayes factors."
  summary_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      type: text/plain
      language: null
      base: ''
      value: "DES-5Y supernovae, combined with DESI BAO, appear to favour Chevallier-Polarski-Linder\
        \ $(w_0, w_a)$ dynamical dark energy over $\u039B$CDM. arXiv:2408.07175 suggested\
        \ that this is driven by a systematic in the DES pipeline, which particularly\
        \ affects the low-redshift supernovae brought in from legacy surveys. It is\
        \ difficult to investigate these data in isolation, however, as the complicated\
        \ supernovae pipelines must properly account for selection effects. In this\
        \ work, we discover that the Bayesian evidence previously found for flexknot\
        \ dark energy (arXiv:2503.17342) is beaten by a magnitude offset between the\
        \ low- and high-redshift supernovae. In addition, we find that the possible\
        \ tension between DES-5Y and DESI is significantly reduced by such an offset.\
        \ We also take the opportunity to trial Nested Bridge Sampling with Sequential\
        \ Monte Carlo as an alternative method for calculating Bayes factors."
  tags:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: astro-ph.CO
      scheme: http://arxiv.org/schemas/atom
      label: null
  published: '2025-09-16T16:27:46Z'
  published_parsed: !!python/object/apply:time.struct_time
  - !!python/tuple
    - 2025
    - 9
    - 16
    - 16
    - 27
    - 46
    - 1
    - 259
    - 0
  - tm_zone: null
    tm_gmtoff: null
  arxiv_comment: 8 pages, 5 figures, 2 tables. Author list updated and minor other
    changes to match MNRAS submission. Comments welcome!
  arxiv_primary_category:
    term: astro-ph.CO
  authors:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: A. N. Ormondroyd
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: W. J. Handley
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: M. P. Hobson
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: A. N. Lasenby
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: D. Yallup
  author_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: D. Yallup
  author: D. Yallup

```

3. **Paper Source (TeX):**
```tex
% mnras_template.tex 
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}

% Allow "Thomas van Noord" and "Simon de Laguarde" and alike to be sorted by "N" and "L" etc. in the bibliography.
% Write the name in the bibliography as "\VAN{Noord}{Van}{van} Noord, Thomas"
\DeclareRobustCommand{\VAN}[3]{#2}
\let\VANthebibliography\thebibliography
\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{siunitx}    % units for physical quantities
\usepackage{hyperref}   % links to github
% \usepackage[frozencache,cachedir=.]{minted}  % code block in appendix
% \usepackage{minted}
\usepackage{multirow, makecell}   % used to place images in tensions table
\usepackage[table]{xcolor}     % coloured rows in table
\usepackage{color, soul}% Highlighting todo
% \usepackage{amssymb}	% Extra maths symbols

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%
\newcommand{\ncr}[2]{{}^{#1}C_{#2}}
\newcommand{\lcdm}{$\Lambda$CDM}
\newcommand{\dmb}{\ensuremath{\Delta m_\mathrm B}}

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[Dynamic or Systematic?]{Dynamic or Systematic? Bayesian model selection between dark energy and supernova biases}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[A.N.~Ormondroyd et al.]{
    A.N.~Ormondroyd,$^{1,2}$\thanks{E-mail: ano23@cam.ac.uk}
    W.J.~Handley,$^{2,3}$
    M.P.~Hobson,$^{1}$
    A.N.~Lasenby,$^{1,2}$
    and D.~Yallup$^{2,3}$
    \\
    % List of institutions
    $^{1}$Astrophysics Group, Cavendish Laboratory, J.J.~Thomson Avenue, Cambridge, CB3 0HE, UK\\
    $^{2}$Kavli Institute for Cosmology, Madingley Road, Cambridge, CB3 0HA, UK\\
    $^{3}$Institute of Astronomy, Madingley Road, Cambridge, CB3 0HA, UK\\
    }

    % These dates will be filled out by the publisher
    \date{Accepted XXX. Received YYY; in original form ZZZ}

    % Enter the current year, for the copyright statements etc.
    \pubyear{2025}

    % Don't change these lines
    \begin{document}
    \label{firstpage}
    \pagerange{\pageref{firstpage}--\pageref{lastpage}}
    \maketitle

    % Abstract of the paper
    \begin{abstract}
        DES-5Y supernovae, combined with DESI BAO, appear to favour Chevallier-Polarski-Linder $(w_0, w_a)$ dynamical dark energy over \lcdm{}.
        It has been suggested in other work that this is driven by a systematic in the DES pipeline, which particularly affects the low-redshift supernovae brought in from legacy surveys.
        It is difficult to investigate these data in isolation, however, as the complicated supernovae pipelines must properly account for selection effects.
        In this work, we discover that the Bayesian evidence previously found for flexknot dark energy in our previous work is beaten by a magnitude offset between the low- and high-redshift supernovae.
        In addition, we find that the possible tension between DES-5Y and DESI is significantly reduced by such an offset.
        We also take the opportunity to trial Nested Bridge Sampling with Sequential Monte Carlo as an alternative method for calculating Bayes factors.
    \end{abstract}

    % Select between one and six entries from the list of approved keywords.
    % Don't make up new ones.
    \begin{keywords}
        methods: statistical -- cosmology: dark energy, cosmological parameters
    \end{keywords}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

    \section{Introduction}

    Despite the successes of the standard cosmological model, known as \lcdm{}, the nature of dark energy has remained an enigma for almost three decades \citep{SupernovaSearchTeam:1998, SupernovaCosmologyProject:1998}.
    This has motivated the exploration of alternative phenomenological hypotheses, ranging from a first-order expansion, Gaussian processes, and our previous flexknot reconstruction. Ultimately, these approaches all seek evidence that the dark energy equation of state parameter, $w$, has not been $-1$ for all cosmic time \citep{einstein1917, einstein1917centenary}.

    \cite{georgedes5y} claims that there is a systematic offset between the distance moduli of low- and high-redshift data in the Dark Energy Survey 5-year (DES-5Y) type Ia supernovae \citep{des5y}.
    DES \citep{vincenzi} have responded to this, reporting that this claim is unsubstantiated and does not properly account for the complicated nature of type Ia supernovae standardisation.
    \cite{baovssnevidence} investigated discarding the mutual supernovae between DES-5Y and Pantheon+ \citep{pantheonplus}, then performed an offset between the low- and high-redshift supernovae in DES-5Y, and found that a cosmological constant is less strongly excluded.
    In this work, we ask: is there any \textit{Bayesian} evidence for this?

    We extend the flexknot dark energy reconstructions from \cite{paper1, paper2} with an additional low-redshift magnitude offset parameter, and investigate how this additional degree of freedom affects the Bayesian evidence for \lcdm{}, the CPL \citep{Chevallier:2000qy, Linder:2002et} parameterisation, and our free-form flexknot reconstructions.
    We will also examine how the tension ratio is affected by this offset.
    This paper is organised as follows. Section~\ref{sec:context} will outline the wider context of this work. In Section~\ref{sec:data}, the additional offset parameter in the DES-5Y supernovae likelihoods will be explained. Section~\ref{sec:methods} will offer a brief recap of the reconstruction method employed in this work, and demonstrate nested bridge sampling as an alternative method for producing posterior samples and Bayes factors. Results will be discussed in Section~\ref{sec:results}, and our conclusions in Section~\ref{sec:conclusions}.

    \section{Context}\label{sec:context}

    Type Ia supernovae are a crucial tool to search for evidence of deviation from \lcdm, and are known as ``standard candles'' --- in fact, \textit{standardisable} candles would be a more appropriate moniker, as meticulous calibration work has to be done by collaborations such as DES to create a standardised dataset suitable for use by others to constrain their favourite alternative cosmologies.

    \cite{georgedes5y} suggests that the detection of evolving dark energy by the combination of the second release DESI BAO and DES-5Y type Ia supernovae is driven by a systematic in the DES pipeline.
    In particular, it was suggested that the low-redshift supernovae included in the DES pipeline which are \textit{not} from DES's own measurements, but from the CfA \citep{Hicken2009, Hicken2012}, the Carnegie Supernova Project \citep{Krisciunas2017, Krisciunas2020} and the Foundation Sample \citep{foundation}, have an apparent magnitude $m_\mathrm B$ which is systematically 0.04 magnitudes above both the same supernovae (SNe) in Pantheon+, another supernova dataset, and the Planck best fit cosmology. 
    \cite{vincenzi} responded to this claim, citing analysis improvements compared to Pantheon+, and how different selection criteria between the two means it is expected that the datasets contain differences. 

    The DES-5Y supernova sample represents the largest single-survey dataset of its kind, with over 1600 photometrically classified SNe Ia from the DES programme.
    This sample also includes 194 SNe at low redshifts from a number of external surveys to serve as a cosmological anchor.
    Broadly, the external supernovae are at redshifts of less than $0.1$, and the DES SNe are from $0.1$ to $1.13$.
    In contrast to Pantheon+, DES-5Y presents a stronger preference for evolving dark energy.

    Supernova surveys are magnitude-limited, that is, bluer and longer events are more likely to be detected than redder, shorter events, the Malmquist bias.
    This bias must be corrected for on a supernova-by-supernova basis, which is done using simulations.
    Of course, there are differences between the Pantheon+ and DES-5Y pipelines, which are covered in detail in the appendix of \cite{vincenzi}.
    For example, it was found that replacing the \textsc{SALT3} light curve fitting model \citep{Kenworthy2021, taylor2023} of DES-5Y with the older \textsc{SALT2} \citep{salt2} model used by Pantheon+ would have halved the offset found by \cite{georgedes5y}.
    DES-5Y and Pantheon+ also use different selection functions, which means that the bias corrections \textit{should} be different, and equivalence of the ``same'' supernova event should not be expected. In fact, Pantheon+ contains a strongly biased selection of the DES-3Y supernovae: those with spectroscopic follow-up.
    \cite{baovssnevidence} compare DES-5Y and Pantheon+, with the mutual supernovae excluded, of course, but this does not properly account for selection effects. If it were to be done properly, the mutual supernovae should be deleted before bias corrections.
    The approach here suffers a similar limitation, but in lieu of a viable alternative, we proceed.

    Two things may be true at the same time. There can be a systematic issue with the low-redshift external supernovae in DES-5Y, even if expecting them to be identical to Pantheon+ is an oversimplification.
    Therefore, we seek to investigate whether there is any Bayesian evidence for such an offset \citep{bayes1763essay}.
    Deliberately, we do not include the Pantheon+ supernovae in this work.
    That way, any suggestion we find for this offset is entirely independent of comparisons between the two pipelines.\footnote{Of course, if it were not for the suggestion of an offset between the two, this work would not have been carried out. It is left as an exercise to the reader to choose an appropriate prior given they are reading this paper.}

    \section{Data}\label{sec:data}

    In this work, an agnostic approach is taken.
    Rather than relying on the -0.04 value as chosen in \citet{georgedes5y}, we add an additional parameter to our likelihood, \dmb{}, which is an offset applied only to the non-DES supernovae:
    %
    \begin{equation}
        \begin{aligned}
            \mathcal L(D | \theta) &= \frac{1}{\sqrt{|2\pi\Sigma|}}\exp - \frac 1 2 \mathbf{\Delta}^T\Sigma^{-1}\mathbf{\Delta}\text,\\
            \mathbf{\Delta} &= (\mathbfit m_\mathrm B + \mathbfit s\dmb{} - M_\mathrm B) - \mu(\mathbfit z, \theta)\text.\\
        \end{aligned}
    \end{equation}
    %
    $\mathbfit s$ is a binary selection mask which is 1 iff the corresponding supernova was not from the DES catalogue itself, and 0 if it was from DES.
    Setting $\dmb=0$ is equivalent to the standard supernova likelihood.
    The distance modulus $\mu$ is calculated from the luminosity distance:
    %
    \begin{equation}
        D_\mathrm L(z) = (1+z_\mathrm{hel})c\int_0^{z_\mathrm{HD}}\frac{\mathrm dz'}{H(z')}\text{,} \quad \mu(z) = 5\log_{10}\left(\frac{D_\mathrm L(z)}{\SI{10}{pc}}\right) \text.
    \end{equation}
    %
    If \dmb{} is unsubstantiated, then it will be Occam-penalised; if the Bayesian evidence is significantly greater with this parameter, however, then it follows that there is a systematic offset between the supernovae from DES.

    Since the absolute magnitude $M_\mathrm B$ is also a parameter to be fitted, an overall offset in $m_\mathrm B$ has no effect on cosmology, so one would obtain the same results with the opposite mask, albeit \dmb{} would have the opposite sign.
    In fact, it is possible to marginalise out \dmb{} analytically, this is discussed further in Appendix~\ref{apx:marginalisation}, but all of the results simply sample the parameter.

    For a more complete reconstruction of the expansion history, likelihoods with and without the \dmb{} offset were combined with the second major data release of DESI DR2 BAO measurements.
    Since release two is the current state of the art, we will refer to this simply as ``DESI BAO'', or just ``DESI'', for brevity.

    \section{Methods}\label{sec:methods}

    \subsection{Flexknot dark energy reconstructions}
    The ``flexknot'' approach of reconstructing the dark energy equation of state parameter is explained in detail in \cite{paper1} and \cite{paper2}, but we will outline the approach again here.
    Flexknots are a free-form, model-independent method for reconstructing one-dimensional functions, in this case, $w(a)$.
    They consist of a linear spline between $n$ nodes ``knots'', whose positions are parameters of the model to be fitted using nested sampling.
    The horizontal coordinate of the left- and rightmost knots are fixed, in this case, at zero and one respectively, and the remaining coordinates are free to vary within these bounds, with the restriction that they remain sorted.
    The number of knots $n$ is also a parameter of the model; in practice, separate nested sampling runs are performed for each $n$ using \textsc{PolyChord} \citep{polychord1, polychord2}, and the posterior of $n$ is proportional to the evidence for each run.
    This is a well-established technique across many areas of cosmology beyond the dark energy equation of state \citep{paper1, paper2, sonke, devazquez}, including the primordial power spectrum \citep{pkhandley, pkvazquez, pkknottedsky, pkcore, pkplanck13, pkplanck15}, the cosmic reionisation history \citep{flexknotreionization, heimersheimfrb}, galaxy cluster profiles \citep{flexknotclusters}, and the $\SI{21}{\centi\metre}$ signal \citep{heimersheim21cm, shen}.
    A similar approach was one of the methods used by \cite{2025RPPh...88i8401M}, though with a polynomial of degree $n-1$ through all the knots, rather than separate linear segments.
    %
    \begin{table}
        \centering
        \rowcolors{2}{}{gray!25}
        \begin{tabular}{|l|c|}
            \hline
            Parameter & Prior \\
            \hline
            \dmb{} & $[-0.1, 0.1]$ \\
            $n$ & $[1, 20]$ \\
            $a_{n-1}$ & $0$ \\
            $a_{n-2}, \dots, a_1$ & sorted($[a_{n-1}, a_0])$ \\
            $a_0$ & $1$ \\
            $w_{n-1}, \dots, w_0$ & $[-3, 1]$ \\
            $w_a$ & $[-3, 2]$, $w_0+w_a<0$ \\
            $\Omega_\mathrm m$ & $[0.01, 0.99]$ \\
            $H_0r_\mathrm d$ (DESI)& $[3650, 18250]$ \\
            $H_0$ (Ia) & $[20, 100]$ \\
            \hline
        \end{tabular}
        \caption{
            Cosmological priors used in this work.
            Fixed values are indicated by a single number, while uniform priors are denoted by brackets.
            As BAO depend only on the product $H_0r_\mathrm d$, and supernovae depend on $H_0$, the former is sampled only when DESI is included, and the latter is analytically marginalised out.
            Similarly, $w_a$ is used only for the CPL model, with the restriction that the value of $w$ is negative at early times, $w_0 + w_a<0$, as used by other work.
        }
        \label{tab:priors}
    \end{table}
    %

    Flexknots also have the advantage that $n=1$ and $n=2$ cases correspond to the $w$CDM and CPL models respectively, though, in the latter case, separate sampling runs are also performed with priors consistent with other works for completeness.
    In response to comments from presenting our previous work, one small change has been made which was previously shown in an appendix of \citet{paper2}; that is, the upper limit of the prior of the $w$-coordinates of the knots is now $1$. This is more consistent with typical priors used for CPL (e.g: \citet{planck15parameters, planck18vi, desivi, desi2i, desi2ii, desi2de}), and means that \lcdm{} is in the centre of the prior, though of course this is of no consequence for a uniform prior.
    Under each reconstruction, the Kullback-Leibler divergence (KL divergence) is shown as a function of scale factor or redshift as appropriate.
    This is a more robust way of assessing the constraining power of each dataset throughout cosmic history than simply comparing their contours.

    Figure \ref{fig:prior} shows prior samples from the usual CPL prior used in most analyses, and the equivalent flexknot prior, and the corresponding posteriors from DES-5Y combined with DESI as kernel density estimates (KDEs).
    The CPL prior includes the constraint that the value of $w$ at early times, $w_0+w_a$, is less than zero, to ensure a period of matter domination.
    Clearly, the two priors are different, but the posteriors are so similar that it is difficult to see that there are two KDEs overlaid.
    The different prior volumes will impact the evidence and tension values, in Appendix~\ref{apx:prior} it is shown that this effect is small.
    The priors used in this work are listed in Table~\ref{tab:priors}.

    %
    \begin{figure}
        \begin{center}
            \includegraphics[width=0.48\textwidth]{plots/cplprior.pdf}
        \end{center}
        \caption{
            Prior and posterior of CPL and $n=2$ flexknot, using DES-5Y combined with DESI BAO.
            The top panel shows the $(w_0, w_a)$ projection, the lower panel shows $(w_0, w_{n-1})$.
            Prior samples are shown as a scatter, the posteriors are shown as kernel density estimates.
            For reference, the cross marks \lcdm{}.
            The two posteriors are so similar that it is challenging to see one on top of the other!
        }\label{fig:prior}
    \end{figure}

    \subsection{Nested bridge sampling}

    The nested sampling approach used previously and in this work computes the Bayesian evidence for each combination of data and model.
    However, in isolation, evidences are meaningless, and it is only with a pair that one can determine a Bayes factor, to which one may apply Jeffreys' scale \citep{jeffreys1939theory} to determine how to interpret the result, or combine with a model prior to determine the posterior odds.
    An alternative method to compute the Bayes factor, beside two normal nested sampling runs, is to use nested bridge sampling.

    Nested bridge sampling (NBS, \cite{chen2000monte, bridgetutorial}, Yallup et al. (in prep) makes use of the inevitable similarity between the posteriors for the shared parameters between sampling runs with nested models.
    This work consists entirely of nested models.
    \lcdm{} is nested within $w$CDM with $w=0$, which are both nested within CPL, which are all nested within flexknot models.
    Also, each cosmological model with $\dmb{}=0$ is nested within the same model with \dmb{} varying.
    In theory, one could bridge sample from vanilla \lcdm{} to an $n=20$ flexknot with \dmb{}.
    As a demonstration, let us outline nested bridge sampling to add \dmb{} only to an existing run.

    Yallup (in prep) explains this method in detail and its application to toy and cosmological examples; let us recap the methodology here:
    The recipe is as follows: first, produce a set of samples with $\dmb{}=0$.
    Then, sample the likelihood ratio, $\tilde{\mathcal L}$, using the posterior from the first run as part of an effective prior:
    %
    \begin{equation}
        \tilde{\mathcal L}=\frac{\mathcal L(D|\theta, \dmb{})}{\mathcal L(D|\theta, \dmb{}=0)}\text, \quad 
        \tilde\pi = \pi(\dmb{}|\theta)\frac{\pi(\theta)\mathcal L(\theta)}{Z_1}\text,
        \label{eq:ratio}
    \end{equation}
    %
    where $D$ is the data (DES-5Y alone or with DESI BAO), and $\theta$ are the appropriate other parameters for the cosmology in question.
    These arise from rearranging the Bayes factor:
    %
    \begin{equation} \label{eq:ratioratio}
        \begin{aligned}
            \frac{Z_2}{Z_1} &= \frac{\int\mathcal L(\theta, \dmb{})\pi(\theta, \dmb{})\mathrm d\theta\mathrm d(\dmb{})}{Z_1} \\
            &= \int\underbrace{\frac{\mathcal L(\theta, \dmb{})}{\mathcal L(\theta)}}_{\tilde{\mathcal L}}
            \underbrace{\pi(\dmb{}|\theta)\frac{\pi(\theta)\mathcal L(\theta)}{Z_1}}_{\tilde\pi} \mathrm d\theta\mathrm d(\dmb{}) \text,
        \end{aligned}
    \end{equation}
    %
    where the data $D$ is suppressed from the likelihood for clarity, and the probability product rule has been used to factorise the prior $\pi(\theta, \dmb{}) = \pi(\dmb{}|\theta)\pi(\theta)$.
    The word ``nested'' should not be confused with the same in nested sampling, which refers to the onion of likelihood contours.
    However, nested sampling may indeed be used to perform nested bridge sampling, but one may substitute Sequential Monte Carlo \citep{smc}, which is what is used here, for both the initial sampling run and bridging, abbreviated as SMC-NBS.
    A possible disadvantage of SMC compared to nested sampling is that it does not report an error bar, the error bars here are from ten repeats with different initial random seeds.

    \subsection{\textsc{JAX} reimplementation}

    In addition to the pipeline from previous work, the likelihoods have also been ported to \textsc{JAX}.
    This allows them to be sampled using \textsc{BlackJAX} nested slice sampling, which makes use of GPU parallelisation \citep{yallup2025nested, metha, cabezas2024blackjax}.
    In fact, the $n=2$ flexknot prior and posterior shown in Figure~\ref{fig:prior} is from the \textsc{PolyChord} pipeline, which uses \textsc{numpy} and \textsc{scipy} and 64-bit floating point precision, while the CPL result uses the \textsc{JAX} pipeline, with 32-bit floating point precision.
    Reimplementation in \textsc{JAX} posed some additional challenges, for example, the adaptive \textsc{QUADPACK} method used to compute the integral over $\frac{H_0}{H(z)}$ is not available in \textsc{JAX.SCIPY}, so trapezoidal integration was substituted.
    Also, it was found that the precision setting for the Mahalanobis distance matrix multiplication had to be increased from its default level for consistency with the other pipeline.
    Therefore, it is very reassuring that the results, which differ in hardware, sampler, 1-dimensional integration technique for proper motion distance, and floating point representation, are so similar.

    One feature of \textsc{PolyChord} currently omitted from the \textsc{BlackJAX} sampler is live point clustering.
    Clustering is typically considered necessary for nested sampling with multi-modal posteriors, however, the \textsc{BlackJAX} sampler has proven surprisingly effective on other problems.
    It is the subject of ongoing research whether no clustering is genuinely a limitation, therefore, we restrict the application of the \textsc{JAX} pipeline to the unimodal \lcdm{} and CPL likelihoods only in this work.

    \section{Results}\label{sec:results}

    \begin{figure}
        \begin{center}
            \includegraphics[width=0.48\textwidth]{plots/des5yoffset_20_wa.pdf}
        \end{center}
        \caption{
            Flexknot reconstruction of the dark energy equation of state parameter using DES-5Y supernovae only.
            In red is the standard likelihood, in lilac, the version with the \dmb{} offset for the low-redshift supernovae.
            The overall shape of the reconstructions are very similar, but the functional KL divergence and model evidences tell quite different stories.
            Firstly, note that the high-$a$/low-redshift KL divergence lacks the peak just below $z=0.1$, which is to be expected as allowing those magnitudes to float up and down will naturally reduce their constraining power.
            Second, note that the evidence for \lcdm{} has increased with the offset, meanwhile, it has fallen for all flexknots with more than three knots.
            The Bayes factor between \lcdm{} and $w$CDM is similar between the two likelihoods, but $n=2$ is more disfavoured with the offset likelihood.
            Crucially, the evidence for \lcdm{} with the offset is greater than the flexknots without the offset.
            This suggests that the complexity demanded by the flexknot model is just as well, if not better, met by including this additional degree of freedom.\\
            Please note that the posteriors shown in the bottom-left panels do not include \lcdm{}, which is shown separately in Figure~\ref{fig:dmb}.
            With flexknots, \dmb{} is not well constrained.
        }\label{fig:des5y}
    \end{figure}
    
    \begin{figure}
        \begin{center}
            \includegraphics[width=0.48\textwidth]{plots/desidr2_des5yoffset_20_wa.pdf}
        \end{center}
        \caption{
            Similar to Figure~\ref{fig:des5y}, this time with the addition of DESI BAO.
            This time, it is even clearer that \lcdm{} with the \dmb{} offset is the favoured model.
            Once again, the low-redshift KL divergence peak is lost with the additional parameter.
            Unlike the results with DES-5Y alone, the Bayes factor for \lcdm{} with \dmb{} over almost any other model is ``decisive''.
            Again, please note that the posteriors shown in the bottom-left panels do not include \lcdm{}, these are shown in Figure~\ref{fig:dmb}.
        }\label{fig:desidr2des5y}
    \end{figure}

    \begin{figure}
        \begin{center}
            \includegraphics[width=0.4\textwidth]{plots/tension.pdf}
        \end{center}
        \caption{
            Tension values between DES-5Y and DESI BAO, with and without the low-redshift offset.
            \lcdm{} is shown as horizontal dashed lines, for easy comparison with the other points.
            For \lcdm{} the tension has been reduced (more positive) significantly, while for $w$CDM and CPL, it has increased slightly.
            \lcdm{} with the offset is now on-par with CPL, though $w$CDM remains the model with the best dataset concordance.
            In contrast, without the offset, \lcdm{} is the most discrepant model of all.
            For three knots and above, the tension is similar with or without the offset.
        }\label{fig:tension}
    \end{figure}

    \begin{figure}
        \begin{center}
            \includegraphics[width=0.23\textwidth]{plots/dmb_des5y.pdf}
            \includegraphics[width=0.23\textwidth]{plots/dmb_desidr2_des5y.pdf}
        \end{center}
        \caption{
            Posterior histograms of \dmb{} for \lcdm{} and CPL.
            The left panel uses DES-5Y only, the right also includes DESI BAO.
            The prior is uniform over the domain of the plot.
            As predicted by \citet{georgedes5y}, its value is centred on $-0.04$.
            Note that the \lcdm{} posteriors (and the right CPL posterior) are well contained within the prior, therefore, the evidence which would have been found had a wider prior been used can be easily be computed with the ratio of the prior volumes.
        }\label{fig:dmb}
    \end{figure}
    
    \begin{table}
        \caption{
            Values of \dmb{} for DES-5Y, along with the log-Bayes factor between the standard version ($\dmb{} = 0$) and with the offset.
            The Bayes factor is calculated in two ways, first by taking the ratio of the evidences from the nested sampling runs, and also by bridge sampling from $\dmb{}=0$ to the offset version.
            Bridge sampling is not attempted for the flexknot models, as more work is required to determine if this is viable.
            The shown nested sampling Bayes factors are from the \textsc{JAX} pipeline, which are consistent with the \textsc{PolyChord} pipeline, which are not reported here.\\
            Both \lcdm{} values of \dmb{} are consistent with $-0.04$, and inconsistent with zero to over $2\sigma$.
            In contrast, both CPL and flexknot dark energy do not exclude zero, this is reflected in the Bayes factors.
            Of particular note is the large positive log-Bayes factor for \lcdm{} with DES-5Y + DESI BAO in favour of a low-redshift supernova offset.
            The Bayes factors from SMC-NBS are beyond error of those computed from two nested sampling runs, but not so much to affect any conclusions.
        }\label{tab:des5y}
        \begin{center}
            \begin{tabular}[c]{|l|c|c|c|}
                \hline
                \multicolumn{4}{|c|}{DES-5Y}\\
                \hline
                & \dmb{} & $\log(\text{Bayes factor})$ & SMC-NBS \\
                \hline
                \lcdm{} & $-0.035 \pm 0.016$ & $0.755 \pm 0.098$ & $0.665 \pm 0.030$ \\
                CPL & $-0.042 \pm 0.028$ & $-0.067 \pm 0.127$ & $0.086 \pm 0.109$ \\
                flexknot & $-0.018 \pm 0.040$ & $-0.525 \pm 0.021$ & N/A \\
                \hline
                \multicolumn{4}{|c|}{DES-5Y + DESI BAO}\\
                \hline
                \lcdm{} & $-0.045 \pm 0.012$ & $4.140 \pm 0.182$ & $4.488 \pm 0.037$ \\
                CPL & $-0.022 \pm 0.026$ & $-0.859 \pm 0.217$ & $-0.738 \pm 0.018$ \\
                flexknot & $-0.017 \pm 0.038$ & $-0.396 \pm 0.055$ & N/A \\
                \hline
            \end{tabular}
        \end{center}
    \end{table}

    \subsection{Flexknot reconstructions}
    We begin with DES-5Y supernovae alone.
    Figure~\ref{fig:des5y} shows the flexknot reconstruction of $w(a)$ both with and without the low-redshift offset.
    From the KL divergence panels, it can be seen that the constraining power at low redshifts is reduced with the additional parameter; this is to be expected.
    The model with the greatest evidence is \lcdm{} with the offset.
    This suggests that the complexity picked up by the flexknot model, which brings models with four or more knots in line with \lcdm{}, is better explained by \dmb{}.
    Only \lcdm{}, $w$CDM and CPL ($n=1$ and $n=2$ respectively) have greater evidence with the introduction of the offset; the reverse is true for three or more knots.
    However, at least with DES-5Y alone, these suggestions must be caveated by the relatively small log-Bayes factors between evidences with the same $n$, which are at most around $0.7$.
    In fact, Jeffreys' scale suggests that these differences are ``barely worth mentioning''.
    We will return to this in Section~\ref{sec:priorwidth}.

    When the DESI data are included, the combined reconstruction is shown in Figure~\ref{fig:desidr2des5y}.
    With the BAOs, \lcdm{} with \dmb{} is still the favoured model, this time, with a log-Bayes factor of around four over almost all other models, crucially including standard \lcdm{}.
    The offset has also removed the preference for low numbers of flexknots.
    This strongly suggests that the original preference for dynamical dark energy can be better accounted for by this offset than it can by $w$CDM, CPL, or even flexknots.

    The \textsc{JAX} pipeline produced \lcdm{} and CPL evidences in perfect agreement with those from \textsc{PolyChord}; in fact, it is those reported in Table~\ref{tab:des5y}.
    These nested sampling runs completed in less than a second on a Google Compute Engine Nvidia L4 GPU, plus a few seconds of compile time.
    This compares very well to tens of minutes on a 76-core CSD3 CPU, plus submission delay, and has the potential to transform the typical Bayesian's workflow.

    It is important to assess how \dmb{} may affect the possibility of tension between DES-5Y and DESI BAO.
    Once again, we follow \cite{paper1}, using the techniques developed in \cite{lemos, hergt, balancingact} via the $\log R$ statistic.
    The results are shown in Figure~\ref{fig:tension}.
    Without the offset, \lcdm{} is the most discrepant model, with the only negative $\log R$.
    With the offset, \lcdm{} is now on par with CPL, while $w$CDM remains the model with the best concordance between the two datasets.
    For three knots and above, the tension is similar with or without the offset.
    This is reassuring, as it suggests that the offset is better able to explain the discrepancy between the datasets than flexknot dark energy.

    \subsection{Effect of \dmb{} prior width}\label{sec:priorwidth}
    
    From Figure~\ref{fig:dmb}, it can be seen that, for \lcdm{}, the posterior for \dmb{} is reasonably well contained within our chosen prior.
    This allows us to examine the Bayes factors in more detail.

    The uniform prior contains a factor of the reciprocal of the prior volume $V$, which, in the well contained case, persists into the Bayesian evidence.
    This means that the evidence that \textit{would} have been found for a different, wider, uniform prior can be calculated as:
    %
    \begin{equation}
        \log Z \rightarrow \log\left(Z\times\frac{V_\text{narrow}}{V_\text{wide}}\right) = \log Z - \log\frac{V_\text{wide}}{V_\text{narrow}}\text.
        \label{eq:}
    \end{equation}
    %
    Our prior of $\dmb\in[-0.1, 0.1]$ was chosen to be relatively narrow for two reasons: firstly, the nested sampling runtime is proportional to the KL divergence from prior to posterior; second, if the low-redshift systematic offset does exist and had a value beyond this range, it is unlikely that it would fall to third parties to investigate it, and frankly this should impact on our model prior.
    For example, let us examine our conclusions if a prior ten times wider, $[-1.0, 1.0]$, had been chosen.
    This would reduce log-Bayes factors by approximately $\log 10 = 2.30$.
    Consider DES-5Y alone: the only positive log-Bayes factor is \lcdm{} at around $0.76$, this would be reversed with this more liberal prior.
    In fact, we may reverse-engineer this argument to note that, had a prior $e^{0.76}\approx2.14$ times larger been chosen, the log-Bayes factor would have been precisely zero.
    One may rightly criticise that post-hoc revisions to the prior are unwise, however, if this were all the data we had, it seems this would be enough to suggest that there is no practical evidence for the low-redshift offset, since it is so vulnerable to somewhat reasonable alternative priors.

    However, now reintroduce the DESI BAO, with a log-Bayes factor of $\log Z = 4.140\pm0.127$ in favour of \dmb{}.
    This time, one would have needed a prior at least sixty-two times wider to reverse the conclusion --- an offset of such magnitude is completely unreasonable.
    Therefore, we will conclude that there is only evidence for the low-redshift supernova offset in DES-5Y if, and only if, it is combined with baryon acoustic oscillations, and \lcdm{} is correct.

    \subsection{Nested bridge sampling Bayes factors}

    Table~\ref{tab:des5y} also includes the SMC-NBS Bayes factors.
    Error bars were estimated by running SMC-NBS ten times with different random seeds, while those from nested sampling were computed using \textsc{anesthetic}, which samples possible nested sampling volume compression histories \citep{anesthetic}.
    It is interesting that all four of the NBS error bars are tighter than those from nested sampling.
    The different approaches, in some cases, like outside the error of each other, though not so significantly as to affect any conclusions.
    For example, it is not surprising that the very small log-Bayes factors with CPL DES-5Y have the opposite signs.
    Crucially, the Bayes factor of the most interest, which happens to be the largest, of \dmb{} or no \dmb{} with DESI in \lcdm, is similarly large with NBS.
    The SMC implementation also uses \textsc{BlackJAX}, and similarly takes mere seconds to run.
    
    \section{Conclusions}\label{sec:conclusions}

    In this work, it has been found that there is substantial Bayesian evidence for a low-redshift supernovae systematic in DES-5Y, when it is combined with DESI BAO.
    However, without the BAO, the claim is unsubstantiated, though the posterior on the offset value indeed agrees with the $-0.04$ of \citet{georgedes5y} and excludes zero to $2\sigma$.
    Crucially, the Bayesian evidence favours offset \lcdm{} over flexknot dark energy with or even without the offset, so we must conclude that the systematic is a better model than dynamical dark energy.
    We accept that this approach is limited, and adjusting a subset of apparent magnitudes post bias correction does not constitute a sensible supernova catalogue.
    Nevertheless, the options are clear: either there is a systematic issue with the DES-5Y supernovae, or dark energy really is dynamical.

    It has also been found that the \lcdm{} tension between DES-5Y and DESI BAO is significantly reduced with \dmb{}, while $w$CDM and CPL tensions are slightly increased.
    $w$CDM still remains the model with the best agreement.

    The alternative \textsc{JAX} pipeline has produced \lcdm{} and CPL posterior samples and evidences in excellent agreement with those from our original \textsc{PolyChord}-powered approach.
    The reduced sampling time to mere seconds has the potential to be transformative for the Bayesian workflow.
    We leave it for future work to investigate whether these tools are robust for the more challenging multi-modal posterior of flexknot reconstructions.
    We also find that Bayes factors obtained using nested bridge sampling are sufficiently similar to those obtained by the ratio of nested sampling evidences. Therefore, we hope that this approach can be trialled more widely.

    \section*{Acknowledgements}

    A.N.~Ormondroyd and W.J.~Handley were supported by the research environment and infrastructure of the Handley Lab at the University of Cambridge.
    This work was performed using the Cambridge Service for Data Driven Discovery (CSD3), part of which is operated by the University of Cambridge Research Computing on behalf of the STFC DiRAC HPC Facility (\url{www.dirac.ac.uk}).
    The DiRAC component of CSD3 was funded by BEIS capital funding via STFC capital grants ST/P002307/1 and ST/R002452/1 and STFC operations grant ST/R00689X/1.
    DiRAC is part of the National e-Infrastructure.
    WJH was supported by a Royal Society University Research Fellowship.
    The authors thank Toby Lovick for useful correspondence regarding SMC-NBS.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section*{Software and Data Availability}

    The \textsc{python} pipeline in this work made use of \textsc{NumPy} \citep{numpy}, \textsc{SciPy} \citep{scipy}, and \textsc{pandas} \citep{pandaszenodo, pandaspaper}.
    The GPU-accelerated pipeline was written in \textsc{JAX} \citep{jax2018github}, and supported by the Google Cloud research credits program, with the award GCP397499138.
    The nested sampling chains were analysed using \textsc{anesthetic} \citep{anesthetic}; plots were produced in \textsc{matplotlib} \citep{matplotlib}, using the \textsc{smplotlib} template created by \citet{smplotlib}.
    The \textsc{Python} and \textsc{JAX} pipelines and nested sampling chains used in this work can be obtained from Zenodo \citep{zenodo}.

    %%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

    % The best way to enter references is to use BibTeX:

    \bibliographystyle{mnras}
    \bibliography{desi3} % if your bibtex file is called example.bib

    % Alternatively you could enter them by hand, like this:
    % This method is tedious and prone to error if you have lots of references
    %\begin{thebibliography}{99}
    %\bibitem[\protect\citeauthoryear{Author}{2012}]{Author2012}
    %Author A.~N., 2013, Journal of Improbable Astronomy, 1, 1
    %\bibitem[\protect\citeauthoryear{Others}{2013}]{Others2013}
    %Others S., 2012, Journal of Interesting Stuff, 17, 198
    %\end{thebibliography}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    %%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%


    \appendix

    \section{Analytic marginalisation over \dmb{}}\label{apx:marginalisation}

    This appendix contains an extension to the analytical marginalisation in \cite{paper1} and \cite{paper2}.
    In those works, it was demonstrated that the constant offset from $M_\mathrm B$ could be analytically marginalised from the SNe likelihoods.
    Since this value affects all terms in the data vector, there is essentially an accompanying mask of all ones.
    In fact, the algebra is identical for any mask, assuming that the likelihood tends to zero at the limits of the prior, so \dmb{} could be marginalised over by replacing the covariance matrix:
    %
    \begin{equation}
        \Sigma^{-1} \rightarrow \Sigma^{-1} - \frac{\Sigma^{-1} \mathbfit s^T\mathbfit s\Sigma^{-1}}{\mathbfit s^T\Sigma^{-1}\mathbfit s} \text,
    \end{equation}
    %
    and adjusting the normalisation by a factor of $\frac 1 {V_\mathbfit s}\sqrt{\frac{2\pi}{\mathbfit s^T\Sigma\mathbfit s}}$, where $V_\mathbfit s$ is the prior volume of the offset, \dmb{} in this case.
    However, this introduces two challenges.
    First, each one of these marginalisations introduces an additional zero eigenvalue to the inverse covariance matrix, reducing its rank by one.
    This introduces precision issues, in particular when working in 32-bit floating point.
    Secondly, the posterior of \dmb{} is of interest, and while it is possible to then sample it from the posterior of the other parameters, it is more straightforward to simply include it as a parameter during nested sampling, as this poses no challenge for our nested slice sampling tools.

    \section{Effect of CPL prior on evidence and tension}\label{apx:prior}

    This appendix investigates the impact of the difference between the flexknot and CPL priors on their Bayesian evidences and the tension ratio.
    The Bayesian evidence may be split into two terms, the average log-likelihood over the posterior, and the KL divergence from prior to posterior:
    %
    \begin{equation}
        \log Z = \langle\log\mathcal L\rangle_\mathcal P - \mathcal D_\mathrm{KL}(\mathcal P||\pi)\text.
    \end{equation}
    %
    In the uniform prior case, the KL divergence may be further simplified to:
    %
    \begin{equation}
        \begin{aligned}
            \mathcal D_\mathrm{KL}(\mathcal P||\pi) &= \int \mathcal P(\theta)\log\frac{\mathcal P(\theta)}{\pi(\theta)}\mathrm d\theta\\
            &= \int\mathcal P(\theta)\log\mathcal P(\theta)\mathrm d\theta - \int\mathcal P(\theta)\log\pi(\theta)\mathrm d\theta\\
            &= \int\mathcal P(\theta)\log\mathcal P(\theta)\mathrm d\theta + \log V_\pi\text,
        \end{aligned}
    \end{equation}
    %
    where $V_\pi$ is the prior volume.
    Assume that the two posteriors are very similar, as is the case for CPL and $n=2$ flexknots, then the posterior-averaged log-likelihoods will be approximately equal.
    From Table~\ref{tab:priors}, the flexknot prior volume is 16, while the CPL prior volume is 15.5.
    Therefore, the CPL log-evidences are expected to be $\log\frac{16}{15.5}\approx0.03$ greater than those for $n=2$ flexknots.

    Consider the expression for the tension ratio:
    %
    \begin{equation}
            \log R = \log Z_\mathrm{SNe+BAO} - \log Z_\mathrm{SNe} - \log Z_\mathrm{BAO}\text.
    \end{equation}
    %
    It can be seen that the tension ratio for CPL is expected to be approximately $0.03$ less than the equivalent flexknot.
    This difference is not negligible, but small compared to the sampling uncertainty in the evidences themselves, see Table~\ref{tab:des5y} and Figure~\ref{fig:tension} for examples.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    % Don't change these lines
    \bsp	% typesetting comment
    \label{lastpage}
\end{document}

% End of mnras_template.tex

```

4. **Bibliographic Information:**
```bbl
\begin{thebibliography}{}
\makeatletter
\relax
\def\mn@urlcharsother{\let\do\@makeother \do\$\do\&\do\#\do\^\do\_\do\%\do\~}
\def\mn@doi{\begingroup\mn@urlcharsother \@ifnextchar [ {\mn@doi@}
  {\mn@doi@[]}}
\def\mn@doi@[#1]#2{\def\@tempa{#1}\ifx\@tempa\@empty \href
  {http://dx.doi.org/#2} {doi:#2}\else \href {http://dx.doi.org/#2} {#1}\fi
  \endgroup}
\def\mn@eprint#1#2{\mn@eprint@#1:#2::\@nil}
\def\mn@eprint@arXiv#1{\href {http://arxiv.org/abs/#1} {{\tt arXiv:#1}}}
\def\mn@eprint@dblp#1{\href {http://dblp.uni-trier.de/rec/bibtex/#1.xml}
  {dblp:#1}}
\def\mn@eprint@#1:#2:#3:#4\@nil{\def\@tempa {#1}\def\@tempb {#2}\def\@tempc
  {#3}\ifx \@tempc \@empty \let \@tempc \@tempb \let \@tempb \@tempa \fi \ifx
  \@tempb \@empty \def\@tempb {arXiv}\fi \@ifundefined
  {mn@eprint@\@tempb}{\@tempb:\@tempc}{\expandafter \expandafter \csname
  mn@eprint@\@tempb\endcsname \expandafter{\@tempc}}}

\bibitem[\protect\citeauthoryear{{Aslanyan}, {Price}, {Abazajian}  \&
  {Easther}}{{Aslanyan} et~al.}{2014}]{pkknottedsky}
{Aslanyan} G.,  {Price} L.~C.,  {Abazajian} K.~N.,   {Easther} R.,  2014,
  \mn@doi [\jcap] {10.1088/1475-7516/2014/08/052}, \href
  {https://ui.adsabs.harvard.edu/abs/2014JCAP...08..052A} {2014, 052}

\bibitem[\protect\citeauthoryear{Bayes}{Bayes}{1763}]{bayes1763essay}
Bayes T.,  1763, {Philosophical Transactions of the Royal Society of London},
  53, 370

\bibitem[\protect\citeauthoryear{Bradbury et~al.,}{Bradbury
  et~al.}{2018}]{jax2018github}
Bradbury J.,  et~al., 2018, {JAX}: composable transformations of
  {P}ython+{N}um{P}y programs, \url {http://github.com/jax-ml/jax}

\bibitem[\protect\citeauthoryear{{Brout} et~al.,}{{Brout}
  et~al.}{2022}]{pantheonplus}
{Brout} D.,  et~al., 2022, \mn@doi [\apj] {10.3847/1538-4357/ac8e04}, \href
  {https://ui.adsabs.harvard.edu/abs/2022ApJ...938..110B} {938, 110}

\bibitem[\protect\citeauthoryear{Cabezas, Corenflos, Lao  \& Louf}{Cabezas
  et~al.}{2024}]{cabezas2024blackjax}
Cabezas A.,  Corenflos A.,  Lao J.,   Louf R.,  2024, BlackJAX: Composable
  {B}ayesian inference in {JAX} (\mn@eprint {arXiv} {2402.10797})

\bibitem[\protect\citeauthoryear{Chen, Shao  \& Ibrahim}{Chen
  et~al.}{2000}]{chen2000monte}
Chen M.-H.,  Shao Q.-M.,   Ibrahim J.~G.,  2000, Monte Carlo Methods in
  Bayesian Computation, 1 edn.
Springer Series in Statistics, Springer New York, NY,
  \mn@doi{10.1007/978-1-4612-1276-8}

\bibitem[\protect\citeauthoryear{Chevallier \& Polarski}{Chevallier \&
  Polarski}{2001}]{Chevallier:2000qy}
Chevallier M.,  Polarski D.,  2001, \mn@doi [Int. J. Mod. Phys. D]
  {10.1142/S0218271801000822}, 10, 213

\bibitem[\protect\citeauthoryear{{DES Collaboration} et~al.,}{{DES
  Collaboration} et~al.}{2024}]{des5y}
{DES Collaboration} et~al., 2024, \mn@doi [\apjl] {10.3847/2041-8213/ad6f9f},
  \href {https://ui.adsabs.harvard.edu/abs/2024ApJ...973L..14D} {973, L14}

\bibitem[\protect\citeauthoryear{{DESI Collaboration} et~al.,}{{DESI
  Collaboration} et~al.}{2024}]{desivi}
{DESI Collaboration} et~al., 2024, \mn@doi [arXiv e-prints]
  {10.48550/arXiv.2404.03002}, \href
  {https://ui.adsabs.harvard.edu/abs/2024arXiv240403002D} {p. arXiv:2404.03002}

\bibitem[\protect\citeauthoryear{{DESI Collaboration} et~al.,}{{DESI
  Collaboration} et~al.}{2025a}]{desi2ii}
{DESI Collaboration} et~al., 2025a, arXiv e-prints, \href
  {https://ui.adsabs.harvard.edu/abs/2025arXiv250314738D} {p. arXiv:2503.14738}

\bibitem[\protect\citeauthoryear{{DESI Collaboration} et~al.,}{{DESI
  Collaboration} et~al.}{2025b}]{desi2i}
{DESI Collaboration} et~al., 2025b, arXiv e-prints, \href
  {https://ui.adsabs.harvard.edu/abs/2025arXiv250314739D} {p. arXiv:2503.14739}

\bibitem[\protect\citeauthoryear{{DESI Collaboration} et~al.,}{{DESI
  Collaboration} et~al.}{2025c}]{desi2de}
{DESI Collaboration} et~al., 2025c, arXiv e-prints, \href
  {https://ui.adsabs.harvard.edu/abs/2025arXiv250314743D} {p. arXiv:2503.14743}

\bibitem[\protect\citeauthoryear{{Efstathiou}}{{Efstathiou}}{2024}]{georgedes5y}
{Efstathiou} G.,  2024, \mn@doi [arXiv e-prints] {10.48550/arXiv.2408.07175},
  \href {https://ui.adsabs.harvard.edu/abs/2024arXiv240807175E} {p.
  arXiv:2408.07175}

\bibitem[\protect\citeauthoryear{{Einstein}}{{Einstein}}{1917}]{einstein1917}
{Einstein} A.,  1917, Sitzungsberichte der K\&ouml;niglich Preussischen
  Akademie der Wissenschaften, \href
  {https://ui.adsabs.harvard.edu/abs/1917SPAW.......142E} {pp 142--152}

\bibitem[\protect\citeauthoryear{{Finelli} et~al.,}{{Finelli}
  et~al.}{2018}]{pkcore}
{Finelli} F.,  et~al., 2018, \mn@doi [\jcap] {10.1088/1475-7516/2018/04/016},
  \href {https://ui.adsabs.harvard.edu/abs/2018JCAP...04..016F} {2018, 016}

\bibitem[\protect\citeauthoryear{Foley et~al.,}{Foley
  et~al.}{2017}]{foundation}
Foley R.~J.,  et~al., 2017, \mn@doi [Monthly Notices of the Royal Astronomical
  Society] {10.1093/mnras/stx3136}, 475, 193

\bibitem[\protect\citeauthoryear{{Gronau} et~al.,}{{Gronau}
  et~al.}{2017}]{bridgetutorial}
{Gronau} Q.~F.,  et~al., 2017, \mn@doi [arXiv e-prints]
  {10.48550/arXiv.1703.05984}, \href
  {https://ui.adsabs.harvard.edu/abs/2017arXiv170305984G} {p. arXiv:1703.05984}

\bibitem[\protect\citeauthoryear{{Guy} et~al.,}{{Guy} et~al.}{2007}]{salt2}
{Guy} J.,  et~al., 2007, \mn@doi [\aap] {10.1051/0004-6361:20066930}, \href
  {https://ui.adsabs.harvard.edu/abs/2007A&A...466...11G} {466, 11}

\bibitem[\protect\citeauthoryear{Handley}{Handley}{2019}]{anesthetic}
Handley W.,  2019, \mn@doi [The Journal of Open Source Software]
  {10.21105/joss.01414}, 4, 1414

\bibitem[\protect\citeauthoryear{{Handley} \& {Lemos}}{{Handley} \&
  {Lemos}}{2019}]{lemos}
{Handley} W.,  {Lemos} P.,  2019, \mn@doi [\prd] {10.1103/PhysRevD.100.043504},
  \href {https://ui.adsabs.harvard.edu/abs/2019PhRvD.100d3504H} {100, 043504}

\bibitem[\protect\citeauthoryear{{Handley}, {Hobson}  \& {Lasenby}}{{Handley}
  et~al.}{2015a}]{polychord1}
{Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,  2015a, \mn@doi [\mnras]
  {10.1093/mnrasl/slv047}, \href
  {https://ui.adsabs.harvard.edu/abs/2015MNRAS.450L..61H} {450, L61}

\bibitem[\protect\citeauthoryear{{Handley}, {Hobson}  \& {Lasenby}}{{Handley}
  et~al.}{2015b}]{polychord2}
{Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,  2015b, \mn@doi [\mnras]
  {10.1093/mnras/stv1911}, \href
  {https://ui.adsabs.harvard.edu/abs/2015MNRAS.453.4384H} {453, 4384}

\bibitem[\protect\citeauthoryear{{Handley}, {Lasenby}, {Peiris}  \&
  {Hobson}}{{Handley} et~al.}{2019}]{pkhandley}
{Handley} W.~J.,  {Lasenby} A.~N.,  {Peiris} H.~V.,   {Hobson} M.~P.,  2019,
  \mn@doi [\prd] {10.1103/PhysRevD.100.103511}, \href
  {https://ui.adsabs.harvard.edu/abs/2019PhRvD.100j3511H} {100, 103511}

\bibitem[\protect\citeauthoryear{Harris et~al.,}{Harris et~al.}{2020}]{numpy}
Harris C.~R.,  et~al., 2020, \mn@doi [Nature] {10.1038/s41586-020-2649-2}, 585,
  357

\bibitem[\protect\citeauthoryear{{Hee}, {Handley}, {Hobson}  \&
  {Lasenby}}{{Hee} et~al.}{2016}]{sonke}
{Hee} S.,  {Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,  2016, \mn@doi
  [\mnras] {10.1093/mnras/stv2217}, \href
  {https://ui.adsabs.harvard.edu/abs/2016MNRAS.455.2461H} {455, 2461}

\bibitem[\protect\citeauthoryear{{Heimersheim}, {Sartorio}, {Fialkov}  \&
  {Lorimer}}{{Heimersheim} et~al.}{2022}]{heimersheimfrb}
{Heimersheim} S.,  {Sartorio} N.~S.,  {Fialkov} A.,   {Lorimer} D.~R.,  2022,
  \mn@doi [\apj] {10.3847/1538-4357/ac70c9}, \href
  {https://ui.adsabs.harvard.edu/abs/2022ApJ...933...57H} {933, 57}

\bibitem[\protect\citeauthoryear{{Heimersheim}, {R{\o}nneberg}, {Linton},
  {Pagani}  \& {Fialkov}}{{Heimersheim} et~al.}{2024}]{heimersheim21cm}
{Heimersheim} S.,  {R{\o}nneberg} L.,  {Linton} H.,  {Pagani} F.,   {Fialkov}
  A.,  2024, \mn@doi [\mnras] {10.1093/mnras/stad3936}, \href
  {https://ui.adsabs.harvard.edu/abs/2024MNRAS.52711404H} {527, 11404}

\bibitem[\protect\citeauthoryear{{Hergt}, {Handley}, {Hobson}  \&
  {Lasenby}}{{Hergt} et~al.}{2021}]{hergt}
{Hergt} L.~T.,  {Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,  2021,
  \mn@doi [\prd] {10.1103/PhysRevD.103.123511}, \href
  {https://ui.adsabs.harvard.edu/abs/2021PhRvD.103l3511H} {103, 123511}

\bibitem[\protect\citeauthoryear{Hicken et~al.,}{Hicken
  et~al.}{2009}]{Hicken2009}
Hicken M.,  et~al., 2009, \mn@doi [The Astrophysical Journal]
  {10.1088/0004-637X/700/1/331}, 700, 331

\bibitem[\protect\citeauthoryear{Hicken et~al.,}{Hicken
  et~al.}{2012}]{Hicken2012}
Hicken M.,  et~al., 2012, \mn@doi [The Astrophysical Journal Supplement Series]
  {10.1088/0067-0049/200/2/12}, 200, 12

\bibitem[\protect\citeauthoryear{Hunter}{Hunter}{2007}]{matplotlib}
Hunter J.~D.,  2007, \mn@doi [Computing in Science \& Engineering]
  {10.1109/MCSE.2007.55}, 9, 90

\bibitem[\protect\citeauthoryear{Jeffreys}{Jeffreys}{1939}]{jeffreys1939theory}
Jeffreys H.,  1939, The Theory of Probability, second edn.
Clarendon Press, Oxford

\bibitem[\protect\citeauthoryear{Kenworthy et~al.,}{Kenworthy
  et~al.}{2021}]{Kenworthy2021}
Kenworthy W.~D.,  et~al., 2021, \mn@doi [The Astrophysical Journal]
  {10.3847/1538-4357/ac30d8}, 923, 265

\bibitem[\protect\citeauthoryear{Krisciunas et~al.,}{Krisciunas
  et~al.}{2017}]{Krisciunas2017}
Krisciunas K.,  et~al., 2017, \mn@doi [The Astronomical Journal]
  {10.3847/1538-3881/aa9a3d}, 154, 278

\bibitem[\protect\citeauthoryear{Krisciunas et~al.,}{Krisciunas
  et~al.}{2020}]{Krisciunas2020}
Krisciunas K.,  et~al., 2020, \mn@doi [The Astronomical Journal]
  {10.3847/1538-3881/abc431}, 160, 289

\bibitem[\protect\citeauthoryear{Li}{Li}{2023}]{smplotlib}
Li J.,  2023, AstroJacobLi/smplotlib: v0.0.9, \mn@doi{10.5281/zenodo.8126529},
  \url {https://doi.org/10.5281/zenodo.8126529}

\bibitem[\protect\citeauthoryear{Linder}{Linder}{2003}]{Linder:2002et}
Linder E.~V.,  2003, \mn@doi [Phys. Rev. Lett.]
  {10.1103/PhysRevLett.90.091301}, 90, 091301

\bibitem[\protect\citeauthoryear{{M}c{K}inney}{{M}c{K}inney}{2010}]{pandaspaper}
{M}c{K}inney W.,  2010, in {S}t\'efan van~der {W}alt {J}arrod {M}illman eds,
  {P}roceedings of the 9th {P}ython in {S}cience {C}onference. pp 56 -- 61,
  \mn@doi{10.25080/Majora-92bf1922-00a}

\bibitem[\protect\citeauthoryear{{Millea} \& {Bouchet}}{{Millea} \&
  {Bouchet}}{2018}]{flexknotreionization}
{Millea} M.,  {Bouchet} F.,  2018, \mn@doi [\aap]
  {10.1051/0004-6361/201833288}, \href
  {https://ui.adsabs.harvard.edu/abs/2018A&A...617A..96M} {617, A96}

\bibitem[\protect\citeauthoryear{{Mukherjee} \& {Sen}}{{Mukherjee} \&
  {Sen}}{2025}]{2025RPPh...88i8401M}
{Mukherjee} P.,  {Sen} A.~A.,  2025, \mn@doi [Reports on Progress in Physics]
  {10.1088/1361-6633/ae082c}, \href
  {https://ui.adsabs.harvard.edu/abs/2025RPPh...88i8401M} {88, 098401}

\bibitem[\protect\citeauthoryear{{Naesseth}, {Lindsten}  \&
  {Sch{\"o}n}}{{Naesseth} et~al.}{2019}]{smc}
{Naesseth} C.~A.,  {Lindsten} F.,   {Sch{\"o}n} T.~B.,  2019, \mn@doi [arXiv
  e-prints] {10.48550/arXiv.1903.04797}, \href
  {https://ui.adsabs.harvard.edu/abs/2019arXiv190304797N} {p. arXiv:1903.04797}

\bibitem[\protect\citeauthoryear{{Notari}, {Redi}  \& {Tesi}}{{Notari}
  et~al.}{2025}]{baovssnevidence}
{Notari} A.,  {Redi} M.,   {Tesi} A.,  2025, \mn@doi [\jcap]
  {10.1088/1475-7516/2025/04/048}, \href
  {https://ui.adsabs.harvard.edu/abs/2025JCAP...04..048N} {2025, 048}

\bibitem[\protect\citeauthoryear{{O'Raifeartaigh}, {O'Keeffe}, {Nahm}  \&
  {Mitton}}{{O'Raifeartaigh} et~al.}{2017}]{einstein1917centenary}
{O'Raifeartaigh} C.,  {O'Keeffe} M.,  {Nahm} W.,   {Mitton} S.,  2017, \mn@doi
  [European Physical Journal H] {10.1140/epjh/e2017-80002-5}, \href
  {https://ui.adsabs.harvard.edu/abs/2017EPJH...42..431O} {42}

\bibitem[\protect\citeauthoryear{{Olamaie}, {Hobson}, {Feroz}, {Grainge},
  {Lasenby}, {Perrott}, {Rumsey}  \& {Saunders}}{{Olamaie}
  et~al.}{2018}]{flexknotclusters}
{Olamaie} M.,  {Hobson} M.~P.,  {Feroz} F.,  {Grainge} K. J.~B.,  {Lasenby} A.,
   {Perrott} Y.~C.,  {Rumsey} C.,   {Saunders} R. D.~E.,  2018, \mn@doi
  [\mnras] {10.1093/mnras/sty2495}, \href
  {https://ui.adsabs.harvard.edu/abs/2018MNRAS.481.3853O} {481, 3853}

\bibitem[\protect\citeauthoryear{Ormondroyd}{Ormondroyd}{2025}]{zenodo}
Ormondroyd A.,  2025, {Dynamic or Systematic? Bayesian model selection between
  dark energy and supernova biases}, \mn@doi{10.5281/zenodo.17130553}, \url
  {https://doi.org/10.5281/zenodo.17130553}

\bibitem[\protect\citeauthoryear{{Ormondroyd}, {Handley}, {Hobson}  \&
  {Lasenby}}{{Ormondroyd} et~al.}{2023}]{balancingact}
{Ormondroyd} A.~N.,  {Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,
  2023, \mn@doi [arXiv e-prints] {10.48550/arXiv.2310.08490}, \href
  {https://ui.adsabs.harvard.edu/abs/2023arXiv231008490O} {p. arXiv:2310.08490}

\bibitem[\protect\citeauthoryear{{Ormondroyd}, {Handley}, {Hobson}  \&
  {Lasenby}}{{Ormondroyd} et~al.}{2025a}]{paper2}
{Ormondroyd} A.~N.,  {Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,
  2025a, \mn@doi [arXiv e-prints] {10.48550/arXiv.2503.17342}, \href
  {https://ui.adsabs.harvard.edu/abs/2025arXiv250317342O} {p. arXiv:2503.17342}

\bibitem[\protect\citeauthoryear{{Ormondroyd}, {Handley}, {Hobson}  \&
  {Lasenby}}{{Ormondroyd} et~al.}{2025b}]{paper1}
{Ormondroyd} A.~N.,  {Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,
  2025b, \mn@doi [\mnras] {10.1093/mnras/staf1144}, \href
  {https://ui.adsabs.harvard.edu/abs/2025MNRAS.541.3388O} {541, 3388}

\bibitem[\protect\citeauthoryear{{Perlmutter} et~al.,}{{Perlmutter}
  et~al.}{1999}]{SupernovaCosmologyProject:1998}
{Perlmutter} S.,  et~al., 1999, \mn@doi [\apj] {10.1086/307221}, \href
  {https://ui.adsabs.harvard.edu/abs/1999ApJ...517..565P} {517, 565}

\bibitem[\protect\citeauthoryear{{Planck Collaboration} et~al.,}{{Planck
  Collaboration} et~al.}{2014}]{pkplanck13}
{Planck Collaboration} et~al., 2014, \mn@doi [\aap]
  {10.1051/0004-6361/201321569}, \href
  {https://ui.adsabs.harvard.edu/abs/2014A&A...571A..22P} {571, A22}

\bibitem[\protect\citeauthoryear{{Planck Collaboration} et~al.,}{{Planck
  Collaboration} et~al.}{2016a}]{planck15parameters}
{Planck Collaboration} et~al., 2016a, \mn@doi [\aap]
  {10.1051/0004-6361/201525830}, \href
  {https://ui.adsabs.harvard.edu/abs/2016A&A...594A..13P} {594, A13}

\bibitem[\protect\citeauthoryear{{Planck Collaboration} et~al.,}{{Planck
  Collaboration} et~al.}{2016b}]{pkplanck15}
{Planck Collaboration} et~al., 2016b, \mn@doi [\aap]
  {10.1051/0004-6361/201525898}, \href
  {https://ui.adsabs.harvard.edu/abs/2016A&A...594A..20P} {594, A20}

\bibitem[\protect\citeauthoryear{{Planck Collaboration} et~al.,}{{Planck
  Collaboration} et~al.}{2021}]{planck18vi}
{Planck Collaboration} et~al., 2021, \mn@doi [\aap]
  {10.1051/0004-6361/201833910e}, \href
  {https://ui.adsabs.harvard.edu/abs/2021A&A...652C...4P} {652, C4}

\bibitem[\protect\citeauthoryear{{Prathaban}, {Yallup}, {Alvey}, {Yang},
  {Templeton}  \& {Handley}}{{Prathaban} et~al.}{2025}]{metha}
{Prathaban} M.,  {Yallup} D.,  {Alvey} J.,  {Yang} M.,  {Templeton} W.,
  {Handley} W.,  2025, \mn@doi [arXiv e-prints] {10.48550/arXiv.2509.04336},
  \href {https://ui.adsabs.harvard.edu/abs/2025arXiv250904336P} {p.
  arXiv:2509.04336}

\bibitem[\protect\citeauthoryear{{Riess} et~al.,}{{Riess}
  et~al.}{1998}]{SupernovaSearchTeam:1998}
{Riess} A.~G.,  et~al., 1998, \mn@doi [\aj] {10.1086/300499}, \href
  {https://ui.adsabs.harvard.edu/abs/1998AJ....116.1009R} {116, 1009}

\bibitem[\protect\citeauthoryear{{Shen}, {Anstey}, {de Lera Acedo}  \&
  {Fialkov}}{{Shen} et~al.}{2024}]{shen}
{Shen} E.,  {Anstey} D.,  {de Lera Acedo} E.,   {Fialkov} A.,  2024, \mn@doi
  [\mnras] {10.1093/mnras/stae614}, \href
  {https://ui.adsabs.harvard.edu/abs/2024MNRAS.529.1642S} {529, 1642}

\bibitem[\protect\citeauthoryear{Taylor et~al.,}{Taylor
  et~al.}{2023}]{taylor2023}
Taylor G.,  et~al., 2023, \mn@doi [Monthly Notices of the Royal Astronomical
  Society] {10.1093/mnras/stad320}, 520, 5209

\bibitem[\protect\citeauthoryear{{The pandas development team}}{{The pandas
  development team}}{2023}]{pandaszenodo}
{The pandas development team} 2023, pandas-dev/pandas: Pandas,
  \mn@doi{10.5281/zenodo.8092754}, \url
  {https://doi.org/10.5281/zenodo.8092754}

\bibitem[\protect\citeauthoryear{{V{\'a}zquez}, {Bridges}, {Hobson}  \&
  {Lasenby}}{{V{\'a}zquez} et~al.}{2012a}]{pkvazquez}
{V{\'a}zquez} J.~A.,  {Bridges} M.,  {Hobson} M.~P.,   {Lasenby} A.~N.,  2012a,
  \mn@doi [\jcap] {10.1088/1475-7516/2012/06/006}, \href
  {https://ui.adsabs.harvard.edu/abs/2012JCAP...06..006V} {2012, 006}

\bibitem[\protect\citeauthoryear{{V{\'a}zquez}, {Bridges}, {Hobson}  \&
  {Lasenby}}{{V{\'a}zquez} et~al.}{2012b}]{devazquez}
{V{\'a}zquez} J.~A.,  {Bridges} M.,  {Hobson} M.~P.,   {Lasenby} A.~N.,  2012b,
  \mn@doi [\jcap] {10.1088/1475-7516/2012/09/020}, \href
  {https://ui.adsabs.harvard.edu/abs/2012JCAP...09..020V} {2012, 020}

\bibitem[\protect\citeauthoryear{{Vincenzi} et~al.,}{{Vincenzi}
  et~al.}{2025}]{vincenzi}
{Vincenzi} M.,  et~al., 2025, \mn@doi [\mnras] {10.1093/mnras/staf943}, \href
  {https://ui.adsabs.harvard.edu/abs/2025MNRAS.541.2585V} {541, 2585}

\bibitem[\protect\citeauthoryear{Virtanen et~al.,}{Virtanen
  et~al.}{2020}]{scipy}
Virtanen P.,  et~al., 2020, \mn@doi [Nature Methods]
  {10.1038/s41592-019-0686-2}, \href {https://rdcu.be/b08Wh} {17, 261}

\bibitem[\protect\citeauthoryear{Yallup, Kroupa  \& Handley}{Yallup
  et~al.}{2025}]{yallup2025nested}
Yallup D.,  Kroupa N.,   Handley W.,  2025, in Frontiers in Probabilistic
  Inference: Learning meets Sampling. \url
  {https://openreview.net/forum?id=ekbkMSuPo4}

\makeatother
\end{thebibliography}

```

5. **Author Information:**
- Lead Author: {'name': 'A. N. Ormondroyd'}
- Full Authors List:
```yaml
A. N. Ormondroyd:
  phd:
    start: 2021-10-01
    end: 2025-09-30
    supervisors:
    - Mike Hobson
    - Will Handley
    - Anthony Lasenby
    thesis: 'Bayesian methods in a flexible universe: sampling, tension and dark energy'
  original_image: images/originals/adam_ormondroyd.jpg
  image: /assets/group/images/adam_ormondroyd.jpg
  links:
    linkedin: https://www.linkedin.com/in/adam-ormondroyd/
    GitHub: https://github.com/AdamOrmondroyd
W. J. Handley:
  pi:
    start: 2020-10-01
    thesis: null
  postdoc:
    start: 2016-10-01
    end: 2020-10-01
    thesis: null
  phd:
    start: 2012-10-01
    end: 2016-09-30
    supervisors:
    - Anthony Lasenby
    - Mike Hobson
    thesis: 'Kinetic initial conditions for inflation: theory, observation and methods'
  original_image: images/originals/will_handley.jpeg
  image: /assets/group/images/will_handley.jpg
  links:
    Webpage: https://willhandley.co.uk
M. P. Hobson:
  coi:
    start: 2012-10-01
    thesis: null
  image: https://www.phy.cam.ac.uk/wp-content/uploads/2025/04/hobson-150x150.jpg
  links:
    Department webpage: https://www.phy.cam.ac.uk/directory/hobsonm
A. N. Lasenby:
  coi:
    start: 2012-10-01
    thesis: null
  image: https://www.phy.cam.ac.uk/wp-content/uploads/2025/04/lasenby-150x150.jpg
  links:
    Department webpage: https://www.phy.cam.ac.uk/directory/lasenbya
D. Yallup:
  postdoc:
    start: 2021-01-10
    thesis: null
  original_image: images/originals/david_yallup.jpg
  image: /assets/group/images/david_yallup.jpg
  links:
    ORCiD: https://orcid.org/0000-0003-4716-5817
    linkedin: https://www.linkedin.com/in/dyallup/

```
This YAML file provides a concise snapshot of an academic research group. It lists members by name along with their academic rolesâ€”ranging from Part III and summer projects to MPhil, PhD, and postdoctoral positionsâ€”with corresponding dates, thesis topics, and supervisor details. Supplementary metadata includes image paths and links to personal or departmental webpages. A dedicated "coi" section profiles senior researchers, highlighting the groupâ€™s collaborative mentoring network and career trajectories in cosmology, astrophysics, and Bayesian data analysis.



====================================================================================
Final Output Instructions
====================================================================================

- Combine all data sources to create a seamless, engaging narrative.
- Follow the exact Markdown output format provided at the top.
- Do not include any extra explanation, commentary, or wrapping beyond the specified Markdown.
- Validate that every bibliographic reference with a DOI or arXiv identifier is converted into a Markdown link as per the examples.
- Validate that every Markdown author link corresponds to a link in the author information block.
- Before finalizing, confirm that no LaTeX citation commands or other undesired formatting remain.
- Before finalizing, confirm that the link to the paper itself [2509.13220](https://arxiv.org/abs/2509.13220) is featured in the first sentence.

Generate only the final Markdown output that meets all these requirements.

{% endraw %}