{% raw %}

Title: Create a Markdown Blog Post Integrating Research Details and a Featured Paper
====================================================================================

This task involves generating a Markdown file (ready for a GitHub-served Jekyll site) that integrates our research details with a featured research paper. The output must follow the exact format and conventions described below.

====================================================================================
Output Format (Markdown):
------------------------------------------------------------------------------------
---
layout: post
title:  "Galactification: painting galaxies onto dark matter only simulations using a transformer-based model"
date:   2025-11-11
categories: papers
---
![AI generated image](/assets/images/posts/2025-11-11-2511.08438.png)

<!-- BEGINNING OF GENERATED POST -->
<!-- END OF GENERATED POST -->

<img src="/assets/group/images/chris_lovell.jpg" alt="Chris Lovell" style="width: auto; height: 25vw;">

Content generated by [gemini-2.5-pro](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/2025-11-11-2511.08438.txt).

Image generated by [imagen-4.0-generate-001](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/2025-11-11-2511.08438.txt).

------------------------------------------------------------------------------------
====================================================================================

Please adhere strictly to the following instructions:

====================================================================================
Section 1: Content Creation Instructions
====================================================================================

1. **Generate the Page Body:**
   - Write a well-composed, engaging narrative that is suitable for a scholarly audience interested in advanced AI and astrophysics.
   - Ensure the narrative is original and reflective of the tone and style and content in the "Homepage Content" block (provided below), but do not reuse its content.
   - Use bullet points, subheadings, or other formatting to enhance readability.

2. **Highlight Key Research Details:**
   - Emphasize the contributions and impact of the paper, focusing on its methodology, significance, and context within current research.
   - Specifically highlight the lead author ({'name': 'Shivam Pandey'}). When referencing any author, use Markdown links from the Author Information block (choose academic or GitHub links over social media).

3. **Integrate Data from Multiple Sources:**
   - Seamlessly weave information from the following:
     - **Paper Metadata (YAML):** Essential details including the title and authors.
     - **Paper Source (TeX):** Technical content from the paper.
     - **Bibliographic Information (bbl):** Extract bibliographic references.
     - **Author Information (YAML):** Profile details for constructing Markdown links.
   - Merge insights from the Paper Metadata, TeX source, Bibliographic Information, and Author Information blocks into a coherent narrative—do not treat these as separate or isolated pieces.
   - Insert the generated narrative between the HTML comments:
     <!-- BEGINNING OF GENERATED POST --> and <!-- END OF GENERATED POST -->

4. **Generate Bibliographic References:**
   - Review the Bibliographic Information block carefully.
   - For each reference that includes a DOI or arXiv identifier:
     - For DOIs, generate a link formatted as:
       [10.1234/xyz](https://doi.org/10.1234/xyz)
     - For arXiv entries, generate a link formatted as:
       [2103.12345](https://arxiv.org/abs/2103.12345)
    - **Important:** Do not use any LaTeX citation commands (e.g., `\cite{...}`). Every reference must be rendered directly as a Markdown link. For example, instead of `\cite{mycitation}`, output `[mycitation](https://doi.org/mycitation)`
        - **Incorrect:** `\cite{10.1234/xyz}`  
        - **Correct:** `[10.1234/xyz](https://doi.org/10.1234/xyz)`
   - Ensure that at least three (3) of the most relevant references are naturally integrated into the narrative.
   - Ensure that the link to the Featured paper [2511.08438](https://arxiv.org/abs/2511.08438) is included in the first sentence.

5. **Final Formatting Requirements:**
   - The output must be plain Markdown; do not wrap it in Markdown code fences.
   - Preserve the YAML front matter exactly as provided.

====================================================================================
Section 2: Provided Data for Integration
====================================================================================

1. **Homepage Content (Tone and Style Reference):**
```markdown
---
layout: home
---

![AI generated image](/assets/images/index.png)

<!-- START OF WEBSITE SUMMARY -->
The Handley Research Group stands at the forefront of cosmological exploration, pioneering novel approaches that fuse fundamental physics with the transformative power of artificial intelligence. We are a dynamic team of researchers, including PhD students, postdoctoral fellows, and project students, based at the University of Cambridge. Our mission is to unravel the mysteries of the Universe, from its earliest moments to its present-day structure and ultimate fate. We tackle fundamental questions in cosmology and astrophysics, with a particular focus on leveraging advanced Bayesian statistical methods and AI to push the frontiers of scientific discovery. Our research spans a wide array of topics, including the [primordial Universe](https://arxiv.org/abs/1907.08524), [inflation](https://arxiv.org/abs/1807.06211), the nature of [dark energy](https://arxiv.org/abs/2503.08658) and [dark matter](https://arxiv.org/abs/2405.17548), [21-cm cosmology](https://arxiv.org/abs/2210.07409), the [Cosmic Microwave Background (CMB)](https://arxiv.org/abs/1807.06209), and [gravitational wave astrophysics](https://arxiv.org/abs/2411.17663).

### Our Research Approach: Innovation at the Intersection of Physics and AI

At The Handley Research Group, we develop and apply cutting-edge computational techniques to analyze complex astronomical datasets. Our work is characterized by a deep commitment to principled [Bayesian inference](https://arxiv.org/abs/2205.15570) and the innovative application of [artificial intelligence (AI) and machine learning (ML)](https://arxiv.org/abs/2504.10230).

**Key Research Themes:**
*   **Cosmology:** We investigate the early Universe, including [quantum initial conditions for inflation](https://arxiv.org/abs/2002.07042) and the generation of [primordial power spectra](https://arxiv.org/abs/2112.07547). We explore the enigmatic nature of [dark energy, using methods like non-parametric reconstructions](https://arxiv.org/abs/2503.08658), and search for new insights into [dark matter](https://arxiv.org/abs/2405.17548). A significant portion of our efforts is dedicated to [21-cm cosmology](https://arxiv.org/abs/2104.04336), aiming to detect faint signals from the Cosmic Dawn and the Epoch of Reionization.
*   **Gravitational Wave Astrophysics:** We develop methods for [analyzing gravitational wave signals](https://arxiv.org/abs/2411.17663), extracting information about extreme astrophysical events and fundamental physics.
*   **Bayesian Methods & AI for Physical Sciences:** A core component of our research is the development of novel statistical and AI-driven methodologies. This includes advancing [nested sampling techniques](https://arxiv.org/abs/1506.00171) (e.g., [PolyChord](https://arxiv.org/abs/1506.00171), [dynamic nested sampling](https://arxiv.org/abs/1704.03459), and [accelerated nested sampling with $\beta$-flows](https://arxiv.org/abs/2411.17663)), creating powerful [simulation-based inference (SBI) frameworks](https://arxiv.org/abs/2504.10230), and employing [machine learning for tasks such as radiometer calibration](https://arxiv.org/abs/2504.16791), [cosmological emulation](https://arxiv.org/abs/2503.13263), and [mitigating radio frequency interference](https://arxiv.org/abs/2211.15448). We also explore the potential of [foundation models for scientific discovery](https://arxiv.org/abs/2401.00096).

**Technical Contributions:**
Our group has a strong track record of developing widely-used scientific software. Notable examples include:
*   [**PolyChord**](https://arxiv.org/abs/1506.00171): A next-generation nested sampling algorithm for Bayesian computation.
*   [**anesthetic**](https://arxiv.org/abs/1905.04768): A Python package for processing and visualizing nested sampling runs.
*   [**GLOBALEMU**](https://arxiv.org/abs/2104.04336): An emulator for the sky-averaged 21-cm signal.
*   [**maxsmooth**](https://arxiv.org/abs/2007.14970): A tool for rapid maximally smooth function fitting.
*   [**margarine**](https://arxiv.org/abs/2205.12841): For marginal Bayesian statistics using normalizing flows and KDEs.
*   [**fgivenx**](https://arxiv.org/abs/1908.01711): A package for functional posterior plotting.
*   [**nestcheck**](https://arxiv.org/abs/1804.06406): Diagnostic tests for nested sampling calculations.

### Impact and Discoveries
Our research has led to significant advancements in cosmological data analysis and yielded new insights into the Universe. Key achievements include:
*   Pioneering the development and application of advanced Bayesian inference tools, such as [PolyChord](https://arxiv.org/abs/1506.00171), which has become a cornerstone for cosmological parameter estimation and model comparison globally.
*   Making significant contributions to the analysis of major cosmological datasets, including the [Planck mission](https://arxiv.org/abs/1807.06209), providing some of the tightest constraints on cosmological parameters and models of [inflation](https://arxiv.org/abs/1807.06211).
*   Developing novel AI-driven approaches for astrophysical challenges, such as using [machine learning for radiometer calibration in 21-cm experiments](https://arxiv.org/abs/2504.16791) and [simulation-based inference for extracting cosmological information from galaxy clusters](https://arxiv.org/abs/2504.10230).
*   Probing the nature of dark energy through innovative [non-parametric reconstructions of its equation of state](https://arxiv.org/abs/2503.08658) from combined datasets.
*   Advancing our understanding of the early Universe through detailed studies of [21-cm signals from the Cosmic Dawn and Epoch of Reionization](https://arxiv.org/abs/2301.03298), including the development of sophisticated foreground modelling techniques and emulators like [GLOBALEMU](https://arxiv.org/abs/2104.04336).
*   Developing new statistical methods for quantifying tensions between cosmological datasets ([Quantifying tensions in cosmological parameters: Interpreting the DES evidence ratio](https://arxiv.org/abs/1902.04029)) and for robust Bayesian model selection ([Bayesian model selection without evidences: application to the dark energy equation-of-state](https://arxiv.org/abs/1506.09024)).
*   Exploring fundamental physics questions such as potential [parity violation in the Large-Scale Structure using machine learning](https://arxiv.org/abs/2410.16030).

### Charting the Future: AI-Powered Cosmological Discovery
The Handley Research Group is poised to lead a new era of cosmological analysis, driven by the explosive growth in data from next-generation observatories and transformative advances in artificial intelligence. Our future ambitions are centred on harnessing these capabilities to address the most pressing questions in fundamental physics.

**Strategic Research Pillars:**
*   **Next-Generation Simulation-Based Inference (SBI):** We are developing advanced SBI frameworks to move beyond traditional likelihood-based analyses. This involves creating sophisticated codes for simulating [Cosmic Microwave Background (CMB)](https://arxiv.org/abs/1908.00906) and [Baryon Acoustic Oscillation (BAO)](https://arxiv.org/abs/1607.00270) datasets from surveys like DESI and 4MOST, incorporating realistic astrophysical effects and systematic uncertainties. Our AI initiatives in this area focus on developing and implementing cutting-edge SBI algorithms, particularly [neural ratio estimation (NRE) methods](https://arxiv.org/abs/2407.15478), to enable robust and scalable inference from these complex simulations.
*   **Probing Fundamental Physics:** Our enhanced analytical toolkit will be deployed to test the standard cosmological model ($\Lambda$CDM) with unprecedented precision and to explore [extensions to Einstein's General Relativity](https://arxiv.org/abs/2006.03581). We aim to constrain a wide range of theoretical models, from modified gravity to the nature of [dark matter](https://arxiv.org/abs/2106.02056) and [dark energy](https://arxiv.org/abs/1701.08165). This includes leveraging data from upcoming [gravitational wave observatories](https://arxiv.org/abs/1803.10210) like LISA, alongside CMB and large-scale structure surveys from facilities such as Euclid and JWST.
*   **Synergies with Particle Physics:** We will continue to strengthen the connection between cosmology and particle physics by expanding the [GAMBIT framework](https://arxiv.org/abs/2009.03286) to interface with our new SBI tools. This will facilitate joint analyses of cosmological and particle physics data, providing a holistic approach to understanding the Universe's fundamental constituents.
*   **AI-Driven Theoretical Exploration:** We are pioneering the use of AI, including [large language models and symbolic computation](https://arxiv.org/abs/2401.00096), to automate and accelerate the process of theoretical model building and testing. This innovative approach will allow us to explore a broader landscape of physical theories and derive new constraints from diverse astrophysical datasets, such as those from GAIA.

Our overarching goal is to remain at the forefront of scientific discovery by integrating the latest AI advancements into every stage of our research, from theoretical modeling to data analysis and interpretation. We are excited by the prospect of using these powerful new tools to unlock the secrets of the cosmos.
<!-- END OF WEBSITE SUMMARY -->

Content generated by [gemini-2.5-pro-preview-05-06](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/index.txt).

Image generated by [imagen-3.0-generate-002](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/index.txt).
```

2. **Paper Metadata:**
```yaml
!!python/object/new:feedparser.util.FeedParserDict
dictitems:
  id: http://arxiv.org/abs/2511.08438v1
  guidislink: true
  link: https://arxiv.org/abs/2511.08438v1
  title: 'Galactification: painting galaxies onto dark matter only simulations using
    a transformer-based model'
  title_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      type: text/plain
      language: null
      base: ''
      value: 'Galactification: painting galaxies onto dark matter only simulations
        using a transformer-based model'
  updated: '2025-11-11T16:39:47Z'
  updated_parsed: !!python/object/apply:time.struct_time
  - !!python/tuple
    - 2025
    - 11
    - 11
    - 16
    - 39
    - 47
    - 1
    - 315
    - 0
  - tm_zone: null
    tm_gmtoff: null
  links:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      href: https://arxiv.org/abs/2511.08438v1
      rel: alternate
      type: text/html
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      href: https://arxiv.org/pdf/2511.08438v1
      rel: related
      type: application/pdf
      title: pdf
  summary: Connecting the formation and evolution of galaxies to the large-scale structure
    is crucial for interpreting cosmological observations. While hydrodynamical simulations
    accurately model the correlated properties of galaxies, they are computationally
    prohibitive to run over volumes that match modern surveys. We address this by
    developing a framework to rapidly generate mock galaxy catalogs conditioned on
    inexpensive dark-matter-only simulations. We present a multi-modal, transformer-based
    model that takes 3D dark matter density and velocity fields as input, and outputs
    a corresponding point cloud of galaxies with their physical properties. We demonstrate
    that our trained model faithfully reproduces a variety of galaxy summary statistics
    and correctly captures their variation with changes in the underlying cosmological
    and astrophysical parameters, making it the first accelerated forward model to
    capture all the relevant galaxy properties, their full spatial distribution, and
    their conditional dependencies in hydrosimulations.
  summary_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      type: text/plain
      language: null
      base: ''
      value: Connecting the formation and evolution of galaxies to the large-scale
        structure is crucial for interpreting cosmological observations. While hydrodynamical
        simulations accurately model the correlated properties of galaxies, they are
        computationally prohibitive to run over volumes that match modern surveys.
        We address this by developing a framework to rapidly generate mock galaxy
        catalogs conditioned on inexpensive dark-matter-only simulations. We present
        a multi-modal, transformer-based model that takes 3D dark matter density and
        velocity fields as input, and outputs a corresponding point cloud of galaxies
        with their physical properties. We demonstrate that our trained model faithfully
        reproduces a variety of galaxy summary statistics and correctly captures their
        variation with changes in the underlying cosmological and astrophysical parameters,
        making it the first accelerated forward model to capture all the relevant
        galaxy properties, their full spatial distribution, and their conditional
        dependencies in hydrosimulations.
  tags:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: astro-ph.CO
      scheme: http://arxiv.org/schemas/atom
      label: null
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: astro-ph.IM
      scheme: http://arxiv.org/schemas/atom
      label: null
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: cs.LG
      scheme: http://arxiv.org/schemas/atom
      label: null
  published: '2025-11-11T16:39:47Z'
  published_parsed: !!python/object/apply:time.struct_time
  - !!python/tuple
    - 2025
    - 11
    - 11
    - 16
    - 39
    - 47
    - 1
    - 315
    - 0
  - tm_zone: null
    tm_gmtoff: null
  arxiv_comment: 8 pages, 4 figures. , accepted at Machine Learning and the Physical
    Sciences Workshop at NeurIPS 2025
  arxiv_primary_category:
    term: astro-ph.CO
  authors:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Shivam Pandey
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Christopher C. Lovell
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Chirag Modi
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Benjamin D. Wandelt
  author_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Benjamin D. Wandelt
  author: Benjamin D. Wandelt

```

3. **Paper Source (TeX):**
```tex
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
 \usepackage[final]{neurips_2025_ml4ps}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}
% Bibliography and bibfile
% Taken from aa.cls
\def\aj{AJ}%
          % Astronomical Journal
\def\araa{ARA\&A}%
          % Annual Review of Astron and Astrophys
\def\apj{ApJ}%
          % Astrophysical Journal
\def\apjl{ApJ}%
          % Astrophysical Journal, Letters
\def\apjs{ApJS}%
          % Astrophysical Journal, Supplement
\def\ao{Appl.~Opt.}%
          % Applied Optics
\def\apss{Ap\&SS}%
          % Astrophysics and Space Science
\def\aap{A\&A}%
          % Astronomy and Astrophysics
\def\aapr{A\&A~Rev.}%
          % Astronomy and Astrophysics Reviews
\def\aaps{A\&AS}%
          % Astronomy and Astrophysics, Supplement
\def\azh{AZh}%
          % Astronomicheskii Zhurnal
\def\baas{BAAS}%
          % Bulletin of the AAS
\def\jrasc{JRASC}%
          % Journal of the RAS of Canada
\def\memras{MmRAS}%
          % Memoirs of the RAS
\def\mnras{MNRAS}%
          % Monthly Notices of the RAS
\def\pra{Phys.~Rev.~A}%
          % Physical Review A: General Physics
\def\prb{Phys.~Rev.~B}%
          % Physical Review B: Solid State
\def\prc{Phys.~Rev.~C}%
          % Physical Review C
\def\prd{Phys.~Rev.~D}%
          % Physical Review D
\def\pre{Phys.~Rev.~E}%
          % Physical Review E
\def\prl{Phys.~Rev.~Lett.}%
          % Physical Review Letters
\def\pasp{PASP}%
          % Publications of the ASP
\def\pasj{PASJ}%
          % Publications of the ASJ
\def\qjras{QJRAS}%
          % Quarterly Journal of the RAS
\def\skytel{S\&T}%
          % Sky and Telescope
\def\solphys{Sol.~Phys.}%
          % Solar Physics
\def\sovast{Soviet~Ast.}%
          % Soviet Astronomy
\def\ssr{Space~Sci.~Rev.}%
          % Space Science Reviews
\def\zap{ZAp}%
          % Zeitschrift fuer Astrophysik
\def\nat{Nature}%
          % Nature
\def\iaucirc{IAU~Circ.}%
          % IAU Cirulars
\def\aplett{Astrophys.~Lett.}%
          % Astrophysics Letters
\def\apspr{Astrophys.~Space~Phys.~Res.}%
          % Astrophysics Space Physics Research
\def\bain{Bull.~Astron.~Inst.~Netherlands}%
          % Bulletin Astronomical Institute of the Netherlands
\def\fcp{Fund.~Cosmic~Phys.}%
          % Fundamental Cosmic Physics
\def\gca{Geochim.~Cosmochim.~Acta}%
          % Geochimica Cosmochimica Acta
\def\grl{Geophys.~Res.~Lett.}%
          % Geophysics Research Letters
\def\jcap{JCAP}%
          % Journal of Cosmology and Astroparticle Physics
\def\jcp{J.~Chem.~Phys.}%
          % Journal of Chemical Physics
\def\jgr{J.~Geophys.~Res.}%
          % Journal of Geophysics Research
\def\jqsrt{J.~Quant.~Spec.~Radiat.~Transf.}%
          % Journal of Quantitiative Spectroscopy and Radiative Trasfer
\def\memsai{Mem.~Soc.~Astron.~Italiana}%
          % Mem. Societa Astronomica Italiana
\def\nphysa{Nucl.~Phys.~A}%
          % Nuclear Physics A
\def\physrep{Phys.~Rep.}%
          % Physics Reports
\def\physscr{Phys.~Scr}%
          % Physica Scripta
\def\planss{Planet.~Space~Sci.}%
          % Planetary Space Science
\def\procspie{Proc.~SPIE}%
          % Proceedings of the SPIE
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{subcaption}
% \usepackage{biblatex}
% \usepackage{cite}
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[capitalize,noabbrev]{cleveref}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{Galactification: painting galaxies onto dark matter only simulations using a transformer-based model}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Shivam Pandey \\
  Department of Physics and Astronomy, Johns Hopkins University, \\
  Baltimore, MD 21218, USA. 
  \texttt{shivamp@jhu.edu} \\
  % examples of more authors
  \And
  Christopher C. Lovell \\
  Kavli Institute for Cosmology, University of Cambridge, UK. \\
  \texttt{chris.lovell.astro@gmail.com} \\
  \And
  Chirag Modi \\
  Center for Cosmology and Particle Physics, New York University \\
  New York, NY 10012, USA.
  % Address \\
  \texttt{modichirag@nyu.edu} \\
  \And
  Benjamin D. Wandelt \\
  Department of Physics and Astronomy, Johns Hopkins University, \\
  Baltimore, MD 21218, USA. 
  % Address \\
  \texttt{wandelt@jhu.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

% \bibliographystyle{icml2025}
\begin{document}


\maketitle


\begin{abstract}
Connecting the formation and evolution of galaxies to the large-scale structure is crucial for interpreting cosmological observations. While hydrodynamical simulations accurately model the correlated properties of galaxies, they are computationally prohibitive to run over volumes that match modern surveys. We address this by developing a framework to rapidly generate mock galaxy catalogs conditioned on inexpensive dark-matter-only simulations. We present a multi-modal, transformer-based model that takes 3D dark matter density and velocity fields as input, and outputs a corresponding point cloud of galaxies with their physical properties. We demonstrate that our trained model faithfully reproduces a variety of galaxy summary statistics and correctly captures their variation with changes in the underlying cosmological and astrophysical parameters, making it the first accelerated forward model to capture all the relevant galaxy properties, their full spatial distribution, and their conditional dependencies in hydrosimulations. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
\vspace{-0.05in}
Large-scale cosmological surveys provide statistical information on billions of galaxies, which is key to understanding the structure and evolution of the Universe. Gravitational collapse induces significant non-Gaussianity in the large-scale structure, embedding information at all orders of correlation that is difficult to capture with purely analytical frameworks. It is therefore essential to create digital analogs of these observations---in the form of simulated universes---to compare against the data. Hydrodynamical simulations, which self-consistently evolve components such as dark matter, gas, stars, and black holes, are the most physically motivated method for creating such mock galaxy catalogs. However, they are too computationally expensive to run at the scale and fidelity required to draw inferences from modern observational data.

% Current state-of-the-art hydrodynamical simulations span volumes of approximately $1 \, {\rm Gpc}^3$ \footnote{Gpc stands for giga-parsec and is approximately equal to $3.2\times 10^9$ lightyears or $2\times 10^{22}$ miles.}, with a single run costing around $2\times 10^8$ CPU hours \citep{Pakmor:2023:MNRAS:}.\footnote{Alternative simulation methods offer varying runtimes but are of a similar order of magnitude \citep{Crain:2023:ARA&A:}.} In contrast, past galaxy surveys already exceed volumes of $27 \, {\rm Gpc}^3$, and upcoming surveys will be significantly larger \citep{EuclidCollaboration:2025:A&A:}. Simulating even a single volume of this magnitude is computationally prohibitive. Furthermore, obtaining robust parameter constraints from observational data within a Bayesian framework, for example via simulation-based inference (SBI) \citep{Cranmer_2020}, requires a large ensemble of simulations. This demand exacerbates the computational challenge and motivates the development of accelerated alternative frameworks.

State-of-the-art, high-resolution hydrodynamical simulations capable of resolving galaxy formation in low-mass systems are limited to box sizes of approximately 100~Mpc/$h$ \citep{Vogelsberger:2020:NatRP:, Crain:2023:ARA&A:}.\footnote{Mpc stands for mega-parsec, approximately $3.2\times 10^6$ lightyears or $2\times 10^{19}$ miles, and $h \approx 0.7$ is related to the Hubble constant.} The hydrosimulations aiming to match the cosmological observations must approximate the complex astrophysics of galaxy formation using subgrid models, whose functional forms and parameters are often poorly constrained. While larger, lower-resolution simulations are possible, a single run of these kind of simulations at a fixed set of parameters costs around $2\times 10^8$ CPU hours \citep{Pakmor:2023:MNRAS:, Crain:2023:ARA&A:}. To obtain robust constraints on our Universe's parameters within a Bayesian framework, such as through simulation-based inference \citep[SBI; see][]{Cranmer_2020}, one needs a large ensemble of simulations that span a wide range of cosmological and astrophysical models and their parameter values. This requirement exacerbates the computational challenge and strongly motivates the development of accelerated model frameworks.

\begin{figure*}[ht]
\centering
\vskip -0.15in
\includegraphics[width=0.8\textwidth, height=0.35\textwidth]{figs/architecture.pdf} 
% \vskip -0.15in
\caption{
\textbf{Model architecture.} Left: Input dark matter density field. Right: Target galaxy distribution. An encoder (CBAM + Vision Transformer) extracts features that condition a cross-attention decoder to generate a tokenized sequence of galaxy properties. See Sec.~\ref{sec:data_methods} and \cite{Pandey:2024:arXiv:gotham} for details.
}
\label{fig:architecture}
% \end{center}
% \vskip -0.2in
\end{figure*}

A much faster alternative is to simulate only the evolution of dark matter, which interacts purely through gravity. These dark-matter-only simulations, known as N-body simulations, are typically over 100 times faster than their hydrodynamical counterparts. In this work, we use N-body simulations as input to learn the distribution and properties of galaxies from a corresponding hydrodynamical simulation that shares the same initial conditions. The task is effectively to learn a high-dimensional conditional probability distribution. The transformer architecture \citep{vaswani2023attentionneed} has recently proven highly efficient for such problems, as it can interface with multi-modal inputs and outputs. We therefore adopt this approach for our framework.




% \vskip -0.1in


\section{Related Works}
\vskip -0.05in
% Machine learning methods have been widely applied to related problems in cosmological simulations. Early works often used convolutional neural networks (CNNs), graph neural networks or recurrent neural networks for deterministic mapping tasks \citep{deSanti:2022:MNRAS:, Jespersen:2022:ApJ:, Chittenden:2023:MNRAS:, Hausen:2023:ApJ:, Li:2021:PNAS:, Zhang:2019:arXiv:}. For instance, \citet{Li:2021:PNAS:} employed CNNs to super-resolve high-resolution N-body simulations from low-resolution inputs. Similarly, \citet{Zhang:2019:arXiv:} used a CNN-based architecture to map from N-body simulations to the corresponding galaxy number counts found in hydrodynamical simulations. However, as galaxy formation is an inherently stochastic process, a simple deterministic mapping is insufficient. A generative approach that captures the conditional probability distribution of galaxy properties is more appropriate.
Machine learning methods have been widely applied to related problems in cosmological simulations and early works used deterministic mapping algorithms \citep{Zhang:2019:arXiv:, Li:2021:PNAS:, deSanti:2022:MNRAS:, Jespersen:2022:ApJ:, Chittenden:2023:MNRAS:, Hausen:2023:ApJ:}. However, as galaxy formation is an inherently stochastic process, a generative approach that captures the conditional probability distribution of galaxy properties is more appropriate.


Following this reasoning, recent studies have focused on generative models \citep{Lovell:2023:mla:, Rodrigues:2025:A&A:, Bourdin:2024:arXiv:, Cuesta-Lazaro:2024:PhRvD:, Maltz:2025:MNRAS:}. In \citet{Bourdin:2024:arXiv:}, the authors trained a score-based diffusion model to predict the distribution of galaxy counts from N-body simulations and show that a simple halo-based mapping between galaxies and N-body simulations (called the halo occupation distribution, HOD, \cite{Zheng:2005:ApJ:}) is insufficient (also see \cite{Hadzhiyska:2020:MNRAS:}). While successful for number counts, realistic mock catalogs for observational comparisons must include additional properties, such as stellar mass, velocity, and apparent magnitudes. The point-cloud diffusion model based-approaches as described in \citet{Cuesta-Lazaro:2024:PhRvD:} focused on learning the spatial distribution and properties of massive halos in N-body simulations. In principle it can be extended to learn galaxy properties but crucially they focus on a emulating a fixed number of objects for each simulation, whereas the total number of galaxies in a hydrosimulation is a strong function of the underlying cosmological and sub-grid parameters (the input parameters when running a hydrosimulation). 

In \citet{Pandey:2024:arXiv:gotham}, the authors developed a multi-modal, transformer-based model to predict the spatial distribution and properties of  \textit{halos at a fixed cosmology and redshift} in N-body simulations, conditioned on faster, approximate gravity solvers. We improve and generalize their architecture for the task of predicting galaxy distributions and properties at a fixed redshift ($z=0$) directly from N-body simulations \textit{conditioned on the varying cosmological and sub-grid parameters of the simulations}. The work presented here provides a first internally consistent way to learn the positions and properties of the galaxies (such as velocity, stellar masses and photometric magnitudes) conditioned on the parameters of the simulations while also going to smaller scales ($k \sim 10 \, h/$Mpc) compared to previous works. 


\section{Data and Methodology}\label{sec:data_methods}
\vskip -0.05in
We use the Illustris-TNG Latin hypercube set from the CAMELS simulation suite \citep{Villaescusa_Navarro_2021}, which provides 1000 pairs of N-body and hydrodynamical simulations that vary two cosmological parameters (total matter density $\Omega_{\rm m}$ and matter clustering amplitude $\sigma_8$) and four astrophysical parameters governing supernova and AGN feedback.\footnote{Although here we only show results for Illustris-TNG set, we have verified that our method also works well for the Astrid set of simulations \cite{Ni:2023:ApJ:}.}\footnote{The N-body simulations are only sensitive to cosmological parameters, as they contain only dark matter and no astrophysical processes.} We divide this dataset into training (80\%), validation (12.5\%), and test (7.5\%) sets.

Our model inputs are derived from the N-body simulations. We extract dark matter density and velocity fields at five snapshots ($z = 0, 0.1, 0.3, 0.6, 1.0$) to capture the time evolution of the large-scale structure. For each simulation box of $(25~{\rm Mpc}/h)^3$, we divide the volume into eight sub-boxes and grid each field at a resolution of $16^3$. To incorporate information about the large-scale environment, we also include lower-resolution density fields from the parent box, down-sampled to match contexts of 1.5 and 3 times the sub-box size (aligned with the higher-resolution field for each sub-box). In total, 30 distinct 3D fields are concatenated along the channel dimension to form the input tensor which contains local, environment and growth of large scale structure information which is crucial to understand the galaxy formation process. 

The model's objective is to generate mock galaxy catalogs, including their 3D positions and properties: line-of-sight velocity ($v_x$), stellar mass ($\log(M_{\star})$), and SDSS g-band magnitude \citep[$M_g$; ][]{Lovell:2024:arXiv:}. We include all galaxies with stellar mass $M_{\star} > 10^{9.5} \, M_{\odot}/h$, a limit sufficient for next-generation surveys \citep{Zou:2019:ApJS:}. We scale the six properties ($x, y, z, v_x, \log(M_{\star}), M_g$) to lie in the range [0, 1] and then discretize this range into 64 bins. Each galaxy is thus represented by six tokens, forming a "word". For each sub-volume, we concatenate the tokens of all its galaxies in descending order of stellar mass to form a "sentence", which is bracketed by \texttt{START} and \texttt{END} tokens. This sequence is the target output for our model.


Our network adapts the encoder-decoder transformer architecture for cosmological simulations from \citet{Pandey:2024:arXiv:gotham} with several key modifications to learn the complicated galaxy formation process. The encoder first processes the multi-channel input fields with a Convolutional Block Attention Module \citep[CBAM;][]{woo2018cbamconvolutionalblockattention}, which uses channel and spatial attention to extract the most informative local features. The resulting feature maps are then passed to a stack of three Vision Transformer (ViT) layers \citep{dosovitskiy2021imageworth16x16words} to learn long-range correlations via self-attention. Finally, the cosmological and astrophysical parameters are appended to the ViT output, and this combined tensor is fed into the cross-attention mechanism of the decoder.

In the decoder, we first embed the galaxy tokens into a 192-dimensional space, adding a learned embedding corresponding to the token's index (1-6) to distinguish between the six different property types. We also employ Rotary Position Embeddings \citep[RoPE;][]{su2023roformerenhancedtransformerrotary} to encode the absolute position of tokens, allowing the self-attention mechanism to better capture relative dependencies. The decoder consists of 4 transformer layers with 8 attention heads. The output of the final layer is projected to predict the probability distribution for the next token in the sequence, and the model is trained by minimizing the cross-entropy loss.
\vskip -0.05in


\begin{figure*}[ht]
\centering
% \vskip -0.15in
\includegraphics[width=0.7\textwidth, height=0.5\textwidth]{figs/pqm_chi2_histogram.pdf} 
% \vskip -0.15in
\caption{
\textbf{Comparison of multi-dimensional data distribution.} We each galaxy as a six dimensional vector (3 position tokens + 3 property tokens) in all the test simulations and compare the distribution of the mock and truth data using the \texttt{PQMass} methodology outlined in \cite{Lemos:2024:arXiv:}. We find that the histogram of difference between the two catalogs agrees with the red line which corresponds to the expected $\chi^2$ curve if the mock are truth come from the same underlying distribution. 
}
\label{fig:pqmass}
% \end{center}
% \vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\centering

% Top Row: One-point statistics
\begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{figs/1pt_all_props_updated.pdf}
\end{subfigure}

% Bottom Row: Two-point statistics
\begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{figs/2pt_all_props_updated.pdf}
\end{subfigure}

\caption{\textbf{Comparison of one- and two-point summary statistics.} The top row compares one-point distributions (histograms) and the bottom row compares two-point statistics. In all panels, 16th-84th percentile regions from mock catalogs sampled from our model (filled regions) are compared against the hydrodynamical simulations (truth; solid lines, squares) colored by their corresponding value of cosmological parameter $\Omega_{\rm m}$. \textbf{Top panels}: Distributions of stellar mass, g-band magnitude and line-of-sight velocity (left to right) of galaxies. Lines are colored by a cosmological parameter, showing the model captures these physical dependencies. \textbf{Bottom panels}: Redshift space power spectra, either unweighted (left), or weighted by g-band magnitude (middle) and stellar mass (right).}
\label{fig:summary_stats} % New label for the combined figure

% \vskip -0.2in
\end{figure*}

\section{Results}
\vskip -0.05in



Once trained, we use the network to generate mock galaxy catalogs for the held-out test simulations by having the model autoregressively predict a token sequence—conditioned on dark matter fields—that is then decoded into galaxy positions and physical properties. Note that inferring the full galaxy catalog takes approximately 30 seconds on a single \texttt{Nvidia-H200} GPU, compared to 6000 CPU-hours for the equivalent hydrosimulation.

Appendix~\ref{app:mockvtrue} provides visual validations, comparing input N-body density fields with the true and predicted galaxy distributions for three different cosmologies. In Fig.~\ref{fig:pqmass} we provide a quantitative comparison between the truth and mock galaxy catalogs. We treat each galaxy as a six-dimensional vector, corresponding to 3 position coordinates and 3 properties considered in this study. Then for each held-out test simulation, we use the \texttt{PQMass} methodology described in \cite{Lemos:2024:arXiv:} to calculate the difference between the distributions of the six-dimensional data corresponding to the true and mock galaxy catalogs. The \texttt{PQMass} method partitions this sample space into non-overlapping regions and then applies $\chi^2$ tests to the number of samples residing in each region. Fig.~\ref{fig:pqmass} shows the histogram of recovered $\chi^2$ values with blue bars which agrees with the red $\chi^2$ curve (obtained for the choice of 20 regions) corresponding to the case that truth and mock data are generated from the same underlying distribution.


We first evaluate the model using one-point statistics, comparing the histograms of inferred stellar masses, g-band apparent magnitudes, and galaxy velocities against the true distributions in the top row of Fig.~\ref{fig:summary_stats}. To illustrate the model's sensitivity, the results are colored by the value of a single cosmological parameter ($\Omega_{\rm m}$), though all six parameters vary across the sample. Plotting the 16th-84th percentile region from 16 mock realizations, the figure demonstrates that our model successfully captures how changes in cosmology significantly alter these properties.

In the bottom row of Fig.~\ref{fig:summary_stats}, we probe galaxy clustering by measuring the redshift-space power spectrum. The left panel shows that the power spectra from our sampled mock catalogs (16 realizations which capture the stochasticity of galaxy formation) agree with the true spectra across various cosmologies. The middle and right panels compute power spectra weighted by g-band magnitude and stellar mass, respectively, which provide a sensitive test of the model's ability to learn the joint distribution of galaxy positions and properties, for which we find a similar level of performance. Note that we do not expect a high cross-correlation coefficient value between our mock realizations and true galaxy catalogs, especially on small scales, as our generative model captures the stochasticity of galaxy formation for a given N-body simulation.

% \vskip -0.1in
\section{Discussion}
\vskip -0.05in

In this work, we have presented a transformer-based, multi-modal framework that generates realistic galaxy catalogs by learning the complex mapping from N-body simulations to their hydrodynamical counterparts. Our model takes dark matter density and velocity fields from inexpensive N-body simulations as input to produce a full point cloud of galaxies with associated properties (stellar mass, velocity, and magnitude), effectively acting as an accelerated forward model that reduces computational costs by a factor of ~100.

We identify several key directions for future work. A natural next step is to apply this framework to larger simulation volumes, which will increase the total number of galaxies and expand the dynamic range of their properties, providing a richer dataset for the network. We also plan to augment the model's output to include more observable properties, such as multi-band photometry and full 3D velocity vectors. To address the computational challenge of the increased context length from these enhancements, we will explore more efficient architectures, such as those incorporating sparse \citep{child2019generatinglongsequencessparse} or linear attention \citep{katharopoulos2020transformersrnnsfastautoregressive}, to accelerate both training and inference.
% \bibliography{example_paper}

\section*{Acknowledgments}
This work is supported by the Simons Collaboration on ``Learning the Universe''. 
This research used the DeltaAI advanced computing and data resource, which is supported by the National Science Foundation (award OAC 2320345) and the State of Illinois. DeltaAI is a joint effort of the University of Illinois Urbana-Champaign and its National Center for Supercomputing Applications.
Some of the computations reported in this paper were also performed using resources made available by the Flatiron Institute. The Flatiron Institute is supported by the Simons Foundation. 

\bibliographystyle{mnras}
\newpage
% \section{Broader impact}
\typeout{}
% \bibliographystyle{JHEP}
\bibliography{example_paper}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\onecolumn

\begin{figure}[ht!]
    \centering
    \newlength{\totalfigheight}
    \setlength{\totalfigheight}{\dimexpr \textheight - \topskip - \headsep - \headheight - 4\baselineskip}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[height=0.25\totalfigheight, width=\linewidth, keepaspectratio]{figs/Nbody_galaxy_truth_mock_sim_991.pdf}
        \label{fig:sub1}
    \end{subfigure}
    \vfill 
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[height=0.25\totalfigheight, width=\linewidth, keepaspectratio]{figs/Nbody_galaxy_truth_mock_sim_994.pdf}
        \label{fig:sub2}
    \end{subfigure}
    \vfill 
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[height=0.25\totalfigheight, width=\linewidth, keepaspectratio]{figs/Nbody_galaxy_truth_mock_sim_997.pdf}
        \label{fig:sub3}
    \end{subfigure}

    \caption{\textbf{Visual comparison of true and mock galaxy distributions.} The figure displays results for three different test simulations, each with a unique set of cosmological and astrophysical parameters (one per row). The left column shows the input dark matter density field that is one of the input fields fed to the model. The middle column shows the true galaxy distribution from the hydrodynamical simulation, while the right column shows the corresponding distribution generated by our model. In the middle and right columns, galaxies are colored by their stellar mass.}
    \label{fig:visual_comparison}
\end{figure}

\section{Visualization of the inferred catalogs}\label{app:mockvtrue}

Here we provide a qualitative validation of our model's performance with a direct visual comparison between its outputs and the ground truth. Figure~\ref{fig:visual_comparison} displays results for three distinct simulations from our test set, each with a unique combination of cosmological and astrophysical parameters. For each case (row), we show the input dark matter density field (left column), the true galaxy distribution from the hydrodynamical simulation (middle column), and the corresponding mock catalog generated by our model (right column). The galaxies are colored by their stellar mass. A visual inspection confirms that our model successfully learns to populate the dense structures of the cosmic web, generating galaxy distributions that are qualitatively indistinguishable from the ground truth across a range of underlying physical models.


\end{document}
```

4. **Bibliographic Information:**
```bbl
\begin{thebibliography}{}
\makeatletter
\relax
\def\mn@urlcharsother{\let\do\@makeother \do\$\do\&\do\#\do\^\do\_\do\%\do\~}
\def\mn@doi{\begingroup\mn@urlcharsother \@ifnextchar [ {\mn@doi@} {\mn@doi@[]}}
\def\mn@doi@[#1]#2{\def\@tempa{#1}\ifx\@tempa\@empty \href {http://dx.doi.org/#2} {doi:#2}\else \href {http://dx.doi.org/#2} {#1}\fi \endgroup}
\def\mn@eprint#1#2{\mn@eprint@#1:#2::\@nil}
\def\mn@eprint@arXiv#1{\href {http://arxiv.org/abs/#1} {{\tt arXiv:#1}}}
\def\mn@eprint@dblp#1{\href {http://dblp.uni-trier.de/rec/bibtex/#1.xml} {dblp:#1}}
\def\mn@eprint@#1:#2:#3:#4\@nil{\def\@tempa {#1}\def\@tempb {#2}\def\@tempc {#3}\ifx \@tempc \@empty \let \@tempc \@tempb \let \@tempb \@tempa \fi \ifx \@tempb \@empty \def\@tempb {arXiv}\fi \@ifundefined {mn@eprint@\@tempb}{\@tempb:\@tempc}{\expandafter \expandafter \csname mn@eprint@\@tempb\endcsname \expandafter{\@tempc}}}

\bibitem[\protect\citeauthoryear{{Bourdin}, {Legin}, {Ho}, {Adam}, {Hezaveh}  \& {Perreault-Levasseur}}{{Bourdin} et~al.}{2024}]{Bourdin:2024:arXiv:}
{Bourdin} A.,  {Legin} R.,  {Ho} M.,  {Adam} A.,  {Hezaveh} Y.,   {Perreault-Levasseur} L.,  2024, \mn@doi [arXiv e-prints] {10.48550/arXiv.2408.00839}, \href {https://ui.adsabs.harvard.edu/abs/2024arXiv240800839B} {p. arXiv:2408.00839}

\bibitem[\protect\citeauthoryear{Child, Gray, Radford  \& Sutskever}{Child et~al.}{2019}]{child2019generatinglongsequencessparse}
Child R.,  Gray S.,  Radford A.,   Sutskever I.,  2019, Generating Long Sequences with Sparse Transformers (\mn@eprint {arXiv} {1904.10509}), \url {https://arxiv.org/abs/1904.10509}

\bibitem[\protect\citeauthoryear{{Chittenden} \& {Tojeiro}}{{Chittenden} \& {Tojeiro}}{2023}]{Chittenden:2023:MNRAS:}
{Chittenden} H.~G.,  {Tojeiro} R.,  2023, \mn@doi [\mnras] {10.1093/mnras/stac3498}, \href {https://ui.adsabs.harvard.edu/abs/2023MNRAS.518.5670C} {518, 5670}

\bibitem[\protect\citeauthoryear{{Crain} \& {van de Voort}}{{Crain} \& {van de Voort}}{2023}]{Crain:2023:ARA&A:}
{Crain} R.~A.,  {van de Voort} F.,  2023, \mn@doi [\araa] {10.1146/annurev-astro-041923-043618}, \href {https://ui.adsabs.harvard.edu/abs/2023ARA&A..61..473C} {61, 473}

\bibitem[\protect\citeauthoryear{Cranmer, Brehmer  \& Louppe}{Cranmer et~al.}{2020}]{Cranmer_2020}
Cranmer K.,  Brehmer J.,   Louppe G.,  2020, \mn@doi [Proceedings of the National Academy of Sciences] {10.1073/pnas.1912789117}, 117, 30055–30062

\bibitem[\protect\citeauthoryear{{Cuesta-Lazaro} \& {Mishra-Sharma}}{{Cuesta-Lazaro} \& {Mishra-Sharma}}{2024}]{Cuesta-Lazaro:2024:PhRvD:}
{Cuesta-Lazaro} C.,  {Mishra-Sharma} S.,  2024, \mn@doi [\prd] {10.1103/PhysRevD.109.123531}, \href {https://ui.adsabs.harvard.edu/abs/2024PhRvD.109l3531C} {109, 123531}

\bibitem[\protect\citeauthoryear{Dosovitskiy et~al.,}{Dosovitskiy et~al.}{2021}]{dosovitskiy2021imageworth16x16words}
Dosovitskiy A.,  et~al., 2021, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (\mn@eprint {arXiv} {2010.11929}), \url {https://arxiv.org/abs/2010.11929}

\bibitem[\protect\citeauthoryear{{Hadzhiyska}, {Bose}, {Eisenstein}, {Hernquist}  \& {Spergel}}{{Hadzhiyska} et~al.}{2020}]{Hadzhiyska:2020:MNRAS:}
{Hadzhiyska} B.,  {Bose} S.,  {Eisenstein} D.,  {Hernquist} L.,   {Spergel} D.~N.,  2020, \mn@doi [\mnras] {10.1093/mnras/staa623}, \href {https://ui.adsabs.harvard.edu/abs/2020MNRAS.493.5506H} {493, 5506}

\bibitem[\protect\citeauthoryear{{Hausen}, {Robertson}, {Zhu}, {Gnedin}, {Madau}, {Schneider}, {Villasenor}  \& {Drakos}}{{Hausen} et~al.}{2023}]{Hausen:2023:ApJ:}
{Hausen} R.,  {Robertson} B.~E.,  {Zhu} H.,  {Gnedin} N.~Y.,  {Madau} P.,  {Schneider} E.~E.,  {Villasenor} B.,   {Drakos} N.~E.,  2023, \mn@doi [\apj] {10.3847/1538-4357/acb25c}, \href {https://ui.adsabs.harvard.edu/abs/2023ApJ...945..122H} {945, 122}

\bibitem[\protect\citeauthoryear{{Jespersen}, {Cranmer}, {Melchior}, {Ho}, {Somerville}  \& {Gabrielpillai}}{{Jespersen} et~al.}{2022}]{Jespersen:2022:ApJ:}
{Jespersen} C.~K.,  {Cranmer} M.,  {Melchior} P.,  {Ho} S.,  {Somerville} R.~S.,   {Gabrielpillai} A.,  2022, \mn@doi [\apj] {10.3847/1538-4357/ac9b18}, \href {https://ui.adsabs.harvard.edu/abs/2022ApJ...941....7J} {941, 7}

\bibitem[\protect\citeauthoryear{Katharopoulos, Vyas, Pappas  \& Fleuret}{Katharopoulos et~al.}{2020}]{katharopoulos2020transformersrnnsfastautoregressive}
Katharopoulos A.,  Vyas A.,  Pappas N.,   Fleuret F.,  2020, Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (\mn@eprint {arXiv} {2006.16236}), \url {https://arxiv.org/abs/2006.16236}

\bibitem[\protect\citeauthoryear{{Lemos}, {Sharief}, {Malkin}, {Salhi}, {Stone}, {Perreault-Levasseur}  \& {Hezaveh}}{{Lemos} et~al.}{2024}]{Lemos:2024:arXiv:}
{Lemos} P.,  {Sharief} S.,  {Malkin} N.,  {Salhi} S.,  {Stone} C.,  {Perreault-Levasseur} L.,   {Hezaveh} Y.,  2024, \mn@doi [arXiv e-prints] {10.48550/arXiv.2402.04355}, \href {https://ui.adsabs.harvard.edu/abs/2024arXiv240204355L} {p. arXiv:2402.04355}

\bibitem[\protect\citeauthoryear{{Li}, {Ni}, {Croft}, {Di Matteo}, {Bird}  \& {Feng}}{{Li} et~al.}{2021}]{Li:2021:PNAS:}
{Li} Y.,  {Ni} Y.,  {Croft} R. A.~C.,  {Di Matteo} T.,  {Bird} S.,   {Feng} Y.,  2021, \mn@doi [Proceedings of the National Academy of Science] {10.1073/pnas.2022038118}, \href {https://ui.adsabs.harvard.edu/abs/2021PNAS..11822038L} {118, e2022038118}

\bibitem[\protect\citeauthoryear{{Lovell} et~al.,}{{Lovell} et~al.}{2023}]{Lovell:2023:mla:}
{Lovell} C.~C.,  et~al., 2023, in Machine Learning for Astrophysics. p.~21 (\mn@eprint {arXiv} {2307.06967}), \mn@doi{10.48550/arXiv.2307.06967}

\bibitem[\protect\citeauthoryear{{Lovell} et~al.,}{{Lovell} et~al.}{2024}]{Lovell:2024:arXiv:}
{Lovell} C.~C.,  et~al., 2024, \mn@doi [arXiv e-prints] {10.48550/arXiv.2411.13960}, \href {https://ui.adsabs.harvard.edu/abs/2024arXiv241113960L} {p. arXiv:2411.13960}

\bibitem[\protect\citeauthoryear{{Maltz} et~al.,}{{Maltz} et~al.}{2025}]{Maltz:2025:MNRAS:}
{Maltz} M. G.~A.,  et~al., 2025, \mn@doi [\mnras] {10.1093/mnras/staf410}, \href {https://ui.adsabs.harvard.edu/abs/2025MNRAS.538.3084M} {538, 3084}

\bibitem[\protect\citeauthoryear{{Ni} et~al.,}{{Ni} et~al.}{2023}]{Ni:2023:ApJ:}
{Ni} Y.,  et~al., 2023, \mn@doi [\apj] {10.3847/1538-4357/ad022a}, \href {https://ui.adsabs.harvard.edu/abs/2023ApJ...959..136N} {959, 136}

\bibitem[\protect\citeauthoryear{{Pakmor} et~al.,}{{Pakmor} et~al.}{2023}]{Pakmor:2023:MNRAS:}
{Pakmor} R.,  et~al., 2023, \mn@doi [\mnras] {10.1093/mnras/stac3620}, \href {https://ui.adsabs.harvard.edu/abs/2023MNRAS.524.2539P} {524, 2539}

\bibitem[\protect\citeauthoryear{{Pandey}, {Lanusse}, {Modi}  \& {Wandelt}}{{Pandey} et~al.}{2024}]{Pandey:2024:arXiv:gotham}
{Pandey} S.,  {Lanusse} F.,  {Modi} C.,   {Wandelt} B.~D.,  2024, \mn@doi [arXiv e-prints] {10.48550/arXiv.2409.11401}, \href {https://ui.adsabs.harvard.edu/abs/2024arXiv240911401P} {p. arXiv:2409.11401}

\bibitem[\protect\citeauthoryear{{Rodrigues}, {de Santi}, {Abramo}  \& {Montero-Dorta}}{{Rodrigues} et~al.}{2025}]{Rodrigues:2025:A&A:}
{Rodrigues} N. V.~N.,  {de Santi} N. S.~M.,  {Abramo} R.,   {Montero-Dorta} A.~D.,  2025, \mn@doi [\aap] {10.1051/0004-6361/202453284}, \href {https://ui.adsabs.harvard.edu/abs/2025A&A...698A...3R} {698, A3}

\bibitem[\protect\citeauthoryear{Su, Lu, Pan, Murtadha, Wen  \& Liu}{Su et~al.}{2023}]{su2023roformerenhancedtransformerrotary}
Su J.,  Lu Y.,  Pan S.,  Murtadha A.,  Wen B.,   Liu Y.,  2023, RoFormer: Enhanced Transformer with Rotary Position Embedding (\mn@eprint {arXiv} {2104.09864}), \url {https://arxiv.org/abs/2104.09864}

\bibitem[\protect\citeauthoryear{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser  \& Polosukhin}{Vaswani et~al.}{2023}]{vaswani2023attentionneed}
Vaswani A.,  Shazeer N.,  Parmar N.,  Uszkoreit J.,  Jones L.,  Gomez A.~N.,  Kaiser L.,   Polosukhin I.,  2023, Attention Is All You Need (\mn@eprint {arXiv} {1706.03762}), \url {https://arxiv.org/abs/1706.03762}

\bibitem[\protect\citeauthoryear{Villaescusa-Navarro et~al.,}{Villaescusa-Navarro et~al.}{2021}]{Villaescusa_Navarro_2021}
Villaescusa-Navarro F.,  et~al., 2021, \mn@doi [The Astrophysical Journal] {10.3847/1538-4357/abf7ba}, 915, 71

\bibitem[\protect\citeauthoryear{{Vogelsberger}, {Marinacci}, {Torrey}  \& {Puchwein}}{{Vogelsberger} et~al.}{2020}]{Vogelsberger:2020:NatRP:}
{Vogelsberger} M.,  {Marinacci} F.,  {Torrey} P.,   {Puchwein} E.,  2020, \mn@doi [Nature Reviews Physics] {10.1038/s42254-019-0127-2}, \href {https://ui.adsabs.harvard.edu/abs/2020NatRP...2...42V} {2, 42}

\bibitem[\protect\citeauthoryear{Woo, Park, Lee  \& Kweon}{Woo et~al.}{2018}]{woo2018cbamconvolutionalblockattention}
Woo S.,  Park J.,  Lee J.-Y.,   Kweon I.~S.,  2018, CBAM: Convolutional Block Attention Module (\mn@eprint {arXiv} {1807.06521}), \url {https://arxiv.org/abs/1807.06521}

\bibitem[\protect\citeauthoryear{{Zhang}, {Wang}, {Zhang}, {Sun}, {He}, {Contardo}, {Villaescusa-Navarro}  \& {Ho}}{{Zhang} et~al.}{2019}]{Zhang:2019:arXiv:}
{Zhang} X.,  {Wang} Y.,  {Zhang} W.,  {Sun} Y.,  {He} S.,  {Contardo} G.,  {Villaescusa-Navarro} F.,   {Ho} S.,  2019, \mn@doi [arXiv e-prints] {10.48550/arXiv.1902.05965}, \href {https://ui.adsabs.harvard.edu/abs/2019arXiv190205965Z} {p. arXiv:1902.05965}

\bibitem[\protect\citeauthoryear{{Zheng} et~al.,}{{Zheng} et~al.}{2005}]{Zheng:2005:ApJ:}
{Zheng} Z.,  et~al., 2005, \mn@doi [\apj] {10.1086/466510}, \href {https://ui.adsabs.harvard.edu/abs/2005ApJ...633..791Z} {633, 791}

\bibitem[\protect\citeauthoryear{{Zou}, {Gao}, {Zhou}  \& {Kong}}{{Zou} et~al.}{2019}]{Zou:2019:ApJS:}
{Zou} H.,  {Gao} J.,  {Zhou} X.,   {Kong} X.,  2019, \mn@doi [\apjs] {10.3847/1538-4365/ab1847}, \href {https://ui.adsabs.harvard.edu/abs/2019ApJS..242....8Z} {242, 8}

\bibitem[\protect\citeauthoryear{{de Santi}, {Rodrigues}, {Montero-Dorta}, {Abramo}, {Tucci}  \& {Artale}}{{de Santi} et~al.}{2022}]{deSanti:2022:MNRAS:}
{de Santi} N. S.~M.,  {Rodrigues} N. V.~N.,  {Montero-Dorta} A.~D.,  {Abramo} L.~R.,  {Tucci} B.,   {Artale} M.~C.,  2022, \mn@doi [\mnras] {10.1093/mnras/stac1469}, \href {https://ui.adsabs.harvard.edu/abs/2022MNRAS.514.2463D} {514, 2463}

\makeatother
\end{thebibliography}

```

5. **Author Information:**
- Lead Author: {'name': 'Shivam Pandey'}
- Full Authors List:
```yaml
Shivam Pandey: {}
Chris Lovell:
  postdoc:
    start: 2025-07-17
    thesis: null
  original_image: images/originals/chris_lovell.png
  image: /assets/group/images/chris_lovell.jpg
Chirag Modi: {}
Benjamin D. Wandelt: {}

```
This YAML file provides a concise snapshot of an academic research group. It lists members by name along with their academic roles—ranging from Part III and summer projects to MPhil, PhD, and postdoctoral positions—with corresponding dates, thesis topics, and supervisor details. Supplementary metadata includes image paths and links to personal or departmental webpages. A dedicated "coi" section profiles senior researchers, highlighting the group’s collaborative mentoring network and career trajectories in cosmology, astrophysics, and Bayesian data analysis.



====================================================================================
Final Output Instructions
====================================================================================

- Combine all data sources to create a seamless, engaging narrative.
- Follow the exact Markdown output format provided at the top.
- Do not include any extra explanation, commentary, or wrapping beyond the specified Markdown.
- Validate that every bibliographic reference with a DOI or arXiv identifier is converted into a Markdown link as per the examples.
- Validate that every Markdown author link corresponds to a link in the author information block.
- Before finalizing, confirm that no LaTeX citation commands or other undesired formatting remain.
- Before finalizing, confirm that the link to the paper itself [2511.08438](https://arxiv.org/abs/2511.08438) is featured in the first sentence.

Generate only the final Markdown output that meets all these requirements.

{% endraw %}