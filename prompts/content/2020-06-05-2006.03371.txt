{% raw %}

Title: Create a Markdown Blog Post Integrating Research Details and a Featured Paper
====================================================================================

This task involves generating a Markdown file (ready for a GitHub-served Jekyll site) that integrates our research details with a featured research paper. The output must follow the exact format and conventions described below.

====================================================================================
Output Format (Markdown):
------------------------------------------------------------------------------------
---
layout: post
title:  "Nested sampling cross-checks using order statistics"
date:   2020-06-05
categories: papers
---
![AI generated image](/assets/images/posts/2020-06-05-2006.03371.png)

<!-- BEGINNING OF GENERATED POST -->
<!-- END OF GENERATED POST -->

<img src="/assets/group/images/will_handley.jpg" alt="Will Handley" style="width: auto; height: 25vw;">

Content generated by [gemini-2.5-pro](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/2020-06-05-2006.03371.txt).

Image generated by [imagen-3.0-generate-002](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/2020-06-05-2006.03371.txt).

------------------------------------------------------------------------------------
====================================================================================

Please adhere strictly to the following instructions:

====================================================================================
Section 1: Content Creation Instructions
====================================================================================

1. **Generate the Page Body:**
   - Write a well-composed, engaging narrative that is suitable for a scholarly audience interested in advanced AI and astrophysics.
   - Ensure the narrative is original and reflective of the tone and style and content in the "Homepage Content" block (provided below), but do not reuse its content.
   - Use bullet points, subheadings, or other formatting to enhance readability.

2. **Highlight Key Research Details:**
   - Emphasize the contributions and impact of the paper, focusing on its methodology, significance, and context within current research.
   - Specifically highlight the lead author ({'name': 'Andrew Fowlie'}). When referencing any author, use Markdown links from the Author Information block (choose academic or GitHub links over social media).

3. **Integrate Data from Multiple Sources:**
   - Seamlessly weave information from the following:
     - **Paper Metadata (YAML):** Essential details including the title and authors.
     - **Paper Source (TeX):** Technical content from the paper.
     - **Bibliographic Information (bbl):** Extract bibliographic references.
     - **Author Information (YAML):** Profile details for constructing Markdown links.
   - Merge insights from the Paper Metadata, TeX source, Bibliographic Information, and Author Information blocks into a coherent narrativeâ€”do not treat these as separate or isolated pieces.
   - Insert the generated narrative between the HTML comments:
     <!-- BEGINNING OF GENERATED POST --> and <!-- END OF GENERATED POST -->

4. **Generate Bibliographic References:**
   - Review the Bibliographic Information block carefully.
   - For each reference that includes a DOI or arXiv identifier:
     - For DOIs, generate a link formatted as:
       [10.1234/xyz](https://doi.org/10.1234/xyz)
     - For arXiv entries, generate a link formatted as:
       [2103.12345](https://arxiv.org/abs/2103.12345)
    - **Important:** Do not use any LaTeX citation commands (e.g., `\cite{...}`). Every reference must be rendered directly as a Markdown link. For example, instead of `\cite{mycitation}`, output `[mycitation](https://doi.org/mycitation)`
        - **Incorrect:** `\cite{10.1234/xyz}`  
        - **Correct:** `[10.1234/xyz](https://doi.org/10.1234/xyz)`
   - Ensure that at least three (3) of the most relevant references are naturally integrated into the narrative.
   - Ensure that the link to the Featured paper [2006.03371](https://arxiv.org/abs/2006.03371) is included in the first sentence.

5. **Final Formatting Requirements:**
   - The output must be plain Markdown; do not wrap it in Markdown code fences.
   - Preserve the YAML front matter exactly as provided.

====================================================================================
Section 2: Provided Data for Integration
====================================================================================

1. **Homepage Content (Tone and Style Reference):**
```markdown
---
layout: home
---

![AI generated image](/assets/images/index.png)

<!-- START OF WEBSITE SUMMARY -->
The Handley Research Group stands at the forefront of cosmological exploration, pioneering novel approaches that fuse fundamental physics with the transformative power of artificial intelligence. We are a dynamic team of researchers, including PhD students, postdoctoral fellows, and project students, based at the University of Cambridge. Our mission is to unravel the mysteries of the Universe, from its earliest moments to its present-day structure and ultimate fate. We tackle fundamental questions in cosmology and astrophysics, with a particular focus on leveraging advanced Bayesian statistical methods and AI to push the frontiers of scientific discovery. Our research spans a wide array of topics, including the [primordial Universe](https://arxiv.org/abs/1907.08524), [inflation](https://arxiv.org/abs/1807.06211), the nature of [dark energy](https://arxiv.org/abs/2503.08658) and [dark matter](https://arxiv.org/abs/2405.17548), [21-cm cosmology](https://arxiv.org/abs/2210.07409), the [Cosmic Microwave Background (CMB)](https://arxiv.org/abs/1807.06209), and [gravitational wave astrophysics](https://arxiv.org/abs/2411.17663).

### Our Research Approach: Innovation at the Intersection of Physics and AI

At The Handley Research Group, we develop and apply cutting-edge computational techniques to analyze complex astronomical datasets. Our work is characterized by a deep commitment to principled [Bayesian inference](https://arxiv.org/abs/2205.15570) and the innovative application of [artificial intelligence (AI) and machine learning (ML)](https://arxiv.org/abs/2504.10230).

**Key Research Themes:**
*   **Cosmology:** We investigate the early Universe, including [quantum initial conditions for inflation](https://arxiv.org/abs/2002.07042) and the generation of [primordial power spectra](https://arxiv.org/abs/2112.07547). We explore the enigmatic nature of [dark energy, using methods like non-parametric reconstructions](https://arxiv.org/abs/2503.08658), and search for new insights into [dark matter](https://arxiv.org/abs/2405.17548). A significant portion of our efforts is dedicated to [21-cm cosmology](https://arxiv.org/abs/2104.04336), aiming to detect faint signals from the Cosmic Dawn and the Epoch of Reionization.
*   **Gravitational Wave Astrophysics:** We develop methods for [analyzing gravitational wave signals](https://arxiv.org/abs/2411.17663), extracting information about extreme astrophysical events and fundamental physics.
*   **Bayesian Methods & AI for Physical Sciences:** A core component of our research is the development of novel statistical and AI-driven methodologies. This includes advancing [nested sampling techniques](https://arxiv.org/abs/1506.00171) (e.g., [PolyChord](https://arxiv.org/abs/1506.00171), [dynamic nested sampling](https://arxiv.org/abs/1704.03459), and [accelerated nested sampling with $\beta$-flows](https://arxiv.org/abs/2411.17663)), creating powerful [simulation-based inference (SBI) frameworks](https://arxiv.org/abs/2504.10230), and employing [machine learning for tasks such as radiometer calibration](https://arxiv.org/abs/2504.16791), [cosmological emulation](https://arxiv.org/abs/2503.13263), and [mitigating radio frequency interference](https://arxiv.org/abs/2211.15448). We also explore the potential of [foundation models for scientific discovery](https://arxiv.org/abs/2401.00096).

**Technical Contributions:**
Our group has a strong track record of developing widely-used scientific software. Notable examples include:
*   [**PolyChord**](https://arxiv.org/abs/1506.00171): A next-generation nested sampling algorithm for Bayesian computation.
*   [**anesthetic**](https://arxiv.org/abs/1905.04768): A Python package for processing and visualizing nested sampling runs.
*   [**GLOBALEMU**](https://arxiv.org/abs/2104.04336): An emulator for the sky-averaged 21-cm signal.
*   [**maxsmooth**](https://arxiv.org/abs/2007.14970): A tool for rapid maximally smooth function fitting.
*   [**margarine**](https://arxiv.org/abs/2205.12841): For marginal Bayesian statistics using normalizing flows and KDEs.
*   [**fgivenx**](https://arxiv.org/abs/1908.01711): A package for functional posterior plotting.
*   [**nestcheck**](https://arxiv.org/abs/1804.06406): Diagnostic tests for nested sampling calculations.

### Impact and Discoveries
Our research has led to significant advancements in cosmological data analysis and yielded new insights into the Universe. Key achievements include:
*   Pioneering the development and application of advanced Bayesian inference tools, such as [PolyChord](https://arxiv.org/abs/1506.00171), which has become a cornerstone for cosmological parameter estimation and model comparison globally.
*   Making significant contributions to the analysis of major cosmological datasets, including the [Planck mission](https://arxiv.org/abs/1807.06209), providing some of the tightest constraints on cosmological parameters and models of [inflation](https://arxiv.org/abs/1807.06211).
*   Developing novel AI-driven approaches for astrophysical challenges, such as using [machine learning for radiometer calibration in 21-cm experiments](https://arxiv.org/abs/2504.16791) and [simulation-based inference for extracting cosmological information from galaxy clusters](https://arxiv.org/abs/2504.10230).
*   Probing the nature of dark energy through innovative [non-parametric reconstructions of its equation of state](https://arxiv.org/abs/2503.08658) from combined datasets.
*   Advancing our understanding of the early Universe through detailed studies of [21-cm signals from the Cosmic Dawn and Epoch of Reionization](https://arxiv.org/abs/2301.03298), including the development of sophisticated foreground modelling techniques and emulators like [GLOBALEMU](https://arxiv.org/abs/2104.04336).
*   Developing new statistical methods for quantifying tensions between cosmological datasets ([Quantifying tensions in cosmological parameters: Interpreting the DES evidence ratio](https://arxiv.org/abs/1902.04029)) and for robust Bayesian model selection ([Bayesian model selection without evidences: application to the dark energy equation-of-state](https://arxiv.org/abs/1506.09024)).
*   Exploring fundamental physics questions such as potential [parity violation in the Large-Scale Structure using machine learning](https://arxiv.org/abs/2410.16030).

### Charting the Future: AI-Powered Cosmological Discovery
The Handley Research Group is poised to lead a new era of cosmological analysis, driven by the explosive growth in data from next-generation observatories and transformative advances in artificial intelligence. Our future ambitions are centred on harnessing these capabilities to address the most pressing questions in fundamental physics.

**Strategic Research Pillars:**
*   **Next-Generation Simulation-Based Inference (SBI):** We are developing advanced SBI frameworks to move beyond traditional likelihood-based analyses. This involves creating sophisticated codes for simulating [Cosmic Microwave Background (CMB)](https://arxiv.org/abs/1908.00906) and [Baryon Acoustic Oscillation (BAO)](https://arxiv.org/abs/1607.00270) datasets from surveys like DESI and 4MOST, incorporating realistic astrophysical effects and systematic uncertainties. Our AI initiatives in this area focus on developing and implementing cutting-edge SBI algorithms, particularly [neural ratio estimation (NRE) methods](https://arxiv.org/abs/2407.15478), to enable robust and scalable inference from these complex simulations.
*   **Probing Fundamental Physics:** Our enhanced analytical toolkit will be deployed to test the standard cosmological model ($\Lambda$CDM) with unprecedented precision and to explore [extensions to Einstein's General Relativity](https://arxiv.org/abs/2006.03581). We aim to constrain a wide range of theoretical models, from modified gravity to the nature of [dark matter](https://arxiv.org/abs/2106.02056) and [dark energy](https://arxiv.org/abs/1701.08165). This includes leveraging data from upcoming [gravitational wave observatories](https://arxiv.org/abs/1803.10210) like LISA, alongside CMB and large-scale structure surveys from facilities such as Euclid and JWST.
*   **Synergies with Particle Physics:** We will continue to strengthen the connection between cosmology and particle physics by expanding the [GAMBIT framework](https://arxiv.org/abs/2009.03286) to interface with our new SBI tools. This will facilitate joint analyses of cosmological and particle physics data, providing a holistic approach to understanding the Universe's fundamental constituents.
*   **AI-Driven Theoretical Exploration:** We are pioneering the use of AI, including [large language models and symbolic computation](https://arxiv.org/abs/2401.00096), to automate and accelerate the process of theoretical model building and testing. This innovative approach will allow us to explore a broader landscape of physical theories and derive new constraints from diverse astrophysical datasets, such as those from GAIA.

Our overarching goal is to remain at the forefront of scientific discovery by integrating the latest AI advancements into every stage of our research, from theoretical modeling to data analysis and interpretation. We are excited by the prospect of using these powerful new tools to unlock the secrets of the cosmos.
<!-- END OF WEBSITE SUMMARY -->

Content generated by [gemini-2.5-pro-preview-05-06](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/content/index.txt).

Image generated by [imagen-3.0-generate-002](https://deepmind.google/technologies/gemini/) using [this prompt](/prompts/images/index.txt).
```

2. **Paper Metadata:**
```yaml
!!python/object/new:feedparser.util.FeedParserDict
dictitems:
  id: http://arxiv.org/abs/2006.03371v2
  guidislink: true
  link: http://arxiv.org/abs/2006.03371v2
  updated: '2020-08-24T03:52:06Z'
  updated_parsed: !!python/object/apply:time.struct_time
  - !!python/tuple
    - 2020
    - 8
    - 24
    - 3
    - 52
    - 6
    - 0
    - 237
    - 0
  - tm_zone: null
    tm_gmtoff: null
  published: '2020-06-05T11:19:03Z'
  published_parsed: !!python/object/apply:time.struct_time
  - !!python/tuple
    - 2020
    - 6
    - 5
    - 11
    - 19
    - 3
    - 4
    - 157
    - 0
  - tm_zone: null
    tm_gmtoff: null
  title: Nested sampling cross-checks using order statistics
  title_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      type: text/plain
      language: null
      base: ''
      value: Nested sampling cross-checks using order statistics
  summary: 'Nested sampling (NS) is an invaluable tool in data analysis in modern

    astrophysics, cosmology, gravitational wave astronomy and particle physics. We

    identify a previously unused property of NS related to order statistics: the

    insertion indexes of new live points into the existing live points should be

    uniformly distributed. This observation enabled us to create a novel

    cross-check of single NS runs. The tests can detect when an NS run failed to

    sample new live points from the constrained prior and plateaus in the

    likelihood function, which break an assumption of NS and thus leads to

    unreliable results. We applied our cross-check to NS runs on toy functions with

    known analytic results in 2 - 50 dimensions, showing that our approach can

    detect problematic runs on a variety of likelihoods, settings and dimensions.

    As an example of a realistic application, we cross-checked NS runs performed in

    the context of cosmological model selection. Since the cross-check is simple,

    we recommend that it become a mandatory test for every applicable NS run.'
  summary_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      type: text/plain
      language: null
      base: ''
      value: 'Nested sampling (NS) is an invaluable tool in data analysis in modern

        astrophysics, cosmology, gravitational wave astronomy and particle physics.
        We

        identify a previously unused property of NS related to order statistics: the

        insertion indexes of new live points into the existing live points should
        be

        uniformly distributed. This observation enabled us to create a novel

        cross-check of single NS runs. The tests can detect when an NS run failed
        to

        sample new live points from the constrained prior and plateaus in the

        likelihood function, which break an assumption of NS and thus leads to

        unreliable results. We applied our cross-check to NS runs on toy functions
        with

        known analytic results in 2 - 50 dimensions, showing that our approach can

        detect problematic runs on a variety of likelihoods, settings and dimensions.

        As an example of a realistic application, we cross-checked NS runs performed
        in

        the context of cosmological model selection. Since the cross-check is simple,

        we recommend that it become a mandatory test for every applicable NS run.'
  authors:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Andrew Fowlie
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Will Handley
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Liangliang Su
  author_detail: !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      name: Liangliang Su
  author: Liangliang Su
  arxiv_doi: 10.1093/mnras/staa2345
  links:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      title: doi
      href: http://dx.doi.org/10.1093/mnras/staa2345
      rel: related
      type: text/html
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      href: http://arxiv.org/abs/2006.03371v2
      rel: alternate
      type: text/html
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      title: pdf
      href: http://arxiv.org/pdf/2006.03371v2
      rel: related
      type: application/pdf
  arxiv_comment: minor changes & clarifications. closely matches published version
  arxiv_primary_category:
    term: stat.CO
    scheme: http://arxiv.org/schemas/atom
  tags:
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: stat.CO
      scheme: http://arxiv.org/schemas/atom
      label: null
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: astro-ph.CO
      scheme: http://arxiv.org/schemas/atom
      label: null
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: astro-ph.IM
      scheme: http://arxiv.org/schemas/atom
      label: null
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: hep-ph
      scheme: http://arxiv.org/schemas/atom
      label: null
  - !!python/object/new:feedparser.util.FeedParserDict
    dictitems:
      term: physics.data-an
      scheme: http://arxiv.org/schemas/atom
      label: null

```

3. **Paper Source (TeX):**
```tex
BAO & 0.89 & 0.82 & 0.07 & 0.05\\
lensing+BAO & 0.72 & 0.54 & 0.19 & 0.43\\
lensing & 0.26 & 0.14 & 0.04 & 0.64\\
lensing+S$H_0$ES & 0.08 & 0.08 & 0.78 & 0.04\\
Planck+BAO & 0.39 & 0.56 & 0.14 & 0.43\\
Planck+lensing+BAO & 0.68 & 0.69 & 0.70 & 0.27\\
Planck+lensing & 0.94 & 0.49 & 0.89 & 0.72\\
Planck+lensing+S$H_0$ES & 0.92 & 0.92 & 0.33 & 0.82\\
Planck & 0.81 & 0.69 & 0.84 & 0.88\\
Planck+S$H_0$ES & 0.20 & 0.48 & 0.92 & 0.97\\
S$H_0$ES & 0.59 & 0.59 & 0.98 & 0.98\\\documentclass[a4paper,fleqn,usenatbib]{mnras}
% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{hyperref}
%\usepackage{natbib}
\usepackage{xspace}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}

% adjust algorithm appearance
\SetAlCapNameFnt{\normalsize}
\SetAlCapFnt{\normalsize}

% fonts
%\usepackage[english]{babel}
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
%\usepackage[scaled=1.04]{biolinum}
%\renewcommand*\familydefault{\rmdefault}
%\usepackage{fourier}
%\usepackage[scaled=0.83]{beramono}
%\usepackage{microtype}

% journal names
\usepackage{aas_macros}

\newcommand{\code}{\textsf}

% nested sampling macros
\newcommand{\nlive}{n_\text{live}}
\newcommand{\niter}{n_\text{iter}}
\newcommand{\pvalue}{\text{\textit{p}-value}\xspace}
\newcommand{\pvalues}{\text{\pvalue{}s}\xspace}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\logZ}{\ensuremath{\log\Z}\xspace}
\newcommand{\like}{\mathcal{L}}
\newcommand{\threshold}{\like^\star}
\newcommand{\pg}[2]{p\left(#1\,\rvert\, #2\right)}
\newcommand{\p}[1]{p\left(#1\right)}
\newcommand{\intd}{\text{d}}
\newcommand{\params}{\mathbf{\Theta}}
\newcommand{\stoppingtol}{\epsilon}
\newcommand{\efr}{\ensuremath{\code{efr}}\xspace}
\newcommand{\nr}{\ensuremath{n_r}\xspace}

% distributions
\newcommand{\loggamma}{\ln\Gamma}
\newcommand{\uniform}{\mathcal{U}}
\newcommand{\normal}{\mathcal{N}}

% ref to sections etc
\usepackage{cleveref}

% comments
\usepackage[usenames]{xcolor}
\newcommand{\AF}[1]{{\color{blue}\textbf{TODO AF:} \textit{#1}}}
\newcommand{\WH}[1]{{\color{red}\textbf{TODO WH:} \textit{#1}}}
\newcommand{\LL}[1]{{\color{green}\textbf{TODO LL:} \textit{#1}}}

% codes
\newcommand{\MN}{\code{MultiNest}\xspace}
\newcommand{\PC}{\code{PolyChord}\xspace}
\newcommand{\MNVersion}{\code{\MN-3.12}\xspace}
\newcommand{\PCVersion}{\code{\PC-1.17.1}\xspace}
\newcommand{\anesthetic}{\code{anesthetic}}

% settings we used
\newcommand{\nliveSetting}{1000\xspace}
\newcommand{\tolSetting}{0.01\xspace}
\newcommand{\nrepeatSetting}{100\xspace}
\newcommand{\nrepeatsPerfectSetting}{10,000\xspace}
\newcommand{\nrepeatsMCSetting}{100,000\xspace}
\newcommand{\niterPerfectSetting}{10,000\xspace}

% Highlight changes in resubmission
%\newcommand{\add}[1]{\textcolor{red}{\textbf{#1}}}
%\newcommand{\remove}[1]{\textcolor{gray}{\textit{\st{#1}}}}

% adjust header
\makeatletter
\def\@printed{}
\def\@journal{}
\def\@oddfoot{}
\def\@evenfoot{}
\makeatother

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title{Nested sampling cross-checks using order statistics}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[A. Fowlie et al.]{%
    Andrew Fowlie$^{1}$\thanks{andrew.j.fowlie@njnu.edu.cn},
    Will Handley$^{2,3}$\thanks{wh260@cam.ac.uk},
    and Liangliang Su$^{1}$\thanks{191002001@stu.njnu.edu.cn}
    \\
% List of institutions
$^{1}$Department of Physics and Institute of Theoretical Physics, Nanjing Normal University, Nanjing, Jiangsu 210023, China\\
$^{2}$Astrophysics Group, Cavendish Laboratory, J.J.Thomson Avenue, Cambridge, CB3 0HE, UK\\
$^{3}$Kavli Institute for Cosmology, Madingley Road, Cambridge, CB3 0HA, UK
}

% These dates will be filled out by the publisher
\date{}
% \date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2020}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle


\begin{abstract}
Nested sampling (NS) is an invaluable tool in data analysis in modern astrophysics, cosmology, gravitational wave astronomy and particle physics. 
%
We identify a previously unused property of NS related to order statistics: the insertion indexes of new live points into the existing live points should be uniformly distributed.
%
This observation enabled us to create a novel cross-check of single NS runs.
%
The tests can detect when an NS run failed to sample new live points from the constrained prior and plateaus in the likelihood function, which break an assumption of NS and thus leads to unreliable results.
%
We applied our cross-check to NS runs on toy functions with known analytic results in $2$ -- $50$ dimensions, showing that our approach can detect problematic runs on a variety of likelihoods, settings and dimensions.
%
As an example of a realistic application, we cross-checked NS runs performed in the context of cosmological model selection.
%
Since the cross-check is simple, we recommend that it become a mandatory test for every applicable NS run.
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
    methods: statistical -- methods: data analysis -- methods: numerical
\end{keywords}

\section{Introduction}

Nested sampling (NS) was introduced by Skilling in 2004~\citep{2004AIPC..735..395S,Skilling:2006gxv} as a novel algorithm for computing Bayesian evidences and posterior distributions. The algorithm requires few tuning parameters and can cope with traditionally-challenging multimodal and degenerate functions. As a result, popular implementations such as \MN~\citep{Feroz:2007kg,Feroz:2008xx,Feroz:2013hea}, \PC~\citep{Handley:2015fda,Handley:2015xxx} and \code{dynesty}~\citep{2020MNRAS.tmp..280S} have become invaluable tools in modern cosmology~\citep{Mukherjee:2005wg,Easther:2011yq,Martin:2013nzq,Hlozek:2014lca,2013JCAP...02..001A,Akrami:2018odb},
astrophysics~\citep{Trotta:2010mx,2007MNRAS.377L..74L,Buchner:2014nha},
gravitational wave astronomy~\citep{Veitch:2014wba,TheLIGOScientific:2016src,TheLIGOScientific:2016pea,Ashton:2018jfp},
and particle physics~\citep{Trotta:2008bp,Feroz:2008wr,Buchmueller:2013rsa,Workgroup:2017htr}.
Other NS applications include statistical physics~\citep{PhysRevLett.120.250601,PhysRevX.4.031034,doi:10.1021/jp1012973,PhysRevE.89.022302,PhysRevE.96.043311,doi:10.1063/1.4821761},
condensed matter physics~\citep{PhysRevB.93.174108},
and biology~\citep{10.1093/sysbio/syy050,10.1093/bioinformatics/btu675}.

In this work, we propose a cross-check of an important assumption in NS that works on single NS runs. This improves upon previous tests of NS that required toy functions with known analytic properties~\citep{2014arXiv1407.5459B} or multiple runs~\citep{Higson:2018cqj}. The cross-check detects faults in the compression of the parameter space that lead to biased estimates of the evidence. We demonstrate our method on toy functions and previous NS runs used for model selection in cosmology~\citep{Handley:2019tkm}. We anticipate that the cross-check could be applied as broadly as NS itself.

The paper is structured as follows. After recapitulating the relevant aspects of NS in \cref{sec:intro}, we introduce our approach in \cref{sec:test}. We apply our methods to toy functions and a cosmological likelihood in \cref{sec:examples}. We briefly discuss the possibility of using the insertion indexes to debias NS evidence estimates in \cref{sec:debiasing} before concluding in \cref{sec:conclusions}.

\section{NS algorithm}\label{sec:intro}

To establish our notation and explain our cross-check, we briefly summarize the NS algorithm. For more detailed and pedagogical introductions, see e.g., \citep{Skilling:2006gxv,Feroz:2008xx,Handley:2015fda,2020MNRAS.tmp..280S}. NS is primarily an algorithm for computing the Bayesian evidence of a model in light of data. Consider a model with parameters $\params$. The evidence may be written
\begin{equation}\label{eq:Z}
\Z \equiv \int_{\Omega_\params}  \like(\params) \, \pi(\params) \,\intd \params,  
\end{equation}
where $\pi(\params)$ is a prior density for the parameters and $\like(\params)$ is a likelihood function describing the probability of the observed experimental data. The evidence is a critical ingredient in Bayesian model selection in which models are compared by Bayes factors, since Bayes factors are ratios of evidences for two models,
\begin{equation}
B_{10} \equiv \frac{\Z_1}{\Z_0}.
\end{equation}
The Bayes factor $B_{10}$ tells us hows much more we should believe in model $1$ relative to model $0$ in light of experimental data. For an introduction to Bayes factors, see e.g., \citep{Kass:1995loi}.

NS works by casting \cref{eq:Z} as a one-dimensional integral via the volume variable,
\begin{equation}\label{eq:X}
X(\lambda) = \int_{\like(\params) > \lambda}  \pi(\params) \,\intd \params.
\end{equation}
This is the prior volume enclosed within the iso-likelihood contour defined by $\lambda$. The evidence may then be written as
\begin{equation}\label{eq:Z1d}
\Z = \int_0^1 \like(X) \,\intd X,
\end{equation}
where in the overloaded notation $\like(X)$ is the inverse of $X(\lambda)$. 

The remaining challenge is computing the one-dimensional integral in \cref{eq:Z1d}. In NS we begin from $\nlive$ live points drawn from the prior. At each iteration of the NS algorithm, we discard the point with the smallest likelihood, $\threshold$, and sample a replacement drawn from the constrained prior, that is, drawn from $\pi(\params)$ subject to $\like(\params) > \threshold$. By the statistical properties of random samples drawn from the constrained prior, we expect that the volume $X(\threshold)$ compresses by $t$ at each iteration, where
\begin{equation}\label{eq:t}
\langle \log t \rangle = -\frac{1}{\nlive}.
\end{equation}
This enables us to estimate the volume at the $i$-th iteration by $X_i \equiv X(\threshold_i) = e^{-i/\nlive}$ and write the one-dimensional integral using the trapezium rule,
\begin{equation}\label{eq:Z_sum}
\Z \approx \sum_i \threshold_i \, w_i, \qquad w_i = \tfrac12 \left(X_{i - 1} - X_{i + 1}\right).
\end{equation}
The algorithm terminates once an estimate of the maximum remaining evidence, $\Delta \Z$, is less than a specified fraction, $\stoppingtol$, of the total evidence found,
\begin{equation}
\frac{\Delta \Z}{\Z} < \stoppingtol.
\label{eqn:stop}
\end{equation}
The main numerical problem in an implementation of NS is efficiently sampling from the constrained prior.

\subsection{Sampling from the constrained prior}\label{sec:constrained_prior}

% NS runs could fail to produce accurate estimates of the evidence for a variety for reasons. 
% For example, although NS is particularly robust to the problems posed by multimodal distributions, modes could be missed.
% We focus on one particular problem: implementations of NS may fail to correctly sample from the constrained prior and thus produce biased evidences. Indeed,
Because rejection sampling from the entire prior would be impractically slow as the volume compresses exponentially, implementations of NS typically employ specialised subalgorithms to sample from the constrained prior. When these subalgorithms fail, the evidences may be unreliable. This was considered the most severe drawback of the NS algorithm in \citep{2018arXiv180503924S}.

One such subalgorithm is ellipsoidal sampling~\citep{Mukherjee:2005wg,Feroz:2007kg}, a rejection sampling algorithm in which the live points are bounded by a set of ellipsoids. Potential live points are sampled from the ellipsoids and accepted only if $\like > \threshold$. Ellipsoidal NS is implemented in \MN~\citep{Feroz:2007kg,Feroz:2008xx,Feroz:2013hea}. For this to faithfully sample from the constrained prior, the ellipsoids must completely enclose the iso-likelihood contour defined by $\threshold$. To ensure this is the case, the ellipsoids are expanded by a factor $1 / \efr$, with $\efr = 0.3$ recommended for reliable evidences.

Slice sampling~\citep{neal} is an alternative scheme for sampling from the constrained prior~\citep{aitken,Handley:2015fda}. A chord is drawn from a live point across the entire region enclosed by the iso-likelihood contour and a candidate point is drawn uniformly from along the chord. This is repeated \nr times to reduce correlations between the new point and the original live point. Slice sampling is implemented in \PC~\citep{Handley:2015fda,Handley:2015xxx}. The recommend number of repeats is $\nr = 2d$ for a $d$-dimensional function.

\subsection{Plateaus in the likelihood}\label{sec:plateaus}

Plateaus in the likelihood function, i.e., regions in which $\like(\params) = \text{const.}$, were discussed in \citep{2004AIPC..735..395S,Skilling:2006gxv} and more recently in \citep{2020arXiv200508602S}. In \citep{2020arXiv200508602S} it was stressed that they can lead to faulty estimates of the compression. In such cases, the live points are not uniformly distributed in $X$ (\cref{eq:X}), violating assumptions in \cref{eq:t}.

\section{Using insertion indexes}\label{sec:test}

\begingroup
\begin{table*}
    \centerline{%
\begin{tabular}{cccccccccc}
$\efr$ & $d$ & Analytic \logZ & Mean $\logZ\pm \Delta\logZ$ & $\sigma_{\logZ}$ & SEM \logZ & Inaccuracy & Bias & Median \pvalue & Median rolling \pvalue\\
\hline
\hyperref[sec:gaussian]{Gaussian}\\
\hline
\input{MN_gaussian.tex}
\hline
\hyperref[sec:rosenbrock]{Rosenbrock}\\
\hline
\input{MN_rosenbrock.tex}
\hline
\hyperref[sec:shells]{Shells}\\
\hline
\input{MN_shells.tex}
\hline
\hyperref[sec:gaussian-log-gamma]{Mixture}\\
\hline
\input{MN_mixture.tex}
\end{tabular}
}
\caption{\label{tab:MN_summary} Summary of results of our insertion index cross-check for \MN. The numerical results are the average from \nrepeatSetting runs. Biases and inaccuracies greater than $3$ and \pvalues less than $0.01$ are highlighted by red.}
\end{table*}
\endgroup

By \emph{insertion index}, we mean the index at which an element must be inserted to maintain order in an sorted list. With a left-sided convention, the insertion index $i$ of a sample $y$ in an sorted list $o$ is such that
\begin{equation}\label{eq:insertion_index}
o_{i - 1} < y \le o_i.
\end{equation}
The key idea in this paper is to use the insertion indexes of new live points relative to existing live points sorted by enclosed prior volume, $X$, to detect problems in sampling from the constrained prior. 
Since the relationship between volume and likelihood is monotonic, we can sort by volume by sorting by likelihood. If new live points are genuinely sampled from the constrained prior leading to a uniform distribution in $X$, the insertion indexes, $i$, should be discrete uniformly distributed from $0$ to $\nlive - 1$,
\begin{equation}\label{EQ:UNIFORM}
    i \sim \uniform(0, \nlive - 1).
\end{equation}
This result from order statistics is proven in \cref{app:proof}. During a NS run of $\niter$ iterations we thus find $\niter$ insertion indexes that should be uniformly distributed. Imagine, however, that during a NS run using ellipsoidal sampling, the ellipsoids encroached on the true iso-likelihood contour. In that case, the insertion indexes near the lowest-likelihood live points could be disfavoured, and the distribution of insertion indexes would deviate from uniformity. Alternatively, imagine that the likelihood function contains a plateau. Any initial live points that lie in the plateau share the same insertion index, leading to many repeated indexes and a strong deviation from a uniform distribution.

Thus, we can perform a statistical test on the insertion indexes to detect deviations from a uniform distribution. The choice of test isn't important to our general idea of using information in the insertion indexes, though in our examples we use a Kolmogorov-Smirnov (KS) test~\citep{smirnov1948,kolmogorov1933sulla}, which we found to be powerful, to compute a \pvalue from all the iterations. We describe the KS test in \cref{app:ks}.

Excepting plateaus, deviations from uniformity are caused by a \emph{change} in the distribution of new live points with respect to the existing live points. Since there is no technical challenge in sampling the initial live points from the prior, failures should typically occur during a run and thus be accompanied by a change in the distribution. In runs with many iterations in which a change occurs only once, the power of the test may be diluted by the many iterations before and after the distribution changes, as the insertion indexes before and after the change should be uniformly distributed. To mitigate this, we also perform multiple tests on chunks of iterations, find the smallest resulting \pvalue and apply a correction for multiple testing. We later refer to this as the rolling \pvalue. Since the volume compresses by $e$ in $\nlive$ iterations, we pick $\nlive$ as a reasonable size for a chunk of iterations. We treat each chunk as independent. The procedure for computing the rolling \pvalue is detailed in \cref{algo:rolling_p_value}. For clarity, let us stress that we later present \pvalues from all the iterations and rolling \pvalues. Functionality to perform these tests on \MN and \PC output is now included in \code{anesthetic-1.3.6 }~\citep{Handley:2019mfs}.


% \begin{figure}
% \begin{algorithm}[H]
% \SetAlgoLined
% \KwIn{$\niter$ insertion indexes}
% Compute empirical CDF for the insertion indexes\;
% Compute expected uniform CDF for insertion indexes\;
% Compute $D_n$ via \cref{eq:Dn}\;
% \KwRet{\pvalue from KS test with $D_n$ and $n = \niter$}
% \caption{Computing \pvalue from insertion indexes.}
% \label{algo:p_value}
% \end{algorithm}
% \end{figure}

\begin{algorithm}[h]
\SetAlgoLined
\KwIn{Set of $\niter$ insertion indexes}
 Split the insertion indexes into consecutive chunks of size $\nlive$. The size of the final chunk may be less than $\nlive$\;
 \ForEach{chunk of insertion indexes}{Apply KS test to obtain a \pvalue\;}
 Let $p$ equal the minimum of such \pvalues\;
 Let $n$ equal the number of chunks\;
 \KwRet{Rolling \pvalue{} --- minimum \pvalue adjusted for multiple tests,~$1 - (1 - p)^n$\;}
 \caption{The rolling \pvalue.}
 \label{algo:rolling_p_value}
\end{algorithm}

We furthermore neglect correlations between the insertion indexes. 
% We anticipate, however, that the insertion indexes \emph{repel} each other, possibly making tests that assume that the indexes are independent conservative. 
Finally, we stress that the magnitude of the deviation from uniform, as well as the \pvalue, should be noted. A small \pvalue alone isn't necessarily cause for concern, if the departure from uniformity is negligible. 

\section{Examples}\label{sec:examples}

\begingroup
\begin{table*}
    \centerline{%
\begin{tabular}{cccccccccc}
$d/\nr$ & $d$ & Analytic \logZ & Mean $\logZ\pm \Delta\logZ$ & $\sigma_{\logZ}$ & SEM \logZ & Inaccuracy & Bias & Median \pvalue & Median rolling \pvalue\\
\hline
\hyperref[sec:gaussian]{Gaussian}\\
\hline
\input{PC_gaussian.tex}
\hline
\hyperref[sec:rosenbrock]{Rosenbrock}\\
\hline
\input{PC_rosenbrock.tex}
\hline
\hyperref[sec:shells]{Shells}\\
\hline
\input{PC_shells.tex}
\hline
\hyperref[sec:gaussian-log-gamma]{Mixture}\\
\hline
\input{PC_mixture.tex}
\end{tabular}
}
\caption{\label{tab:PC_summary} Summary of results of our insertion index cross-check for \PC. See \cref{tab:MN_summary} for further details. In this table we show $d / \nr$, which may be thought of as a ``\PC efficiency'' analogue of the \MN efficiency $\efr$.}
\end{table*}
\endgroup

\subsection{Toy functions}

We now present detailed numerical examples of our cross-check using NS runs on toy functions using \MNVersion~\citep{Feroz:2007kg,Feroz:2008xx,Feroz:2013hea} and \PCVersion~\citep{Handley:2015fda,Handley:2015xxx}. We chose toy functions with known analytic evidences or precisely known numerical estimates of the evidence to demonstrate that biased results from NS are detectable with our approach. The toy functions are described in \cref{app:toy_problems}.

We performed \nrepeatSetting \MN and \PC runs on each toy function to study the statistical properties of their outputs. We used $\nlive = \nliveSetting$ and $\stoppingtol = \tolSetting$ throughout. To generate biased NS runs, we used inappropriate settings, e.g., $\efr > 1$ in \MN or few repeats $\nr<d$ in slice sampling in \PC, and difficult toy functions with $d \ge 30$. We post-processed the results using \anesthetic~\citep{anesthetic}. 

We summarise our results by the average \logZ and error estimate $\Delta \logZ$, and by the median \pvalue from all the insertion indexes and the median running \pvalue. We furthermore report the standard error on the mean, SEM \logZ, and the standard deviation, $\sigma_{\logZ}$. We use the error estimates to compute the average inaccuracy and bias,
\begin{align}
\text{inaccuracy} ={}& \frac{\logZ - \text{analytic}}{\Delta\logZ},\\ 
\text{bias} ={}& \frac{\logZ - \text{analytic}}{\text{SEM} \logZ}.
\end{align}
The inaccuracy shows whether the uncertainty reported by a code from single runs was reasonable.

We present our numerical results using \MN and \PC in tables~\ref{tab:MN_summary} and \ref{tab:PC_summary}, respectively. First, for the Gaussian function, the \MN estimates of \logZ were significantly biased for $d = 30$ and $50$ for all \efr settings, and for $d = 2$ and $10$ for $\efr = 10$. Our cross-check was successful, as the \pvalues corresponding to the biased results were tiny. 

For the Rosenbrock function, our cross-check detected a problem with \MN runs with $d=2$ and $\efr = 10$, even though the \MN evidence estimate was not biased. It did not detect a problem with $\efr=1$, even though the \logZ estimate was biased.  This was, however, the only problem for which this occurred for \MN.

For the shells function, the \MN estimates of \logZ were biased for many combinations of $d$ and \efr. The biased results were all identified by our cross-check with tiny \pvalues. Indeed, when $d=50$, even with $\efr=0.1$, we saw a bias of about $115$ and a median rolling \pvalue of about $10^{-23}$.

Lastly, the $d = 20$ mixture functions are particularly important, as \MN was known to produce biased results even with $\efr = 0.1$. Using all the insertion indexes, we find $\pvalue \approx 10^{-6}$ for this function, i.e., our cross-check successfully detects these failures.

In the analogous results for \PC in \cref{tab:PC_summary} we see fewer significantly biased estimates throughout, and only three biased results when using the recommended $\nr = 2d$ setting, which all occurred in the mixture function. We note, though, that the error estimates from \PC were reasonable even in these cases. The most extremely biased results were detected by our cross-check in the Gaussian, shells and mixture functions. 

Our cross-check detected faults in the $d=2$ shells function for $\nr = 1$, despite no evidence of bias in \PC results. Perhaps this should not be surprising, as \citep{2018arXiv180503924S,2020MNRAS.tmp..280S} suggest that independent samples from the constrained prior are not strictly necessary for correct evidence estimates.
The \pvalues, however, increased monotonically as \nr was increased, as expected. Lastly, we note that in many more cases than for \MN biases were not detected by our cross-check; this may be because the biases are smaller than they were for \MN.

In summary, for both \MN and \PC, we find that our cross-check can detect problematic NS runs in a variety of functions, settings and dimensions, although there is room for refinement. The problem detected by our cross-check usually leads to a faulty estimate of the evidence, though in a few cases the evidence estimate remains reasonable despite the apparent failure to sample correctly from the constrained prior.

\subsection{Cosmological model selection}

In \citep{Handley:2019tkm}, Handley considered the Bayesian evidence for a spatially closed Universe. Bayesian evidences from combinations of four datasets were computed using \PC for a spatially flat Universe and a curved Universe. The resulting Bayes factors showed that a closed Universe was favoured by odds of about $50/1$ for a particular set of data. There were $22$ NS computations in total. The \PC results are publicly archived at \citep{will_handley_2019_3371152}. We ran our cross-check on each of the $22$ NS runs in the archived data, finding \pvalues in the range $4\%$ to $98\%$. The results do not suggest problems with the NS runs. The $\pvalue$ of $4\%$  is not particularly alarming, especially considering that  we conducted $22$ tests. The full results are shown in \cref{tab:cosmology}.

\begingroup
\begin{table*}
\begin{tabular}{lcccc}
& \multicolumn{2}{c}{Flat} & \multicolumn{2}{c}{Curved}\\
\cline{2-3}\cline{4-5}
Data & \pvalue & Rolling \pvalue& \pvalue & Rolling \pvalue\\
\hline
\input{cosmology.tex}
\end{tabular}
\caption{\label{tab:cosmology}Insertion index cross-check applied to NS results from cosmological model selection in \citep{Handley:2019tkm}. We show \pvalues and rolling \pvalues for the NS evidence calculations for flat and curved Universe models with 11 datasets. See \citep{Handley:2019tkm} for further description of the datasets and models.}
\end{table*}
\endgroup

\subsection{Plateaus}

Let us consider the  one-dimensional function in example 2 from \citep{2020arXiv200508602S}. The likelihood function is defined piece-wise to be a Gaussian at the center and zero in the tails;
\begin{equation}
\like(x) \propto
\begin{cases} 
      e^{-\frac{(x - \mu)^2}{2\sigma^2}} & \left(\frac{x - \mu}{\sigma}\right)^2 \leq 1\\
      0 & \text{elsewhere}. 
   \end{cases}
\end{equation}
for $\mu = \tfrac12$ and $\sigma = 1$. The prior is uniform from $-3$ to $3$. We confirm that the NS algorithm produces biased estimates of the evidence in this function. However, since the likelihood is zero in $5/6$ of the prior, approximately $5/6$ of the initial live points have a likelihood of zero and share the same insertion index from \cref{eq:insertion_index}. This results in a tiny $\pvalue \simeq 0$ in our test.

\subsection{Perfect NS}

Lastly, we simulated perfect NS runs that correctly sample from the constrained prior. We simulated them by directly sampling compression factors from uniform distributions and never computing any likelihoods. Of course, with no likelihood we cannot compute an evidence, but we can simulate insertion indexes. We performed \nrepeatsPerfectSetting runs of perfect NS with \niterPerfectSetting iterations and computed the \pvalue via our KS test. 

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{p_value_histogram.pdf}
\caption{Histogram of \pvalue{}s from tests uniformity of insertion indexes from perfect NS (blue), samples from a discrete uniform distribution (orange) and samples from a continuous uniform distribution (green).}\label{fig:p_value_histogram}
\end{figure}

We furthermore computed \nrepeatsMCSetting \pvalue{}s from a KS test on \niterPerfectSetting samples drawn from a continuous uniform distribution and on \niterPerfectSetting samples drawn from a discrete uniform distribution with \nliveSetting bins.  We histogram all \pvalue{}s in \cref{fig:p_value_histogram}. Of course, the KS \pvalue{}s should be uniformly distributed in the continuous case and it appears that it is (green). The impact of discretization on the KS test is visible (orange) but small with \nliveSetting live points. The further impact of correlations amongst the samples in perfect NS (blue) isn't obvious. A two-sample Kolmogorov-Smirnoff test similar to the one-sample test in \cref{app:ks} uncovered a slight difference between \pvalues from perfect NS and from a discrete uniform ($\text{\pvalue} = 0.03$). This suggests that although the correlations and discretization impact the KS test, the effect is small. 

\section{Future use of insertion indexes}\label{sec:debiasing}

For the purposes of evidence estimation, a nested sampling run is fully encoded by recording the {\em birth contour\/} and {\em death contour\/} of each point~\citep{higson2018sampling}. For the purposes of estimating volume in a statistical way, we generally discard the likelihood information, focussing on the ordering of the contours. This makes sense, as barring the stopping criterion in \cref{eqn:stop}, the underlying nested sampling algorithm is athermal and insensitive to monotonic transformations of the likelihood.

Traditional nested sampling uses the fact that
\begin{equation}
    P(X_j|X_{j-1}, n_\mathrm{live}) = \frac{n_j}{X_{j-1}}\left( \frac{X_j}{X_{j-1}} \right)^{n_j-1} [0<X_j<X_{j-1}].
    \label{eqn:prob}
\end{equation}
In the above, one has essentially marginalised out dependency on everything other than $X_{j-1}$, and compressed the birth-death contour information into a vector encoding the number of live points at each iteration $n_i$. One can then use this recursively (alongside the fact that $X_0=1$) to perform inference on $P(X)$ and therefore the evidence via \cref{eq:t,eq:Z_sum}.

The critical question therefore is whether this ``Skilling compression'' from birth-death contours to numbers of live points is lossless or lossy for the purposes of volume estimation (note that it is generically lossy, as it's impossible to go in the reverse direction). The results presented in this paper are suggestive that it is losing some useful information, as insertion indexes do provide further information in the context of a cross check (and are in fact a lossless compression of the birth and death contours). One possibility is that the Skilling compression is lossless in the context of perfect nested sampling, but if a run is biased then you may be able to use insertion indexes to partially correct a biased run. This is the subject of ongoing work by the authors.

\section{Conclusions}\label{sec:conclusions}

We identified a previously unknown property of the NS algorithm: the insertion indexes of new live points into the existing live points should be uniformly distributed. This observation enabled us to invent a cross-check of single NS runs. The cross-check can detect when an NS run fails to sample new live points from the constrained prior, which is the most challenging aspect of an efficient implementation of NS, and functions with plateaus in the likelihood function recently identified in \citep{2020arXiv200508602S}, both of which can lead to unreliable estimates of the evidence and posterior, 

We applied our cross-check to NS runs on several toy functions with known analytic results in $2$ -- $50$ dimensions with \MN and \PC, which sample from the constrained prior using ellipsoidal rejection sampling and slice sampling, respectively. Our numerical results are some of the most detailed checks of \MN and \PC. We found that our cross-check could detect problematic runs for both codes. Since the idea is relatively simple, we suggest that a cross-check of this kind should become a mandatory test of any NS run. The exact form of the cross-check, however, could be refined. We chose a KS test using all the iterations or the most significant $\nlive$ iterations; both choices could be improved. As an example of a realistic application, we furthermore applied our cross-check to results from $22$ NS runs performed in the context of cosmological model selection.

Lastly, we speculated that the information contained in the insertion indexes could be used to debias single NS runs or lead to an improved formula for the evidence summation. We outlined a few difficulties and hope our observations lead to further developments.

Future work will involve extending the method to work in the context of a variable number of live points, as well as exploring the larger possibilities of using order statistics to improve NS accuracy and potentially debias runs.

\section*{Acknowledgments}
The authors would like to thank Gregory Martinez for valuable discussions. We thank the organisers of the GAMBIT XI workshop where some of this work was planned and completed. AF was supported by an NSFC Research Fund for International Young Scientists grant 11950410509.
WH was supported by a George Southgate visiting fellowship grant from the University of Adelaide, and STFC IPS grant number G102229.

\section*{Data availability}
The raw data from our \nrepeatSetting runs for \cref{tab:MN_summary,tab:PC_summary} are publicly available at \citep{fowlie_andrew_2020_3958749}. All other data will be shared on reasonable request to the corresponding author.

\bibliographystyle{mnras}
\bibliography{references}


\appendix
\section{Proof of \cref{EQ:UNIFORM}}\label{app:proof}  % fix bug in mnras style - label inside ref gets turned into capitals! so just make it capitals

In NS we have $n = \nlive - 1$ remaining samples after the worst live point was removed. Their associated volumes were drawn from a (continuous) uniform distribution, $X_i \sim \uniform(0, 1)$. If we draw another sample, the distribution of its insertion index, $i$, relative to the other samples depends on the probability contained in the uniform distribution between the ordered samples. In fact, the probability for each insertion index $i = 0, 1, \ldots, n$ is
\begin{align}
P_i ={}& \int (X_{i+1} - X_i) \, p(X_{i+1}, X_i) \,\intd X_{i+1} \,\intd X_i\\
    ={}& \int X_{i+1} \,  p(X_{i+1}) \,\intd X_{i+1} - \int X_{i} \, p(X_i) \,\intd X_{i}\\
    ={}& \langle X_{i+1} \rangle - \langle X_{i} \rangle,
\end{align}
where we completed two trivial integrals and wrote the terms as expectations. To compute the expectations, note that
\begin{equation}
p(X_i) = \frac{n!}{(i - 1)! (n - i)!} (1 - X_i)^{n - i} X_i^{i - 1},
\end{equation}
since we need $n - i$  samples above $X_i$, $i - 1$ samples below $X_i$ and one sample at $X_i$. The first factor is combinatoric; the second accounts for the $n - i$ samples that must lie above $X_i$; and the third accounts for the $i - 1$ samples that must lie below $X_i$. The factor for a final sample at $X_i$ is just one. By integration, we quickly find $\langle X_i \rangle = i / (n + 1)$, and thus $P_i = 1 / (n + 1)$. That is, the insertion indexes follow a discrete uniform distribution.

Note that this didn't depend especially on the fact that the distribution of the samples was uniform. If the samples had followed a different distribution, we can transform $X_i \to Y_i = F(X_i)$ where $F$ is the cumulative distribution function, such that $Y_i \sim \uniform(0, 1)$, the proof goes through just the same.


\section{Kolmogorov-Smirnov test}\label{app:ks}

We use a one-sample Kolmogorov-Smirnov (KS) test~\citep{kolmogorov1933sulla,smirnov1948} to compare our set of $\niter$ insertion indexes with a (discrete) uniform distribution. First, we compute the KS test-statistic by comparing the empirical cumulative distribution function, $F_\text{data}$, to that from a discrete uniform distribution, $F_U$,
\begin{equation}\label{eq:Dn}
D_n =  \sup_x \left|F_\text{data}(x) - F_U(x)\right|.
\end{equation}
This provides a notion of distance between the observed indexes and a uniform distribution. 
In the continuous case, the null-distribution of this test-statistic does not depend on the reference distribution. We convert the test-statistic into a \pvalue using an asymptotic approximation of the Kolmogorov distribution~\citep{JSSv008i18} implemented in \code{scipy}~\citep{2020SciPy-NMeth},
\begin{equation}
\text{\pvalue} = P\left(D_n^{\phantom{\star}} \ge D_n^\star \,\mid\, H_0\right),
\end{equation}
where $D_n^\star$ is the observed statistic. This assumes that we are testing samples from a continuous distribution. In our discrete case, the \pvalues from the Kolmogorov distribution are known to be conservative~\citep{RJ-2011-016}.

\section{Toy functions}\label{app:toy_problems}
\input{toy_problems}


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}
$0.10$ & $2$ & $0$ & $-0.00\pm0.10$ & $0.10$ & $0.01$ & \textcolor{black}{$-0.04$} & \textcolor{black}{$-0.47$} & \textcolor{black}{$0.50$} & \textcolor{black}{$0.49$}\\
% -0.004972839292884379 0.10478375632315214 0.1047517072836443 0.01047517072836443 -0.042816797948544066 -0.47472632397475273 0.5015474359825 0.490260895299
$0.10$ & $10$ & $0$ & $0.01\pm0.23$ & $0.21$ & $0.02$ & \textcolor{black}{$0.04$} & \textcolor{black}{$0.48$} & \textcolor{black}{$0.59$} & \textcolor{black}{$0.60$}\\
% 0.009879845086566958 0.234250249078537 0.207206545309366 0.0207206545309366 0.0438295630796032 0.47681143816263305 0.59321067658 0.597082487439
$0.10$ & $30$ & $0$ & $0.38\pm0.41$ & $0.36$ & $0.04$ & \textcolor{black}{$0.93$} & \textcolor{red}{$10.56$} & \textcolor{black}{$0.52$} & \textcolor{red}{$2.7\cdot10^{-4}$}\\
% 0.3757709181735423 0.40535681147366887 0.3559242901439913 0.03559242901439913 0.9279702626061137 10.557608136874332 0.5235379014674999 0.000272799401826
$0.10$ & $50$ & $0$ & $2.08\pm0.52$ & $0.50$ & $0.05$ & \textcolor{red}{$3.98$} & \textcolor{red}{$41.25$} & \textcolor{black}{$0.38$} & \textcolor{red}{$4.5\cdot10^{-24}$}\\
% 2.0759657784682743 0.5219336587518693 0.5032428031024886 0.05032428031024886 3.978302745858881 41.251772815626154 0.381430914875 4.5352891209e-24
$1$ & $2$ & $0$ & $-0.00\pm0.10$ & $0.10$ & $0.01$ & \textcolor{black}{$-0.04$} & \textcolor{black}{$-0.46$} & \textcolor{black}{$0.52$} & \textcolor{black}{$0.49$}\\
% -0.004578658792247952 0.10478623689297724 0.09876504163963765 0.009876504163963764 -0.0396437271803236 -0.46359103547528774 0.5232056464975 0.490260895298
$1$ & $10$ & $0$ & $0.57\pm0.23$ & $0.22$ & $0.02$ & \textcolor{black}{$2.43$} & \textcolor{red}{$26.07$} & \textcolor{black}{$0.21$} & \textcolor{red}{$1.2\cdot10^{-4}$}\\
% 0.5655685052978581 0.23308728486925442 0.2169481565017051 0.02169481565017051 2.4281488596120586 26.069292978454648 0.2061561577675 0.00011936310097825
$1$ & $30$ & $0$ & $2.35\pm0.40$ & $0.37$ & $0.04$ & \textcolor{red}{$5.83$} & \textcolor{red}{$63.82$} & \textcolor{black}{$0.23$} & \textcolor{red}{$2.2\cdot10^{-23}$}\\
% 2.3491602024645695 0.40293455490722885 0.3680722755723319 0.03680722755723319 5.831104237016882 63.82334009840204 0.229804411986 2.173506947085e-23
$1$ & $50$ & $0$ & $4.06\pm0.52$ & $0.44$ & $0.04$ & \textcolor{red}{$7.81$} & \textcolor{red}{$92.99$} & \textcolor{black}{$0.30$} & \textcolor{red}{$1.3\cdot10^{-34}$}\\
% 4.063295624085238 0.5200058519900508 0.4369511483427967 0.043695114834279666 7.814630119836961 92.99198868102077 0.29727298216200004 1.3432719101735e-34
$10$ & $2$ & $0$ & $-64.75\pm0.11$ & $93.15$ & $9.31$ & \textcolor{red}{$-532.44$} & \textcolor{red}{$-6.95$} & \textcolor{red}{$7.7\cdot10^{-3}$} & \textcolor{black}{$0.06$}\\
% -64.74862731892573 0.11367905524055769 93.14962814126169 9.314962814126169 -532.4425355265671 -6.951034438992525 0.0076833168486 0.06260297902445
$10$ & $10$ & $0$ & $2.81\pm0.23$ & $0.19$ & $0.02$ & \textcolor{red}{$12.30$} & \textcolor{red}{$150.55$} & \textcolor{red}{$2.1\cdot10^{-6}$} & \textcolor{red}{$1.7\cdot10^{-19}$}\\
% 2.805883518837079 0.22823590768175694 0.1863776744921161 0.01863776744921161 12.295280457388257 150.54826316956596 2.1335282238100002e-06 1.73524225311e-19
$10$ & $30$ & $0$ & $4.30\pm0.40$ & $0.25$ & $0.02$ & \textcolor{red}{$10.75$} & \textcolor{red}{$174.47$} & \textcolor{black}{$0.02$} & \textcolor{red}{$3.1\cdot10^{-68}$}\\
% 4.303397705661554 0.40046745561805336 0.2466621409912816 0.02466621409912816 10.746406188354472 174.4652701207868 0.015101345184300001 3.14030305626e-68
$10$ & $50$ & $0$ & $6.04\pm0.52$ & $0.31$ & $0.03$ & \textcolor{red}{$11.66$} & \textcolor{red}{$197.79$} & \textcolor{black}{$0.08$} & \textcolor{red}{$1.1\cdot10^{-93}$}\\
% 6.043589757065653 0.5181142461867976 0.30556350061867016 0.030556350061867014 11.664904271236294 197.7850674190236 0.0775311776522 1.07705413965e-93$0.10$ & $2$ & $-8.19$ & $-8.18\pm0.06$ & $0.06$ & $0.01$ & \textcolor{black}{$0.08$} & \textcolor{black}{$0.68$} & \textcolor{black}{$0.47$} & \textcolor{black}{$0.58$}\\
% -8.184503807260196 0.06163683141882082 0.06158118554375462 0.006158118554375462 0.07506367774157964 0.6796421905569187 0.46553600422200003 0.577764112647
$0.10$ & $10$ & $-40.94$ & $-40.56\pm0.16$ & $0.10$ & $0.01$ & \textcolor{black}{$2.46$} & \textcolor{red}{$38.50$} & \textcolor{black}{$0.18$} & \textcolor{black}{$0.46$}\\
% -40.560670847054645 0.15563728612103728 0.09941191812781944 0.009941191812781944 2.4603776951268213 38.503912043443925 0.1800757136975 0.458185410603
$0.10$ & $20$ & $-81.89$ & $-79.04\pm0.22$ & $0.14$ & $0.01$ & \textcolor{red}{$13.05$} & \textcolor{red}{$210.43$} & \textcolor{red}{$2.7\cdot10^{-6}$} & \textcolor{red}{$1.5\cdot10^{-3}$}\\
% -79.0374450673185 0.21841467799864564 0.13540810569347694 0.013540810569347694 13.04671058529078 210.43394429974498 2.7287114841499998e-06 0.0014564920387400001
$1$ & $2$ & $-8.19$ & $-8.16\pm0.06$ & $0.06$ & $0.01$ & \textcolor{black}{$0.55$} & \textcolor{red}{$5.65$} & \textcolor{black}{$0.34$} & \textcolor{black}{$0.58$}\\
% -8.155584339996986 0.06145182177135388 0.05854632434669543 0.005854632434669544 0.5450330946896175 5.65445991983664 0.33504883997 0.577764112647
$1$ & $10$ & $-40.94$ & $-38.66\pm0.15$ & $0.08$ & $0.01$ & \textcolor{red}{$15.30$} & \textcolor{red}{$304.65$} & \textcolor{red}{$1.0\cdot10^{-9}$} & \textcolor{red}{$1.7\cdot10^{-4}$}\\
% -38.6575596722193 0.14943628655063348 0.07503425529635264 0.007503425529635265 15.297280084808119 304.6456503064436 1.0077590725575e-09 0.000166638648343
$1$ & $20$ & $-81.89$ & $-76.83\pm0.21$ & $0.13$ & $0.01$ & \textcolor{red}{$23.69$} & \textcolor{red}{$388.70$} & \textcolor{red}{$1.4\cdot10^{-11}$} & \textcolor{red}{$5.6\cdot10^{-7}$}\\
% -76.83197240755746 0.21341465059347894 0.13004543678308844 0.013004543678308843 23.686551439809108 388.70405313152156 1.35098973109e-11 5.64278050841e-07
$10$ & $2$ & $-8.19$ & $-8.19\pm0.06$ & $0.19$ & $0.02$ & \textcolor{black}{$0.08$} & \textcolor{black}{$0.15$} & \textcolor{black}{$0.08$} & \textcolor{black}{$0.18$}\\
% -8.185873543194154 0.06140725797446731 0.19150553011992238 0.019150553011992238 0.08442071694379327 0.14702349578542592 0.08065125298535 0.180250224303
$10$ & $10$ & $-40.94$ & $-36.74\pm0.14$ & $0.07$ & $0.01$ & \textcolor{red}{$29.36$} & \textcolor{red}{$598.27$} & \textcolor{red}{$6.1\cdot10^{-24}$} & \textcolor{red}{$1.1\cdot10^{-8}$}\\
% -36.742281288619914 0.14309816902480615 0.07022222538386208 0.007022222538386208 29.35941744794412 598.2670458869522 6.0561508999999994e-24 1.14836066212e-08
$10$ & $20$ & $-81.89$ & $-74.70\pm0.21$ & $0.12$ & $0.01$ & \textcolor{red}{$34.51$} & \textcolor{red}{$602.35$} & \textcolor{red}{$4.9\cdot10^{-17}$} & \textcolor{red}{$7.1\cdot10^{-13}$}\\
% -74.69662648195703 0.20836596491929738 0.11937109744486149 0.01193710974448615 34.50836670828923 602.3455355938426 4.897281038615e-17 7.06101843662e-13$0.10$ & $2$ & $-5.80$ & $-5.79\pm0.07$ & $0.07$ & $0.01$ & \textcolor{black}{$0.22$} & \textcolor{black}{$2.32$} & \textcolor{black}{$0.50$} & \textcolor{black}{$0.54$}\\
% -5.788961991662762 0.06979313253603918 0.06534878202920567 0.0065348782029205675 0.2233906743805073 2.321446805990809 0.497457069938 0.544686294241
$1$ & $2$ & $-5.80$ & $-5.72\pm0.07$ & $0.05$ & $0.01$ & \textcolor{black}{$1.18$} & \textcolor{red}{$15.31$} & \textcolor{black}{$0.08$} & \textcolor{black}{$0.32$}\\
% -5.722159546995096 0.06949931903033084 0.05353774139675792 0.005353774139675792 1.1836552019304403 15.311220582110549 0.0764993937029 0.3183218499135
$10$ & $2$ & $-5.80$ & $-5.77\pm0.07$ & $0.34$ & $0.03$ & \textcolor{black}{$0.65$} & \textcolor{black}{$1.09$} & \textcolor{red}{$9.6\cdot10^{-3}$} & \textcolor{black}{$0.07$}\\
% -5.767102147308895 0.06961788775152512 0.34024624894482075 0.034024624894482075 0.6516455928575897 1.0883357744696596 0.00963483820007 0.0736116926081$0.10$ & $2$ & $-1.75$ & $-1.75\pm0.05$ & $0.05$ & $0.01$ & \textcolor{black}{$-0.06$} & \textcolor{black}{$-0.64$} & \textcolor{black}{$0.55$} & \textcolor{black}{$0.55$}\\
% -1.7489269882642082 0.05129389559527299 0.051361111598951646 0.0051361111598951644 -0.05517183099356922 -0.6396115884516006 0.548004732478 0.551883893715
$0.10$ & $10$ & $-14.59$ & $-14.59\pm0.12$ & $0.13$ & $0.01$ & \textcolor{black}{$0.02$} & \textcolor{black}{$0.16$} & \textcolor{black}{$0.57$} & \textcolor{black}{$0.56$}\\
% -14.588366251351827 0.12403082869936481 0.1330423087785661 0.01330423087785661 0.021649540058867985 0.15971032469675314 0.566705771061 0.5578727822495
$0.10$ & $30$ & $-60.13$ & $-59.61\pm0.24$ & $0.21$ & $0.02$ & \textcolor{black}{$2.11$} & \textcolor{red}{$24.29$} & \textcolor{black}{$0.37$} & \textcolor{red}{$7.3\cdot10^{-6}$}\\
% -59.61223030524093 0.24414701469670924 0.21225903346726244 0.021225903346726242 2.113093615576275 24.2881081529782 0.373763174461 7.27903662634e-06
$0.10$ & $50$ & $-112.42$ & $-110.15\pm0.33$ & $0.20$ & $0.02$ & \textcolor{red}{$6.87$} & \textcolor{red}{$115.58$} & \textcolor{black}{$0.07$} & \textcolor{red}{$3.7\cdot10^{-23}$}\\
% -110.15091452516565 0.32965123468462365 0.19590817863811413 0.019590817863811415 6.869055178357311 115.57501330721894 0.0679655532251 3.70516507546e-23
$1$ & $2$ & $-1.75$ & $-1.71\pm0.05$ & $0.05$ & $0.00$ & \textcolor{black}{$0.79$} & \textcolor{red}{$8.52$} & \textcolor{red}{$4.7\cdot10^{-3}$} & \textcolor{black}{$0.10$}\\
% -1.705431197217556 0.05110324194457447 0.047211099080028414 0.0047211099080028416 0.7947153736739692 8.517207947445968 0.0046665479341500005 0.102857090329
$1$ & $10$ & $-14.59$ & $-13.92\pm0.12$ & $0.10$ & $0.01$ & \textcolor{red}{$5.57$} & \textcolor{red}{$65.88$} & \textcolor{black}{$0.02$} & \textcolor{red}{$1.1\cdot10^{-5}$}\\
% -13.915660819830132 0.12130171316962607 0.1024257962561466 0.01024257962561466 5.5661883454312 65.88479457532571 0.01707661823365 1.0658790482e-05
$1$ & $30$ & $-60.13$ & $-57.57\pm0.24$ & $0.17$ & $0.02$ & \textcolor{red}{$10.67$} & \textcolor{red}{$151.79$} & \textcolor{red}{$7.7\cdot10^{-3}$} & \textcolor{red}{$1.4\cdot10^{-20}$}\\
% -57.56867610052626 0.2399197716678592 0.16859302579387042 0.01685930257938704 10.667493722826535 151.79105000306942 0.00768432544685 1.42937360571e-20
$1$ & $50$ & $-112.42$ & $-107.97\pm0.33$ & $0.18$ & $0.02$ & \textcolor{red}{$13.63$} & \textcolor{red}{$218.07$} & \textcolor{red}{$3.5\cdot10^{-3}$} & \textcolor{red}{$3.6\cdot10^{-37}$}\\
% -107.96720790831924 0.3263485012593391 0.18243497856977461 0.020396850677786668 13.629827661280785 218.06874612264275 0.003503158882815 3.57114707794e-37
$10$ & $2$ & $-1.75$ & $-1.73\pm0.05$ & $0.12$ & $0.01$ & \textcolor{black}{$0.39$} & \textcolor{black}{$1.45$} & \textcolor{black}{$0.07$} & \textcolor{black}{$0.18$}\\
% -1.7281611977553146 0.051157706377997346 0.12065413728035229 0.01206541372803523 0.38696896440316486 1.4488251033473403 0.0683498562831 0.177410013196
$10$ & $10$ & $-14.59$ & $-11.73\pm0.11$ & $0.09$ & $0.01$ & \textcolor{red}{$25.56$} & \textcolor{red}{$321.53$} & \textcolor{red}{$6.8\cdot10^{-18}$} & \textcolor{red}{$1.7\cdot10^{-19}$}\\
% -11.730277252118151 0.11193685120667957 0.08895683547688561 0.008895683547688561 25.555118342772563 321.52827907308057 6.830593602535e-18 1.67302891528e-19
$10$ & $30$ & $-60.13$ & $-55.41\pm0.24$ & $0.13$ & $0.01$ & \textcolor{red}{$20.03$} & \textcolor{red}{$367.16$} & \textcolor{red}{$3.0\cdot10^{-6}$} & \textcolor{red}{$9.3\cdot10^{-66}$}\\
% -55.413282921646854 0.23540061670865012 0.128404354752777 0.0128404354752777 20.02813875125781 367.15923138284865 3.042142989965e-06 9.308517813505e-66
$10$ & $50$ & $-112.42$ & $-105.82\pm0.32$ & $0.13$ & $0.01$ & \textcolor{red}{$20.42$} & \textcolor{red}{$480.50$} & \textcolor{red}{$9.3\cdot10^{-6}$} & \textcolor{red}{$2.2\cdot10^{-92}$}\\
% -105.8169555117735 0.3230575636891933 0.13171221385754517 0.013731947548333078 20.424404847051743 480.49761517639973 9.28275289479e-06 2.202406352085e-92$0.50$ & $2$ & $0$ & $0.01\pm0.11$ & $0.11$ & $0.01$ & \textcolor{black}{$0.11$} & \textcolor{black}{$1.03$} & \textcolor{black}{$0.54$} & \textcolor{black}{$0.60$}\\
% 0.011433106317409337 0.10526578880366617 0.11104396634860941 0.01110439663486094 0.11371645639190615 1.029601759857573 0.5362370355015961 0.5993495206339235
$0.50$ & $10$ & $0$ & $-0.00\pm0.23$ & $0.23$ & $0.02$ & \textcolor{black}{$-0.01$} & \textcolor{black}{$-0.10$} & \textcolor{black}{$0.48$} & \textcolor{black}{$0.52$}\\
% -0.0023476306907257195 0.23418556256911807 0.23411934802008286 0.023411934802008285 -0.008047102377299161 -0.10027495423079424 0.48169731938803373 0.5184581473495482
$0.50$ & $30$ & $0$ & $-0.06\pm0.41$ & $0.37$ & $0.04$ & \textcolor{black}{$-0.15$} & \textcolor{black}{$-1.61$} & \textcolor{black}{$0.54$} & \textcolor{black}{$0.57$}\\
% -0.05947860820990623 0.4050226031054816 0.36971966815721224 0.03697196681572122 -0.1458850220082644 -1.608748826005511 0.5449617476980702 0.5729431132436015
$0.50$ & $50$ & $0$ & $-0.05\pm0.52$ & $0.59$ & $0.06$ & \textcolor{black}{$-0.10$} & \textcolor{black}{$-0.85$} & \textcolor{black}{$0.58$} & \textcolor{black}{$0.51$}\\
% -0.05001684147811865 0.5246011633017628 0.5857558563388171 0.05857558563388171 -0.10121742855572613 -0.8538854701469267 0.5832134225926935 0.5051181389068602
$1$ & $2$ & $0$ & $-0.02\pm0.11$ & $0.10$ & $0.01$ & \textcolor{black}{$-0.19$} & \textcolor{black}{$-1.96$} & \textcolor{black}{$0.42$} & \textcolor{black}{$0.48$}\\
% -0.020495639311322575 0.10540547508681597 0.10445750265354477 0.010445750265354476 -0.19003944598458536 -1.9621031319598616 0.41951953646816376 0.48105945648025566
$1$ & $10$ & $0$ & $-0.04\pm0.23$ & $0.18$ & $0.02$ & \textcolor{black}{$-0.17$} & \textcolor{black}{$-2.20$} & \textcolor{black}{$0.55$} & \textcolor{black}{$0.59$}\\
% -0.03950968103397693 0.23427145838267704 0.1795621880865596 0.01795621880865596 -0.16746561064967083 -2.2003341268558683 0.5480968874548486 0.5915786462532693
$1$ & $30$ & $0$ & $-0.83\pm0.41$ & $0.40$ & $0.04$ & \textcolor{black}{$-2.06$} & \textcolor{red}{$-20.73$} & \textcolor{black}{$0.61$} & \textcolor{black}{$0.46$}\\
% -0.834979537307505 0.4058767295439603 0.40283746787561703 0.040283746787561706 -2.056064159062397 -20.72745471544169 0.6097552927534602 0.4592198567199635
$1$ & $50$ & $0$ & $-2.48\pm0.52$ & $0.46$ & $0.05$ & \textcolor{red}{$-4.73$} & \textcolor{red}{$-54.22$} & \textcolor{black}{$0.49$} & \textcolor{black}{$0.59$}\\
% -2.4816786315381396 0.524786345956072 0.4577475051798005 0.04577475051798005 -4.728220225603408 -54.215011626624836 0.4948328565844211 0.590641523132594
$2$ & $2$ & $0$ & $-0.01\pm0.11$ & $0.15$ & $0.02$ & \textcolor{black}{$-0.12$} & \textcolor{black}{$-0.89$} & \textcolor{black}{$0.47$} & \textcolor{black}{$0.53$}\\
% -0.013777958988380629 0.1053756765609287 0.15467135660642753 0.015467135660642752 -0.12092017398295486 -0.8907893026011043 0.4736668445070463 0.5348712223725246
$10$ & $10$ & $0$ & $2.20\pm0.23$ & $0.73$ & $0.07$ & \textcolor{red}{$9.50$} & \textcolor{red}{$30.29$} & \textcolor{black}{$0.13$} & \textcolor{black}{$0.22$}\\
% 2.1957352398914525 0.23195576800958087 0.7250201567064172 0.07250201567064171 9.503956824111588 30.28516131008166 0.13205502056042034 0.21656657700509174
$30$ & $30$ & $0$ & $48.37\pm0.64$ & $6.85$ & $0.69$ & \textcolor{red}{$112.25$} & \textcolor{red}{$70.58$} & \textcolor{red}{$8.2\cdot10^{-10}$} & \textcolor{black}{$0.02$}\\
% 48.369963270616715 0.6419231608808755 6.852882256474416 0.6852882256474416 112.25418085352104 70.58338588105478 8.245977026119102e-10 0.02182770096974873
$50$ & $50$ & $0$ & $69.74\pm3.05$ & $6.55$ & $0.65$ & \textcolor{red}{$23.31$} & \textcolor{red}{$106.51$} & \textcolor{red}{$8.0\cdot10^{-86}$} & \textcolor{red}{$1.4\cdot10^{-6}$}\\
% 69.74151669264275 3.045053308792238 6.547617276314974 0.6547617276314974 23.306370612865745 106.51434521825557 7.991781203579044e-86 1.439236276823408e-06$0.50$ & $2$ & $-8.19$ & $-8.17\pm0.06$ & $0.06$ & $0.01$ & \textcolor{black}{$0.35$} & \textcolor{red}{$3.33$} & \textcolor{black}{$0.45$} & \textcolor{black}{$0.56$}\\
% -8.167169589495659 0.06270189783077831 0.06455508250805879 0.006455508250805879 0.35023330801181013 3.333515210960433 0.44663588191575154 0.5605742076352135
$0.50$ & $10$ & $-40.94$ & $-40.87\pm0.16$ & $0.15$ & $0.02$ & \textcolor{black}{$0.49$} & \textcolor{red}{$5.12$} & \textcolor{black}{$0.43$} & \textcolor{black}{$0.52$}\\
% -40.86641118778085 0.15704281775301485 0.15049349812227464 0.015049349812227464 0.4930052330568904 5.118788213532527 0.43024296667059914 0.5157823042428763
$0.50$ & $20$ & $-81.89$ & $-81.75\pm0.23$ & $0.24$ & $0.02$ & \textcolor{black}{$0.61$} & \textcolor{red}{$5.70$} & \textcolor{black}{$0.51$} & \textcolor{black}{$0.51$}\\
% -81.74931099651793 0.22547941122534096 0.2415407568911497 0.02415407568911497 0.6110247027186138 5.695943396669823 0.5071237501821867 0.513096816076821
$1$ & $2$ & $-8.19$ & $-8.16\pm0.06$ & $0.06$ & $0.01$ & \textcolor{black}{$0.39$} & \textcolor{red}{$3.78$} & \textcolor{black}{$0.44$} & \textcolor{black}{$0.49$}\\
% -8.164970475185834 0.06269987362323082 0.0626988858456619 0.00626988858456619 0.3852529598342687 3.782945891056505 0.44045953393638504 0.4861003705435981
$1$ & $10$ & $-40.94$ & $-40.85\pm0.16$ & $0.16$ & $0.02$ & \textcolor{black}{$0.59$} & \textcolor{red}{$5.71$} & \textcolor{black}{$0.48$} & \textcolor{black}{$0.45$}\\
% -40.85155238283803 0.15696939875516724 0.16100327133265743 0.016100327133265743 0.5885063112367759 5.707538649516681 0.4813052011895679 0.44913659845384246
$1$ & $20$ & $-81.89$ & $-81.72\pm0.22$ & $0.27$ & $0.03$ & \textcolor{black}{$0.73$} & \textcolor{red}{$6.18$} & \textcolor{black}{$0.52$} & \textcolor{black}{$0.59$}\\
% -81.72289220854121 0.22490467827970867 0.26535749890405474 0.026535749890405476 0.7311880051678459 6.180305308051539 0.5238163370268282 0.5859996227609684
$2$ & $2$ & $-8.19$ & $-8.18\pm0.06$ & $0.09$ & $0.01$ & \textcolor{black}{$0.09$} & \textcolor{black}{$0.58$} & \textcolor{black}{$0.52$} & \textcolor{black}{$0.49$}\\
% -8.183582252037082 0.06290659907902058 0.08773138456025652 0.008773138456025651 0.09419891091247543 0.5821032499049471 0.5209575698576654 0.48610037054365385
$10$ & $10$ & $-40.94$ & $-40.90\pm0.18$ & $1.31$ & $0.13$ & \textcolor{black}{$0.77$} & \textcolor{black}{$0.31$} & \textcolor{red}{$7.2\cdot10^{-3}$} & \textcolor{black}{$0.27$}\\
% -40.902638887554374 0.18108793622952857 1.3118428728386131 0.1311842872838613 0.7652515996797489 0.31106419458858164 0.007194508607812705 0.2702514047854353
$20$ & $20$ & $-81.89$ & $-88.28\pm0.64$ & $5.80$ & $0.58$ & \textcolor{red}{$-11.25$} & \textcolor{red}{$-11.03$} & \textcolor{red}{$4.5\cdot10^{-15}$} & \textcolor{black}{$0.01$}\\
% -88.2810495102183 0.6418057670936588 5.7975947463718125 0.5797594746371812 -11.253808392352262 -11.028984510822882 4.473445155033488e-15 0.01063439136399974$0.50$ & $2$ & $-5.80$ & $-5.79\pm0.07$ & $0.08$ & $0.01$ & \textcolor{black}{$0.20$} & \textcolor{black}{$1.71$} & \textcolor{black}{$0.42$} & \textcolor{black}{$0.46$}\\
% -5.79027381830839 0.07054841504209244 0.08104271569229884 0.008104271569229885 0.20498764992633664 1.7100297500763586 0.41843431619330573 0.4565280785342637
$1$ & $2$ & $-5.80$ & $-5.81\pm0.07$ & $0.08$ & $0.01$ & \textcolor{black}{$-0.05$} & \textcolor{black}{$-0.55$} & \textcolor{black}{$0.44$} & \textcolor{black}{$0.52$}\\
% -5.808498421829232 0.07066027963572746 0.07960298916551509 0.00796029891655151 -0.05388619515968461 -0.5484791564400843 0.435560669795841 0.5162794766851382
$2$ & $2$ & $-5.80$ & $-5.83\pm0.07$ & $0.09$ & $0.01$ & \textcolor{black}{$-0.36$} & \textcolor{black}{$-2.82$} & \textcolor{black}{$0.56$} & \textcolor{black}{$0.49$}\\
% -5.830208882516457 0.07097320966227418 0.09251107844005506 0.009251107844005506 -0.3572425029703631 -2.81874551261263 0.5620901247825276 0.48640377760968867$0.50$ & $2$ & $-1.75$ & $-1.74\pm0.05$ & $0.05$ & $0.00$ & \textcolor{black}{$0.16$} & \textcolor{black}{$1.54$} & \textcolor{black}{$0.13$} & \textcolor{black}{$0.13$}\\
% -1.7379490260876294 0.05206776916774733 0.04985458468233216 0.004985458468233216 0.15596196958891598 1.543056873936928 0.13440369725485735 0.1260684230103422
$0.50$ & $10$ & $-14.59$ & $-14.59\pm0.12$ & $0.12$ & $0.01$ & \textcolor{black}{$0.02$} & \textcolor{black}{$0.12$} & \textcolor{black}{$0.50$} & \textcolor{black}{$0.48$}\\
% -14.58903835691989 0.12443674816792175 0.12283894342561145 0.012283894342561146 0.015377090340428246 0.11826196357348694 0.4959936015637657 0.48475775055214093
$0.50$ & $30$ & $-60.13$ & $-60.12\pm0.25$ & $0.24$ & $0.02$ & \textcolor{black}{$0.03$} & \textcolor{black}{$0.29$} & \textcolor{black}{$0.56$} & \textcolor{black}{$0.55$}\\
% -60.1208234592772 0.2462908076057811 0.2370191227798059 0.023701912277980592 0.029804447645677738 0.2929671670471306 0.5646862041171168 0.5453415778208199
$0.50$ & $50$ & $-112.42$ & $-112.33\pm0.34$ & $0.33$ & $0.03$ & \textcolor{black}{$0.27$} & \textcolor{black}{$2.65$} & \textcolor{black}{$0.40$} & \textcolor{black}{$0.58$}\\
% -112.32631989548199 0.3359981668484766 0.33481739357258866 0.033481739357258865 0.266055043807418 2.6523014245289884 0.40082722633065926 0.5830698471774588
$1$ & $2$ & $-1.75$ & $-1.75\pm0.05$ & $0.04$ & $0.00$ & \textcolor{black}{$-0.02$} & \textcolor{black}{$-0.30$} & \textcolor{black}{$0.01$} & \textcolor{black}{$0.01$}\\
% -1.7469980608777014 0.05218880594566965 0.04489639441168069 0.004489639441168069 -0.019551993134602017 -0.30207076730973487 0.012681707043520127 0.0116605940441461
$1$ & $10$ & $-14.59$ & $-14.59\pm0.12$ & $0.12$ & $0.01$ & \textcolor{black}{$0.02$} & \textcolor{black}{$0.19$} & \textcolor{black}{$0.49$} & \textcolor{black}{$0.61$}\\
% -14.58814189894962 0.1244233857159315 0.12111389589765517 0.012111389589765518 0.02253816461695516 0.193964154000502 0.48896694284631853 0.6076256671344982
$1$ & $30$ & $-60.13$ & $-60.46\pm0.25$ & $0.23$ & $0.02$ & \textcolor{black}{$-1.36$} & \textcolor{red}{$-14.57$} & \textcolor{black}{$0.48$} & \textcolor{black}{$0.53$}\\
% -60.46387400562985 0.24706748126655712 0.23066483290816614 0.023066483290816613 -1.3585476645719146 -14.571214000045842 0.4811482852226189 0.5290043577363563
$1$ & $50$ & $-112.42$ & $-113.52\pm0.34$ & $0.32$ & $0.03$ & \textcolor{red}{$-3.26$} & \textcolor{red}{$-34.47$} & \textcolor{black}{$0.50$} & \textcolor{black}{$0.51$}\\
% -113.52200918697552 0.3392243929545684 0.3211567448375326 0.032115674483753257 -3.263344773779719 -34.46558866638545 0.5021731235018911 0.5120608270090732
$2$ & $2$ & $-1.75$ & $-1.74\pm0.05$ & $0.06$ & $0.01$ & \textcolor{black}{$0.06$} & \textcolor{black}{$0.43$} & \textcolor{red}{$6.1\cdot10^{-6}$} & \textcolor{red}{$2.1\cdot10^{-5}$}\\
% -1.7430201355688675 0.05216568937242862 0.06095037443728415 0.0060950374437284145 0.06259209085732871 0.4301428009427384 6.106350901202759e-06 2.0764265148853767e-05
$10$ & $10$ & $-14.59$ & $-14.05\pm0.12$ & $0.36$ & $0.04$ & \textcolor{red}{$4.42$} & \textcolor{red}{$15.01$} & \textcolor{black}{$0.09$} & \textcolor{black}{$0.09$}\\
% -14.052875653723165 0.12257629824662856 0.3581556577713455 0.03581556577713455 4.420346380623353 15.010663910975573 0.08648568668479054 0.09093157166989096
$30$ & $30$ & $-60.13$ & $-38.78\pm0.21$ & $1.34$ & $0.13$ & \textcolor{red}{$103.26$} & \textcolor{red}{$159.47$} & \textcolor{red}{$3.5\cdot10^{-5}$} & \textcolor{red}{$5.2\cdot10^{-3}$}\\
% -38.779484709172266 0.20722504047708984 1.3387369328607286 0.13387369328607285 103.26250706713762 159.4658525374344 3.5254178157209185e-05 0.005254339954005982
$50$ & $50$ & $-112.42$ & $-64.20\pm0.63$ & $5.17$ & $0.52$ & \textcolor{red}{$103.63$} & \textcolor{red}{$93.31$} & \textcolor{red}{$5.2\cdot10^{-12}$} & \textcolor{red}{$3.8\cdot10^{-7}$}\\
% -64.19679273741045 0.6347253122689763 5.167276765795631 0.5167276765795631 103.62970794592191 93.31478263800737 5.223958590476514e-12 3.8167293192614693e-07
\subsection{Gaussian}\label{sec:gaussian}
Our first example is a multi-dimensional Gaussian likelihood,
\begin{equation}
\like(\params) = \frac{1}{\sqrt{(2\pi)^n \det \Sigma}} e^{-\tfrac12 (\params - \bm{\mu})^T \Sigma^{-1} (\params - \bm{\mu})},
\end{equation}
with covariance matrix $\Sigma$ and mean $\bm{\mu}$. We pick a uniform prior from $0$ to $1$ for each dimension. The analytic evidence is always $\logZ = 0$ since the likelihood is a pdf in $\params$, modulo small errors as the infinite domain is truncated by the prior. We pick $\mu = 0.5$ and a diagonal covariance matrix with $\sigma = 0.001$ for each dimension.

\subsection{Rosenbrock}\label{sec:rosenbrock}
This is a two-dimensional function exhibiting a pronounced curved degeneracy~\cite{10.1093/comjnl/3.3.175}. The likelihood function is
\begin{equation}
-\ln\like(x, y) = (1 - x)^2  + 100 (y - x^2)^2.
\end{equation}
We consider uniform priors from $-5$ to $5$ for each parameter. The evidence can be found semi-analytically from a one-dimensional integral,
\begin{equation}
\Z = \frac{\sqrt{\pi} }{2000} \, \int_{-5}^5 \left[\text{erf}\left(10 (5 -x^2)\right) + \text{erf}\left(10 (5 +x^2)\right)\right] e^{-(1 - x)^2} \, \intd x
\end{equation}
to be $\logZ = -5.804$. The analytic approximation, which approximates the $y$ domain of integration by the whole real line, leads to
\begin{equation}
\Z \approx \frac{\pi}{20000} \left[\text{erf}(6) - \text{erf}(4)\right],
\end{equation}
and thus $\logZ = -5.763$.

\subsection{Gaussian shells}\label{sec:shells}

The multidimensional likelihood is
\begin{equation}
\like(\params) = \text{shell}(\params; \bm{c}, r, w) + \text{shell}(\params; -\bm{c}, r, w)
\end{equation}
where the shell function is a Gaussian favouring a radial distance $r$ from the point $\bm{c}$,
\begin{equation}
\text{shell}(\params; \bm{c}, r, w) = \frac{1}{\sqrt{2\pi} w} e^{-(|\params - \bm{c}| - r)^2 / (2 w^2)}.
\end{equation}
Thus, the highest likelihood region forms a shell of characteristic width $w$ at the surface of a $d$-sphere of radius $r$. Our likelihood contains two such shells, one at $\bm{c}$ and one at $\bm{-c}$. As usual, we take $w = 0.1$, $r = 2$ and $\bm{c} = (3.5, 0, \ldots, 0)$.

With uniform priors between $-6$ and $6$, the analytic evidence is approximately,
\begin{equation}
\Z = 2  \langle |x|^{d-1}\rangle S_d / 12^d
\end{equation}
where $S_d$ is the surface area of an $d$-sphere and $\langle |x|^{d-1}\rangle$ is the $(d-1)$-th non-central moment of a Gaussian, $\normal(r, w^2)$, and we ignore the truncation of the domain by the finite-sized hypercube.

\subsection{Gaussian-Log-Gamma mixture}\label{sec:gaussian-log-gamma}
This toy function was found in~\cite{2013arXiv1304.7808B,Feroz:2013hea,2014arXiv1407.5459B} to be problematic in \MN without importance sampling. It is defined in even numbers of dimensions.  The likelihood is a product of $d$ factors,
\begin{equation}
\like(\params) = \prod_{i=1}^d \like_i(\theta_i),
\end{equation}
where the factors are
\begin{align}
\like_i(\theta) =
\begin{cases} 
      \tfrac12 \loggamma(\theta \rvert 10, 1, 1) + \tfrac12 \loggamma(\theta \rvert {-10}, 1, 1) & i = 1\\
      \tfrac12 \normal(\theta \rvert 10, 1) + \tfrac12 \normal(\theta \rvert {-10}, 1) & i = 2\\
      \loggamma(\theta \rvert 10, 1, 1) & 3 \le i \le \frac{d+2} {2}\\
      \normal(\theta \rvert 10, 1) & \frac{d+2}{2}  < i \le d\\
   \end{cases}
\end{align}
where e.g., $\loggamma(\theta \rvert 10, 1, 1)$ denotes a one-dimensional log-Gamma density for $\theta$ with mean $10$ and shape parameters $1$ and $1$.
There are four identical modes at $\theta_1 = \pm 10$, $\theta_2 = \pm10$ and $\theta_{i>2} = 10$.

The prior is uniform in each parameter from $-30$ to $30$. Since the likelihood is a pdf in $\params$, the analytic $\logZ$ is governed by the prior normalization factor, $\logZ = \log(1/60^d) \approx -4.1 d$, modulo small truncation errors introduced by the prior.

```

4. **Bibliographic Information:**
```bbl
\begin{thebibliography}{}
\makeatletter
\relax
\def\mn@urlcharsother{\let\do\@makeother \do\$\do\&\do\#\do\^\do\_\do\%\do\~}
\def\mn@doi{\begingroup\mn@urlcharsother \@ifnextchar [ {\mn@doi@}
  {\mn@doi@[]}}
\def\mn@doi@[#1]#2{\def\@tempa{#1}\ifx\@tempa\@empty \href
  {http://dx.doi.org/#2} {doi:#2}\else \href {http://dx.doi.org/#2} {#1}\fi
  \endgroup}
\def\mn@eprint#1#2{\mn@eprint@#1:#2::\@nil}
\def\mn@eprint@arXiv#1{\href {http://arxiv.org/abs/#1} {{\tt arXiv:#1}}}
\def\mn@eprint@dblp#1{\href {http://dblp.uni-trier.de/rec/bibtex/#1.xml}
  {dblp:#1}}
\def\mn@eprint@#1:#2:#3:#4\@nil{\def\@tempa {#1}\def\@tempb {#2}\def\@tempc
  {#3}\ifx \@tempc \@empty \let \@tempc \@tempb \let \@tempb \@tempa \fi \ifx
  \@tempb \@empty \def\@tempb {arXiv}\fi \@ifundefined
  {mn@eprint@\@tempb}{\@tempb:\@tempc}{\expandafter \expandafter \csname
  mn@eprint@\@tempb\endcsname \expandafter{\@tempc}}}

\bibitem[\protect\citeauthoryear{Abbott et~al.}{Abbott
  et~al.}{2016a}]{TheLIGOScientific:2016pea}
Abbott B.~P.,  et~al., 2016a, \mn@doi [Phys. Rev.] {10.1103/PhysRevX.6.041015,
  10.1103/PhysRevX.8.039903}, X6, 041015

\bibitem[\protect\citeauthoryear{Abbott et~al.}{Abbott
  et~al.}{2016b}]{TheLIGOScientific:2016src}
Abbott B.~P.,  et~al., 2016b, \mn@doi [Phys. Rev. Lett.]
  {10.1103/PhysRevLett.116.221101, 10.1103/PhysRevLett.121.129902}, 116, 221101

\bibitem[\protect\citeauthoryear{Aitken \& Akman}{Aitken \&
  Akman}{2013}]{aitken}
Aitken S.,  Akman O.~E.,  2013, \mn@doi [BMC Systems Biology]
  {10.1186/1752-0509-7-72}, 7, 72

\bibitem[\protect\citeauthoryear{Arnold \& Emerson}{Arnold \&
  Emerson}{2011}]{RJ-2011-016}
Arnold T.~B.,  Emerson J.~W.,  2011, \mn@doi [{The R Journal}]
  {10.32614/RJ-2011-016}, 3, 34

\bibitem[\protect\citeauthoryear{Ashton et~al.}{Ashton
  et~al.}{2019}]{Ashton:2018jfp}
Ashton G.,  et~al., 2019, \mn@doi [Astrophys. J. Suppl.]
  {10.3847/1538-4365/ab06fc}, 241, 27

\bibitem[\protect\citeauthoryear{{Audren}, {Lesgourgues}, {Benabed}  \&
  {Prunet}}{{Audren} et~al.}{2013}]{2013JCAP...02..001A}
{Audren} B.,  {Lesgourgues} J.,  {Benabed} K.,   {Prunet} S.,  2013, \mn@doi
  [JCAP] {10.1088/1475-7516/2013/02/001}, \href
  {https://ui.adsabs.harvard.edu/abs/2013JCAP...02..001A} {2013, 001}

\bibitem[\protect\citeauthoryear{Baldock, P\'artay, Bart\'ok, Payne  \&
  Cs\'anyi}{Baldock et~al.}{2016}]{PhysRevB.93.174108}
Baldock R. J.~N.,  P\'artay L.~B.,  Bart\'ok A.~P.,  Payne M.~C.,   Cs\'anyi
  G.,  2016, \mn@doi [Phys. Rev. B] {10.1103/PhysRevB.93.174108}, 93, 174108

\bibitem[\protect\citeauthoryear{Baldock, Bernstein, Salerno, P\'artay  \&
  Cs\'anyi}{Baldock et~al.}{2017}]{PhysRevE.96.043311}
Baldock R. J.~N.,  Bernstein N.,  Salerno K.~M.,  P\'artay L.~B.,   Cs\'anyi
  G.,  2017, \mn@doi [Phys. Rev. E] {10.1103/PhysRevE.96.043311}, 96, 043311

\bibitem[\protect\citeauthoryear{{Beaujean} \& {Caldwell}}{{Beaujean} \&
  {Caldwell}}{2013}]{2013arXiv1304.7808B}
{Beaujean} F.,  {Caldwell} A.,  2013, arXiv e-prints, \href
  {https://ui.adsabs.harvard.edu/abs/2013arXiv1304.7808B} {p. arXiv:1304.7808}

\bibitem[\protect\citeauthoryear{Bolhuis \& Cs\'anyi}{Bolhuis \&
  Cs\'anyi}{2018}]{PhysRevLett.120.250601}
Bolhuis P.~G.,  Cs\'anyi G.,  2018, \mn@doi [Phys. Rev. Lett.]
  {10.1103/PhysRevLett.120.250601}, 120, 250601

\bibitem[\protect\citeauthoryear{Buchmueller et~al.}{Buchmueller
  et~al.}{2014}]{Buchmueller:2013rsa}
Buchmueller O.,  et~al., 2014, \mn@doi [Eur. Phys. J. C]
  {10.1140/epjc/s10052-014-2922-3}, 74, 2922

\bibitem[\protect\citeauthoryear{{Buchner}}{{Buchner}}{2016}]{2014arXiv1407.5459B}
{Buchner} J.,  2016, \mn@doi [Statistics and Computing]
  {10.1007/s11222-014-9512-y}, 26, 383

\bibitem[\protect\citeauthoryear{Buchner et~al.,}{Buchner
  et~al.}{2014}]{Buchner:2014nha}
Buchner J.,  et~al., 2014, \mn@doi [Astron. Astrophys.]
  {10.1051/0004-6361/201322971}, 564, A125

\bibitem[\protect\citeauthoryear{Easther \& Peiris}{Easther \&
  Peiris}{2012}]{Easther:2011yq}
Easther R.,  Peiris H.~V.,  2012, \mn@doi [Phys. Rev. D]
  {10.1103/PhysRevD.85.103533}, 85, 103533

\bibitem[\protect\citeauthoryear{Feroz \& Hobson}{Feroz \&
  Hobson}{2008}]{Feroz:2007kg}
Feroz F.,  Hobson M.~P.,  2008, \mn@doi [Mon. Not. Roy. Astron. Soc.]
  {10.1111/j.1365-2966.2007.12353.x}, 384, 449

\bibitem[\protect\citeauthoryear{Feroz, Allanach, Hobson, AbdusSalam, Trotta
  \& Weber}{Feroz et~al.}{2008}]{Feroz:2008wr}
Feroz F.,  Allanach B.~C.,  Hobson M.,  AbdusSalam S.~S.,  Trotta R.,   Weber
  A.~M.,  2008, \mn@doi [JHEP] {10.1088/1126-6708/2008/10/064}, 10, 064

\bibitem[\protect\citeauthoryear{Feroz, Hobson  \& Bridges}{Feroz
  et~al.}{2009}]{Feroz:2008xx}
Feroz F.,  Hobson M.~P.,   Bridges M.,  2009, \mn@doi [Mon. Not. Roy. Astron.
  Soc.] {10.1111/j.1365-2966.2009.14548.x}, 398, 1601

\bibitem[\protect\citeauthoryear{Feroz, Hobson, Cameron  \& Pettitt}{Feroz
  et~al.}{2013}]{Feroz:2013hea}
Feroz F.,  Hobson M.~P.,  Cameron E.,   Pettitt A.~N.,  2013, \mn@doi [The Open
  Journal of Astrophysics] {10.21105/astro.1306.2144}

\bibitem[\protect\citeauthoryear{Fowlie, Su  \& Handley}{Fowlie
  et~al.}{2020}]{fowlie_andrew_2020_3958749}
Fowlie A.,  Su L.,   Handley W.,  2020, {Supplementary data for Nested sampling
  cross- checks using order statistics}, \mn@doi{10.5281/zenodo.3958749}

\bibitem[\protect\citeauthoryear{Handley}{Handley}{2019a}]{will_handley_2019_3371152}
Handley W.,  2019a, {Curvature tension: evidence for a closed universe
  (supplementary inference products)}, \mn@doi{10.5281/zenodo.3371152}

\bibitem[\protect\citeauthoryear{{Handley}}{{Handley}}{2019b}]{Handley:2019tkm}
{Handley} W.,  2019b, arXiv e-prints, \href
  {https://ui.adsabs.harvard.edu/abs/2019arXiv190809139H} {p. arXiv:1908.09139}

\bibitem[\protect\citeauthoryear{Handley}{Handley}{2019c}]{Handley:2019mfs}
Handley W.,  2019c, \mn@doi [J. Open Source Softw.] {10.21105/joss.01414}, 4,
  1414

\bibitem[\protect\citeauthoryear{Handley}{Handley}{2019d}]{anesthetic}
Handley W.,  2019d, \mn@doi [The Journal of Open Source Software]
  {10.21105/joss.01414}, 4, 1414

\bibitem[\protect\citeauthoryear{Handley, Hobson  \& Lasenby}{Handley
  et~al.}{2015a}]{Handley:2015fda}
Handley W.~J.,  Hobson M.~P.,   Lasenby A.~N.,  2015a, \mn@doi [Mon. Not. Roy.
  Astron. Soc.] {10.1093/mnrasl/slv047}, 450, L61

\bibitem[\protect\citeauthoryear{{Handley}, {Hobson}  \& {Lasenby}}{{Handley}
  et~al.}{2015b}]{Handley:2015xxx}
{Handley} W.~J.,  {Hobson} M.~P.,   {Lasenby} A.~N.,  2015b, \mn@doi [Mon. Not.
  Roy. Astron. Soc.] {10.1093/mnras/stv1911}, \href
  {https://ui.adsabs.harvard.edu/abs/2015MNRAS.453.4384H} {453, 4384}

\bibitem[\protect\citeauthoryear{Higson, Handley, Hobson, Lasenby
  et~al.}{Higson et~al.}{2018}]{higson2018sampling}
Higson E.,  Handley W.,  Hobson M.,  Lasenby A.,   et~al., 2018, Bayesian
  Analysis, 13, 873

\bibitem[\protect\citeauthoryear{Higson, Handley, Hobson  \& Lasenby}{Higson
  et~al.}{2019}]{Higson:2018cqj}
Higson E.,  Handley W.,  Hobson M.,   Lasenby A.,  2019, \mn@doi [Mon. Not.
  Roy. Astron. Soc.] {10.1093/mnras/sty3090}, 483, 2044

\bibitem[\protect\citeauthoryear{Hlozek, Grin, Marsh  \& Ferreira}{Hlozek
  et~al.}{2015}]{Hlozek:2014lca}
Hlozek R.,  Grin D.,  Marsh D. J.~E.,   Ferreira P.~G.,  2015, \mn@doi [Phys.
  Rev. D] {10.1103/PhysRevD.91.103512}, 91, 103512

\bibitem[\protect\citeauthoryear{Johnson, Kirk  \& Stumpf}{Johnson
  et~al.}{2014}]{10.1093/bioinformatics/btu675}
Johnson R.,  Kirk P.,   Stumpf M. P.~H.,  2014, \mn@doi [Bioinformatics]
  {10.1093/bioinformatics/btu675}, 31, 604

\bibitem[\protect\citeauthoryear{Kass \& Raftery}{Kass \&
  Raftery}{1995}]{Kass:1995loi}
Kass R.~E.,  Raftery A.~E.,  1995, \mn@doi [J. Am. Statist. Assoc.]
  {10.1080/01621459.1995.10476572}, 90, 773

\bibitem[\protect\citeauthoryear{Kolmogorov}{Kolmogorov}{1933}]{kolmogorov1933sulla}
Kolmogorov A.,  1933, Giornale dellâ€™ Instuto Italiano degli Attuari, 4, 83

\bibitem[\protect\citeauthoryear{{Liddle}}{{Liddle}}{2007}]{2007MNRAS.377L..74L}
{Liddle} A.~R.,  2007, \mn@doi [\mnras] {10.1111/j.1745-3933.2007.00306.x},
  \href {https://ui.adsabs.harvard.edu/abs/2007MNRAS.377L..74L} {377, L74}

\bibitem[\protect\citeauthoryear{Marsaglia, Tsang  \& Wang}{Marsaglia
  et~al.}{2003}]{JSSv008i18}
Marsaglia G.,  Tsang W.~W.,   Wang J.,  2003, \mn@doi [Journal of Statistical
  Software, Articles] {10.18637/jss.v008.i18}, 8, 1

\bibitem[\protect\citeauthoryear{Martin, Ringeval, Trotta  \& Vennin}{Martin
  et~al.}{2014}]{Martin:2013nzq}
Martin J.,  Ringeval C.,  Trotta R.,   Vennin V.,  2014, \mn@doi [JCAP]
  {10.1088/1475-7516/2014/03/039}, 03, 039

\bibitem[\protect\citeauthoryear{Martinez, McKay, Farmer, Scott, Roebber, Putze
   \& Conrad}{Martinez et~al.}{2017}]{Workgroup:2017htr}
Martinez G.~D.,  McKay J.,  Farmer B.,  Scott P.,  Roebber E.,  Putze A.,
  Conrad J.,  2017, \mn@doi [Eur. Phys. J.] {10.1140/epjc/s10052-017-5274-y},
  C77, 761

\bibitem[\protect\citeauthoryear{Martiniani, Stevenson, Wales  \&
  Frenkel}{Martiniani et~al.}{2014}]{PhysRevX.4.031034}
Martiniani S.,  Stevenson J.~D.,  Wales D.~J.,   Frenkel D.,  2014, \mn@doi
  [Phys. Rev. X] {10.1103/PhysRevX.4.031034}, 4, 031034

\bibitem[\protect\citeauthoryear{Mukherjee, Parkinson  \& Liddle}{Mukherjee
  et~al.}{2006}]{Mukherjee:2005wg}
Mukherjee P.,  Parkinson D.,   Liddle A.~R.,  2006, \mn@doi [Astrophys. J.]
  {10.1086/501068}, 638, L51

\bibitem[\protect\citeauthoryear{Neal}{Neal}{2003}]{neal}
Neal R.~M.,  2003, \mn@doi [Ann. Statist.] {10.1214/aos/1056562461}, 31, 705

\bibitem[\protect\citeauthoryear{Nielsen}{Nielsen}{2013}]{doi:10.1063/1.4821761}
Nielsen S.~O.,  2013, \mn@doi [The Journal of Chemical Physics]
  {10.1063/1.4821761}, 139, 124104

\bibitem[\protect\citeauthoryear{P\'artay, Bart\'ok  \& Cs\'anyi}{P\'artay
  et~al.}{2014}]{PhysRevE.89.022302}
P\'artay L.~B.,  Bart\'ok A.~P.,   Cs\'anyi G.,  2014, \mn@doi [Phys. Rev. E]
  {10.1103/PhysRevE.89.022302}, 89, 022302

\bibitem[\protect\citeauthoryear{{Planck Collaboration} et~al.,}{{Planck
  Collaboration} et~al.}{2018}]{Akrami:2018odb}
{Planck Collaboration} et~al., 2018, arXiv e-prints, \href
  {https://ui.adsabs.harvard.edu/abs/2018arXiv180706211P} {p. arXiv:1807.06211}

\bibitem[\protect\citeauthoryear{PÃ¡rtay, BartÃ³k  \& CsÃ¡nyi}{PÃ¡rtay
  et~al.}{2010}]{doi:10.1021/jp1012973}
PÃ¡rtay L.~B.,  BartÃ³k A.~P.,   CsÃ¡nyi G.,  2010, \mn@doi [The Journal of
  Physical Chemistry B] {10.1021/jp1012973}, 114, 10502

\bibitem[\protect\citeauthoryear{Rosenbrock}{Rosenbrock}{1960}]{10.1093/comjnl/3.3.175}
Rosenbrock H.~H.,  1960, \mn@doi [The Computer Journal]
  {10.1093/comjnl/3.3.175}, 3, 175

\bibitem[\protect\citeauthoryear{Russel, Brewer, Klaere  \& Bouckaert}{Russel
  et~al.}{2018}]{10.1093/sysbio/syy050}
Russel P.~M.,  Brewer B.~J.,  Klaere S.,   Bouckaert R.~R.,  2018, \mn@doi
  [Systematic Biology] {10.1093/sysbio/syy050}, 68, 219

\bibitem[\protect\citeauthoryear{{Salomone}, {South}, {Drovandi}  \&
  {Kroese}}{{Salomone} et~al.}{2018}]{2018arXiv180503924S}
{Salomone} R.,  {South} L.~F.,  {Drovandi} C.~C.,   {Kroese} D.~P.,  2018,
  arXiv e-prints, \href {https://ui.adsabs.harvard.edu/abs/2018arXiv180503924S}
  {p. arXiv:1805.03924}

\bibitem[\protect\citeauthoryear{{Schittenhelm} \& {Wacker}}{{Schittenhelm} \&
  {Wacker}}{2020}]{2020arXiv200508602S}
{Schittenhelm} D.,  {Wacker} P.,  2020, arXiv e-prints, \href
  {https://ui.adsabs.harvard.edu/abs/2020arXiv200508602S} {p. arXiv:2005.08602}

\bibitem[\protect\citeauthoryear{{Skilling}}{{Skilling}}{2004}]{2004AIPC..735..395S}
{Skilling} J.,  2004, in {Fischer} R.,  {Preuss} R.,   {Toussaint} U.~V.,  eds,
   American Institute of Physics Conference Series Vol. 735, American Institute
  of Physics Conference Series. pp 395--405, \mn@doi{10.1063/1.1835238}

\bibitem[\protect\citeauthoryear{Skilling}{Skilling}{2006}]{Skilling:2006gxv}
Skilling J.,  2006, \mn@doi [Bayesian Analysis] {10.1214/06-BA127}, 1, 833

\bibitem[\protect\citeauthoryear{Smirnov}{Smirnov}{1948}]{smirnov1948}
Smirnov N.,  1948, \mn@doi [Ann. Math. Statist.] {10.1214/aoms/1177730256}, 19,
  279

\bibitem[\protect\citeauthoryear{{Speagle}}{{Speagle}}{2020}]{2020MNRAS.tmp..280S}
{Speagle} J.~S.,  2020, \mn@doi [Mon. Not. Roy. Astron. Soc.]
  {10.1093/mnras/staa278}, \href
  {https://ui.adsabs.harvard.edu/abs/2020MNRAS.tmp..280S} {}

\bibitem[\protect\citeauthoryear{Trotta, Feroz, Hobson, Roszkowski  \& Ruiz~de
  Austri}{Trotta et~al.}{2008}]{Trotta:2008bp}
Trotta R.,  Feroz F.,  Hobson M.~P.,  Roszkowski L.,   Ruiz~de Austri R.,
  2008, \mn@doi [JHEP] {10.1088/1126-6708/2008/12/024}, 12, 024

\bibitem[\protect\citeauthoryear{Trotta, JÃ³hannesson, Moskalenko, Porter, de
  Austri  \& Strong}{Trotta et~al.}{2011}]{Trotta:2010mx}
Trotta R.,  JÃ³hannesson G.,  Moskalenko I.~V.,  Porter T.~A.,  de Austri
  R.~R.,   Strong A.~W.,  2011, \mn@doi [Astrophys. J.]
  {10.1088/0004-637X/729/2/106}, 729, 106

\bibitem[\protect\citeauthoryear{Veitch et~al.}{Veitch
  et~al.}{2015}]{Veitch:2014wba}
Veitch J.,  et~al., 2015, \mn@doi [Phys. Rev.] {10.1103/PhysRevD.91.042003},
  D91, 042003

\bibitem[\protect\citeauthoryear{{Virtanen} et~al.}{{Virtanen}
  et~al.}{2020}]{2020SciPy-NMeth}
{Virtanen} P.,  et~al., 2020, \mn@doi [Nature Methods]
  {https://doi.org/10.1038/s41592-019-0686-2}, \href {https://rdcu.be/b08Wh} {}

\makeatother
\end{thebibliography}

```

5. **Author Information:**
- Lead Author: {'name': 'Andrew Fowlie'}
- Full Authors List:
```yaml
Andrew Fowlie: {}
Will Handley:
  pi:
    start: 2020-10-01
    thesis: null
  postdoc:
    start: 2016-10-01
    end: 2020-10-01
    thesis: null
  phd:
    start: 2012-10-01
    end: 2016-09-30
    supervisors:
    - Anthony Lasenby
    - Mike Hobson
    thesis: 'Kinetic initial conditions for inflation: theory, observation and methods'
  original_image: images/originals/will_handley.jpeg
  image: /assets/group/images/will_handley.jpg
  links:
    Webpage: https://willhandley.co.uk
Liangliang Su: {}

```
This YAML file provides a concise snapshot of an academic research group. It lists members by name along with their academic rolesâ€”ranging from Part III and summer projects to MPhil, PhD, and postdoctoral positionsâ€”with corresponding dates, thesis topics, and supervisor details. Supplementary metadata includes image paths and links to personal or departmental webpages. A dedicated "coi" section profiles senior researchers, highlighting the groupâ€™s collaborative mentoring network and career trajectories in cosmology, astrophysics, and Bayesian data analysis.



====================================================================================
Final Output Instructions
====================================================================================

- Combine all data sources to create a seamless, engaging narrative.
- Follow the exact Markdown output format provided at the top.
- Do not include any extra explanation, commentary, or wrapping beyond the specified Markdown.
- Validate that every bibliographic reference with a DOI or arXiv identifier is converted into a Markdown link as per the examples.
- Validate that every Markdown author link corresponds to a link in the author information block.
- Before finalizing, confirm that no LaTeX citation commands or other undesired formatting remain.
- Before finalizing, confirm that the link to the paper itself [2006.03371](https://arxiv.org/abs/2006.03371) is featured in the first sentence.

Generate only the final Markdown output that meets all these requirements.

{% endraw %}